{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05546405-4532-4cbb-a141-92858a357a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd =  os.getcwd().replace(\"notebooks/research\",\"\")\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d43cbe9b-e578-47cb-adb4-fae0dbbdf69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoders shim ready (SBERTEncoder + SBERTFeaturizer) and sys.path configured\n"
     ]
    }
   ],
   "source": [
    "# --- Robust notebook shim for legacy joblib artifacts expecting `encoders.*` ---\n",
    "import sys, types, numpy as np\n",
    "\n",
    "# Create/replace a lightweight 'encoders' module in sys.modules\n",
    "enc_mod = types.ModuleType(\"encoders\")\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except Exception as e:\n",
    "    SentenceTransformer = None\n",
    "    print(\"NOTE: sentence-transformers not available:\", e)\n",
    "\n",
    "class _SBERTBase:\n",
    "    \"\"\"\n",
    "    Compat shim implementing the sklearn Transformer API expected by saved Pipelines.\n",
    "    Handles pickles that don't call __init__ and are missing attributes.\n",
    "    Provides both class names: SBERTEncoder and SBERTFeaturizer.\n",
    "    \"\"\"\n",
    "    # NOTE: __init__ might not be called during unpickle; use _ensure_attrs() everywhere.\n",
    "    def __init__(self, model=\"sentence-transformers/all-MiniLM-L6-v2\", **kwargs):\n",
    "        self.model_name = model\n",
    "        self._enc = None\n",
    "        self._kwargs = kwargs\n",
    "\n",
    "    def _ensure_attrs(self):\n",
    "        # Add any attributes that might be missing from legacy pickles\n",
    "        if not hasattr(self, \"model_name\") or self.model_name is None:\n",
    "            self.model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        if not hasattr(self, \"_enc\"):\n",
    "            self._enc = None\n",
    "        if not hasattr(self, \"_kwargs\"):\n",
    "            self._kwargs = {}\n",
    "\n",
    "    def _ensure_encoder(self):\n",
    "        self._ensure_attrs()\n",
    "        if self._enc is None:\n",
    "            if SentenceTransformer is None:\n",
    "                raise RuntimeError(\n",
    "                    \"sentence-transformers not installed in this kernel; \"\n",
    "                    \"pip install sentence-transformers && restart kernel\"\n",
    "                )\n",
    "            self._enc = SentenceTransformer(self.model_name)\n",
    "\n",
    "    # sklearn API\n",
    "    def fit(self, X, y=None):\n",
    "        self._ensure_attrs()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        self._ensure_encoder()\n",
    "        return np.asarray(self._enc.encode(list(X), show_progress_bar=False))\n",
    "\n",
    "    # some older code may call .encode directly; alias it\n",
    "    def encode(self, X):\n",
    "        return self.transform(X)\n",
    "\n",
    "# Expose both legacy names on the encoders module\n",
    "class SBERTEncoder(_SBERTBase): ...\n",
    "class SBERTFeaturizer(_SBERTBase): ...\n",
    "\n",
    "enc_mod.SBERTEncoder = SBERTEncoder\n",
    "enc_mod.SBERTFeaturizer = SBERTFeaturizer\n",
    "sys.modules[\"encoders\"] = enc_mod\n",
    "\n",
    "# Make sure your package code is importable too (if needed)\n",
    "import pathlib\n",
    "if str(pathlib.Path(\"src\").resolve()) not in sys.path:\n",
    "    sys.path.append(str(pathlib.Path(\"src\").resolve()))\n",
    "print(\"encoders shim ready (SBERTEncoder + SBERTFeaturizer) and sys.path configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "add5fe41-7f49-4b22-a559-56f2bbd389af",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No mapper artifact found in .artifacts/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m joblib\u001b[38;5;241m.\u001b[39mload(p\u001b[38;5;241m.\u001b[39mas_posix())\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo mapper artifact found in .artifacts/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mload_mapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(pipe)\n",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m, in \u001b[0;36mload_mapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading:\u001b[39m\u001b[38;5;124m\"\u001b[39m, p\u001b[38;5;241m.\u001b[39mas_posix())\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m joblib\u001b[38;5;241m.\u001b[39mload(p\u001b[38;5;241m.\u001b[39mas_posix())\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo mapper artifact found in .artifacts/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No mapper artifact found in .artifacts/"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "def load_mapper():\n",
    "    for name in [\".artifacts/defi_mapper.joblib\", \".artifacts/defi_mapper_embed.joblib\"]:\n",
    "        p = Path(name).resolve()\n",
    "        if p.exists():\n",
    "            print(\"Loading:\", p.as_posix())\n",
    "            return joblib.load(p.as_posix())\n",
    "    raise FileNotFoundError(\"No mapper artifact found in .artifacts/\")\n",
    "\n",
    "pipe = load_mapper()\n",
    "print(pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b83c4e60-2c75-4703-8cdf-6b9c9e215a24",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupply 7.0245 SOL to maker\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m pred  \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[38;5;241m.\u001b[39mpredict([prompt])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m probs \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39mpredict_proba([prompt])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted:\u001b[39m\u001b[38;5;124m\"\u001b[39m, pred)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipe' is not defined"
     ]
    }
   ],
   "source": [
    "prompt = \"supply 7.0245 SOL to maker\"\n",
    "pred  = pipe.predict([prompt])[0]\n",
    "probs = pipe.predict_proba([prompt])[0]\n",
    "print(\"Predicted:\", pred)\n",
    "print(\"Top-3:\", sorted(zip(pipe.classes_, probs), key=lambda t: t[1], reverse=True)[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8176a62-c9a4-4dbd-874f-81b97953f2e4",
   "metadata": {},
   "source": [
    "### WDD Doctrine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2fccec9-9bf6-41d1-a245-bd995c6941d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt                                     | prior  | keep | sigma | proto_w | which_prior      | note\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "supply 7.0245 SOL to maker                 |   PASS |  -   |   4   |   13    | deposit(L-5)     | fallback: MF_peak=8.46\n",
      "swap 10 ETH to USDC on uniswap             |   PASS |  2   |   4   |   13    | swap(L-4)        | \n",
      "swap 10 ETH to USDC on uniswa              |   PASS |  2   |   4   |   13    | swap(L-4)        | \n",
      "attempt a borrow with low health factor    | ABSTAIN |  -   |   -   |    -    | unknown          | \n",
      "that's a wrap                              | ABSTAIN |  -   |   -   |    -    | unknown          | \n",
      "sing a swap                                | ABSTAIN |  -   |   -   |    -    | swap(L-4)        | \n",
      "trade a pop                                | ABSTAIN |  -   |   -   |    -    | swap(L-4)        | \n",
      "trade me a drop top                        | ABSTAIN |  -   |   4   |   13    | swap(L-4)        | \n",
      "trade 5.6456 WETH for AAVE on sushiswap (optimism) |   PASS |  2   |   4   |   14    | swap(L-4)        | \n",
      "trade 5.9195 ETH for ARB on sushiswap (arbitrum) |   PASS |  2   |   4   |   16    | swap(L-4)        | \n",
      "market swap 4709.1849 ARB->WBTC using uniswap on ethereum |   PASS |  2   |   4   |   15    | swap(L-4)        | \n",
      "top up uniswap with 10 ARB                 |   PASS |  1   |   4   |   13    | deposit(L-5)     | \n"
     ]
    }
   ],
   "source": [
    "# === Make deposits PASS: auto-layer search (-5,-7), rebuild warp+prior, gentle gates, MF fallback ===\n",
    "import os, re, difflib, numpy as np, torch, joblib\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# ---------------- base encoder ----------------\n",
    "BASE_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "mdl = AutoModel.from_pretrained(BASE_MODEL, output_hidden_states=True).eval()\n",
    "\n",
    "def get_hidden_states(text: str, layer_offset: int) -> np.ndarray:\n",
    "    with torch.no_grad():\n",
    "        out = mdl(**tok(text, return_tensors=\"pt\"))\n",
    "    hs = out.hidden_states\n",
    "    k  = max(-(len(hs)-1), min(layer_offset, -1))\n",
    "    return hs[k].squeeze(0).float().cpu().numpy()  # [T,H]\n",
    "\n",
    "# ---------------- PCA warp H->3 ----------------\n",
    "def fit_token_warp(hiddens, d=3, whiten=True):\n",
    "    X = np.vstack(hiddens); mu = X.mean(0); Xc = X - mu\n",
    "    U,S,Vt = np.linalg.svd(Xc, full_matrices=False)\n",
    "    pcs = Vt[:d,:]; Y = Xc @ pcs.T\n",
    "    scales = Y.std(0, ddof=1) + 1e-8 if whiten else np.ones(d)\n",
    "    return {\"mean\": mu, \"pcs\": pcs, \"scales\": scales}\n",
    "\n",
    "def apply_token_warp(Z, warp):\n",
    "    Y = (Z - warp[\"mean\"]) @ warp[\"pcs\"].T\n",
    "    return Y / (warp[\"scales\"] + 1e-8)\n",
    "\n",
    "def traces_from_text(warp, text, layer_offset):\n",
    "    Z = get_hidden_states(text, layer_offset=layer_offset)\n",
    "    Yw = apply_token_warp(Z, warp)   # [T,3]\n",
    "    return [Yw[:,0], Yw[:,1], Yw[:,2]]\n",
    "\n",
    "# ---------------- NGF bits ----------------\n",
    "from ngeodesic.core.parser import moving_average, geodesic_parse_with_prior\n",
    "from ngeodesic.core.matched_filter import half_sine_proto, nxcorr\n",
    "from ngeodesic.core.funnel_profile import (\n",
    "    fit_radial_profile, analytic_core_template, blend_profiles,\n",
    "    priors_from_profile, attach_projection_info\n",
    ")\n",
    "\n",
    "def normalize_protocols(text: str) -> str:\n",
    "    vocab = [\"uniswap\",\"maker\",\"makerdao\",\"aave\",\"compound\",\"curve\",\"balancer\"]\n",
    "    toks = text.split()\n",
    "    fixed = []\n",
    "    for t in toks:\n",
    "        cand = difflib.get_close_matches(t.lower(), vocab, n=1, cutoff=0.75)\n",
    "        fixed.append(cand[0] if cand else t)\n",
    "    # unify \"maker\" → \"makerdao\" for consistency\n",
    "    txt = \" \".join(fixed)\n",
    "    txt = re.sub(r\"\\bmaker\\b\", \"makerdao\", txt, flags=re.I)\n",
    "    return txt\n",
    "\n",
    "def infer_action(text: str) -> str:\n",
    "    t=text.lower()\n",
    "    if re.search(r\"\\b(supply|deposit|top up)\\b\", t): return \"deposit\"\n",
    "    if re.search(r\"\\b(swap|exchange|trade)\\b\", t): return \"swap\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def adaptive_windows_short(T: int):\n",
    "    proto_w = max(13, min(int(0.7*T), 61))\n",
    "    sigma   = max(4,  min(int(T/8),  9))\n",
    "    return sigma, proto_w\n",
    "\n",
    "def parser_features(x, w):\n",
    "    pos=np.maximum(0.0,x); ma=moving_average(pos, k=w); j=int(np.argmax(ma))\n",
    "    halfw=max(1,w//2); area=float(pos[max(0,j-halfw):j+halfw+1].sum()); meanp=float(pos.mean())\n",
    "    return np.array([j/max(1,len(x)-1), area, meanp], float)\n",
    "\n",
    "def mf_peak(x, proto_w):\n",
    "    q = half_sine_proto(width=proto_w)\n",
    "    c = nxcorr(x, q, mode=\"same\")\n",
    "    return float(np.maximum(0.0, c).max())\n",
    "\n",
    "def build_priors_feature_MFpeak(warp, texts, layer_offset, proto_w=160):\n",
    "    F, Zs = [], []\n",
    "    for t in texts:\n",
    "        tr = traces_from_text(warp, t, layer_offset=layer_offset)\n",
    "        S  = [moving_average(ch, k=min(9, max(3, len(ch)//6))) for ch in tr]\n",
    "        for ch in S:\n",
    "            F.append(parser_features(ch, w=proto_w))\n",
    "            Zs.append(mf_peak(ch, proto_w))\n",
    "    F=np.asarray(F); Zs=np.asarray(Zs)\n",
    "    center = np.median(F[:,:2], axis=0)\n",
    "    R = np.linalg.norm(F[:,:2] - center[None,:], axis=1)\n",
    "    r_grid, z_data = fit_radial_profile(R, Zs, n_r=220, fit_quantile=0.65)\n",
    "    z_core  = analytic_core_template(r_grid, k=0.18, p=1.7, r0_frac=0.14)\n",
    "    z_blend = blend_profiles(z_data, z_core, blend_core=0.25)\n",
    "    pri     = priors_from_profile(r_grid, z_blend)\n",
    "    proj    = {\"mean\": np.zeros(3), \"pcs\": np.eye(3), \"scales\": np.ones(3), \"center\": center.astype(float)}\n",
    "    return attach_projection_info(pri, proj)\n",
    "\n",
    "def wdd_prior_pass(traces, priors, *, z=1.7, rel_floor=0.55, alpha=0.16, beta_s=0.54, q_s=2.0):\n",
    "    T = len(traces[0])\n",
    "    if T < 6: return False, {\"reason\":\"too_short\",\"T\":T}\n",
    "    sigma, proto_w = adaptive_windows_short(T)\n",
    "    keep, order = geodesic_parse_with_prior(\n",
    "        traces, priors=priors, sigma=sigma, proto_width=proto_w,\n",
    "        z=z, rel_floor=rel_floor, alpha=alpha, beta_s=beta_s, q_s=q_s\n",
    "    )\n",
    "    return bool(keep), {\"keep\": keep, \"order\": order, \"sigma\": sigma, \"proto_w\": proto_w, \"mode\":\"prior\"}\n",
    "\n",
    "# ---------------- Existing swap path (kept at L=-4) ----------------\n",
    "SWAP_LAYER = -4\n",
    "swap_cal = [\n",
    "    \"swap 10 ETH to USDC on uniswap\",\n",
    "    \"swap 2000 USDC to ETH on uniswap\",\n",
    "    \"swap 1 WBTC for WETH on curve\",\n",
    "    \"swap 50 SOL to USDC on uniswap\",\n",
    "    \"swap 0.75 ETH to DAI on balancer\",\n",
    "    \"swap 250 DAI to USDC on uniswap\",\n",
    "]\n",
    "SWAP_WARP   = \".artifacts/wdd_warp_swap_L-4.joblib\"\n",
    "SWAP_PRIORS = \".artifacts/wdd_priors_swap_L-4.joblib\"\n",
    "\n",
    "if os.path.exists(SWAP_WARP):\n",
    "    warp_swap = joblib.load(SWAP_WARP)\n",
    "else:\n",
    "    Hs = [get_hidden_states(t, layer_offset=SWAP_LAYER) for t in swap_cal]\n",
    "    warp_swap = fit_token_warp(Hs, d=3, whiten=True); joblib.dump(warp_swap, SWAP_WARP)\n",
    "\n",
    "if os.path.exists(SWAP_PRIORS):\n",
    "    priors_swap = joblib.load(SWAP_PRIORS)\n",
    "else:\n",
    "    priors_swap = build_priors_feature_MFpeak(warp_swap, swap_cal, SWAP_LAYER, proto_w=160)\n",
    "    joblib.dump(priors_swap, SWAP_PRIORS)\n",
    "\n",
    "# ---------------- Deposit path: auto-pick best layer ----------------\n",
    "deposit_cal = [\n",
    "    \"supply 7.0245 SOL to makerdao\",\n",
    "    \"deposit 3 WBTC into vault\",\n",
    "    \"supply 150 USDC to aave\",\n",
    "    \"deposit 2 ETH to compound\",\n",
    "    \"supply 0.5 WETH to makerdao\",\n",
    "    \"deposit 200 DAI into vault\",\n",
    "    \"supply 10 SOL to aave\",\n",
    "    \"deposit 25 USDC to makerdao\",\n",
    "    \"supply 3 ETH to makerdao\",\n",
    "    \"top up lido with 10671 USDC on solana — safe mode\",\n",
    "    \"top up curve with 6.5818 AVAX on polygon — ok with higher gas\",\n",
    "    \"top up yearn with 31.7832 MATIC on base\",\n",
    "]\n",
    "DEP_CAND_LAYERS = [-5, -6, -7]\n",
    "\n",
    "def avg_mf_on_cal(layer):\n",
    "    # build a quick warp per layer, measure mean MF peak across deposit cal\n",
    "    Hd = [get_hidden_states(t, layer_offset=layer) for t in deposit_cal]\n",
    "    w  = fit_token_warp(Hd, d=3, whiten=True)\n",
    "    peaks = []\n",
    "    for t in deposit_cal:\n",
    "        tr = traces_from_text(w, t, layer_offset=layer)\n",
    "        # mild smoothing on each channel, take max across channels\n",
    "        pks = []\n",
    "        for ch in tr:\n",
    "            T = len(ch); proto_w = max(13, min(int(0.7*T), 61))\n",
    "            pks.append(mf_peak(moving_average(ch, k=min(9, max(3, T//6))), proto_w))\n",
    "        peaks.append(max(pks))\n",
    "    return np.mean(peaks), w\n",
    "\n",
    "best_layer, best_warp, best_score = None, None, -1\n",
    "for L in DEP_CAND_LAYERS:\n",
    "    score, w = avg_mf_on_cal(L)\n",
    "    if score > best_score:\n",
    "        best_score, best_layer, best_warp = score, L, w\n",
    "\n",
    "# persist the best deposit warp + priors\n",
    "DEP_LAYER  = best_layer\n",
    "DEP_WARP   = f\".artifacts/wdd_warp_deposit_L{DEP_LAYER}.joblib\"\n",
    "DEP_PRIORS = f\".artifacts/wdd_priors_deposit_L{DEP_LAYER}.joblib\"\n",
    "joblib.dump(best_warp, DEP_WARP)\n",
    "priors_dep = build_priors_feature_MFpeak(best_warp, deposit_cal, DEP_LAYER, proto_w=160)\n",
    "joblib.dump(priors_dep, DEP_PRIORS)\n",
    "\n",
    "# ---------------- Test set (+ deposit fallback) ----------------\n",
    "def deposit_fallback_pass(traces, floor=0.18):\n",
    "    # if prior abstains but MF peak (any channel) is decent, accept\n",
    "    T = len(traces[0]); proto_w = max(13, min(int(0.7*T), 61))\n",
    "    mx = 0.0\n",
    "    for ch in traces:\n",
    "        mx = max(mx, mf_peak(moving_average(ch, k=min(9, max(3, len(ch)//6))), proto_w))\n",
    "    return mx >= floor, mx\n",
    "\n",
    "tests = [\n",
    "    \"supply 7.0245 SOL to maker\",        # normalizes to makerdao\n",
    "    \"swap 10 ETH to USDC on uniswap\",\n",
    "    \"swap 10 ETH to USDC on uniswa\",\n",
    "    \"attempt a borrow with low health factor\",\n",
    "    \"that's a wrap\",\n",
    "    \"sing a swap\",\n",
    "    \"trade a pop\",\n",
    "    \"trade me a drop top\",\n",
    "    \"trade 5.6456 WETH for AAVE on sushiswap (optimism)\",\n",
    "    \"trade 5.9195 ETH for ARB on sushiswap (arbitrum)\",\n",
    "    \"market swap 4709.1849 ARB->WBTC using uniswap on ethereum\",\n",
    "    \"top up uniswap with 10 ARB\"\n",
    "]\n",
    "\n",
    "print(f\"{'prompt'.ljust(42)} | prior  | keep | sigma | proto_w | which_prior      | note\")\n",
    "print(\"-\"*120)\n",
    "for raw in tests:\n",
    "    text = normalize_protocols(raw)\n",
    "    act  = infer_action(text)\n",
    "    note = \"\"  # <-- ensure defined for all branches\n",
    "\n",
    "    if act == \"swap\":\n",
    "        traces = traces_from_text(warp_swap, text, layer_offset=SWAP_LAYER)\n",
    "        ok, info = wdd_prior_pass(traces, priors_swap, z=1.7, rel_floor=0.55, alpha=0.14, beta_s=0.50)\n",
    "        which = \"swap(L-4)\"\n",
    "\n",
    "    elif act == \"deposit\":\n",
    "        traces = traces_from_text(best_warp, text, layer_offset=DEP_LAYER)\n",
    "        ok, info = wdd_prior_pass(traces, priors_dep,  z=1.7, rel_floor=0.55, alpha=0.16, beta_s=0.54)\n",
    "        which = f\"deposit(L{DEP_LAYER})\"\n",
    "        if not ok:\n",
    "            # fallback: MF floor\n",
    "            ok_fallback, mx = deposit_fallback_pass(traces, floor=0.18)\n",
    "            if ok_fallback:\n",
    "                ok = True\n",
    "                info.setdefault(\"keep\", [])\n",
    "                note = f\"fallback: MF_peak={mx:.2f}\"\n",
    "            else:\n",
    "                note = f\"mf={mx:.2f}\"\n",
    "    else:\n",
    "        ok, info, which = False, {\"keep\":[], \"sigma\":None, \"proto_w\":None}, \"unknown\"\n",
    "\n",
    "    print(f\"{raw.ljust(42)} | {('PASS' if ok else 'ABSTAIN'):>6} | {','.join(info.get('keep',[])) or '-':^4} | \"\n",
    "          f\"{info.get('sigma') if info.get('sigma') is not None else '-':^5} | \"\n",
    "          f\"{info.get('proto_w') if info.get('proto_w') is not None else '-':^7} | \"\n",
    "          f\"{which:<16} | {note}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
