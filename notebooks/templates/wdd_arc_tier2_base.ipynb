{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad9b562a-6ea1-440c-8be1-f67d677b7c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd =  os.getcwd().replace(\"notebooks/templates\",\"\")\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7a8e459-25b4-4d16-bbc7-434a4c3eea97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoders shim ready (SBERTEncoder + SBERTFeaturizer) and sys.path configured\n"
     ]
    }
   ],
   "source": [
    "# --- Robust notebook shim for legacy joblib artifacts expecting `encoders.*` ---\n",
    "import sys, types, numpy as np\n",
    "\n",
    "# Create/replace a lightweight 'encoders' module in sys.modules\n",
    "enc_mod = types.ModuleType(\"encoders\")\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except Exception as e:\n",
    "    SentenceTransformer = None\n",
    "    print(\"NOTE: sentence-transformers not available:\", e)\n",
    "\n",
    "class _SBERTBase:\n",
    "    \"\"\"\n",
    "    Compat shim implementing the sklearn Transformer API expected by saved Pipelines.\n",
    "    Handles pickles that don't call __init__ and are missing attributes.\n",
    "    Provides both class names: SBERTEncoder and SBERTFeaturizer.\n",
    "    \"\"\"\n",
    "    # NOTE: __init__ might not be called during unpickle; use _ensure_attrs() everywhere.\n",
    "    def __init__(self, model=\"sentence-transformers/all-MiniLM-L6-v2\", **kwargs):\n",
    "        self.model_name = model\n",
    "        self._enc = None\n",
    "        self._kwargs = kwargs\n",
    "\n",
    "    def _ensure_attrs(self):\n",
    "        # Add any attributes that might be missing from legacy pickles\n",
    "        if not hasattr(self, \"model_name\") or self.model_name is None:\n",
    "            self.model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        if not hasattr(self, \"_enc\"):\n",
    "            self._enc = None\n",
    "        if not hasattr(self, \"_kwargs\"):\n",
    "            self._kwargs = {}\n",
    "\n",
    "    def _ensure_encoder(self):\n",
    "        self._ensure_attrs()\n",
    "        if self._enc is None:\n",
    "            if SentenceTransformer is None:\n",
    "                raise RuntimeError(\n",
    "                    \"sentence-transformers not installed in this kernel; \"\n",
    "                    \"pip install sentence-transformers && restart kernel\"\n",
    "                )\n",
    "            self._enc = SentenceTransformer(self.model_name)\n",
    "\n",
    "    # sklearn API\n",
    "    def fit(self, X, y=None):\n",
    "        self._ensure_attrs()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        self._ensure_encoder()\n",
    "        return np.asarray(self._enc.encode(list(X), show_progress_bar=False))\n",
    "\n",
    "    # some older code may call .encode directly; alias it\n",
    "    def encode(self, X):\n",
    "        return self.transform(X)\n",
    "\n",
    "# Expose both legacy names on the encoders module\n",
    "class SBERTEncoder(_SBERTBase): ...\n",
    "class SBERTFeaturizer(_SBERTBase): ...\n",
    "\n",
    "enc_mod.SBERTEncoder = SBERTEncoder\n",
    "enc_mod.SBERTFeaturizer = SBERTFeaturizer\n",
    "sys.modules[\"encoders\"] = enc_mod\n",
    "\n",
    "# Make sure your package code is importable too (if needed)\n",
    "import pathlib\n",
    "if str(pathlib.Path(\"src\").resolve()) not in sys.path:\n",
    "    sys.path.append(str(pathlib.Path(\"src\").resolve()))\n",
    "print(\"encoders shim ready (SBERTEncoder + SBERTFeaturizer) and sys.path configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de25993f-7a09-4fe8-b0b5-f3ceb9179820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: /Users/ian_moore/repos/micro-lm/.artifacts/arc_mapper.joblib\n",
      "Pipeline(steps=[('sbertencoder', <__main__.SBERTEncoder object at 0x16c0fb940>),\n",
      "                ('calibratedclassifiercv',\n",
      "                 CalibratedClassifierCV(cv=3,\n",
      "                                        estimator=LogisticRegression(C=8.0,\n",
      "                                                                     class_weight='balanced',\n",
      "                                                                     max_iter=2000,\n",
      "                                                                     random_state=0),\n",
      "                                        method='isotonic'))])\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "def load_mapper():\n",
    "    for name in [\".artifacts/arc_mapper.joblib\"]:\n",
    "        p = Path(name).resolve()\n",
    "        if p.exists():\n",
    "            print(\"Loading:\", p.as_posix())\n",
    "            return joblib.load(p.as_posix())\n",
    "    raise FileNotFoundError(\"No mapper artifact found in .artifacts/\")\n",
    "\n",
    "mapper = load_mapper()\n",
    "print(mapper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c42a4e19-babd-461f-9c05-1d391809128c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: flip_h\n",
      "Top-3: [('flip_h', 0.9375), ('flip_v', 0.06250000000000001), ('rotate', 0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "prompt = \"flip the grid horizontally\"\n",
    "#prompt = \"Rotate the grid 90 degrees clockwise, then flip it horizontally\"\n",
    "pred  = mapper.predict([prompt])[0]\n",
    "probs = mapper.predict_proba([prompt])[0]\n",
    "print(\"Predicted:\", pred)\n",
    "print(\"Top-3:\", sorted(zip(mapper.classes_, probs), key=lambda t: t[1], reverse=True)[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b25197-bc9f-4ecd-ba84-9ee9c561cfb4",
   "metadata": {},
   "source": [
    "### WDD Doctrine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40cf0bff-5152-4e79-9a3a-b57ac64fd7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "————————————————————————————————————————————————————————————————————————\n",
      "prompt: rotate the grid 90 degrees clockwise, then flip it horizontally\n",
      "mapper top: [('rotate', 1.0), ('flip_h', 0.0), ('flip_v', 0.0)]\n",
      "selected (WDD order): ['rotate', 'flip_v']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "————————————————————————————————————————————————————————————————————————\n",
      "prompt: flip the grid horizontally\n",
      "mapper top: [('flip_h', 0.9375), ('flip_v', 0.06250000000000001), ('rotate', 0.0)]\n",
      "selected (WDD order): ['flip_h']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "————————————————————————————————————————————————————————————————————————\n",
      "prompt: flip the grid vertically\n",
      "mapper top: [('flip_v', 0.8553240740740741), ('flip_h', 0.14467592592592593), ('rotate', 0.0)]\n",
      "selected (WDD order): ['flip_v']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "————————————————————————————————————————————————————————————————————————\n",
      "prompt: mirror left-to-right\n",
      "mapper top: [('flip_h', 1.0), ('flip_v', 0.0), ('rotate', 0.0)]\n",
      "selected (WDD order): ['flip_h']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "————————————————————————————————————————————————————————————————————————\n",
      "prompt: rotate the grid 90 degrees, then flip the grid vertically\n",
      "mapper top: [('rotate', 1.0), ('flip_h', 0.0), ('flip_v', 0.0)]\n",
      "selected (WDD order): ['rotate', 'flip_v']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "————————————————————————————————————————————————————————————————————————\n",
      "prompt: recolor blue to yellow, then rotate right\n",
      "mapper top: [('rotate', 1.0), ('flip_h', 0.0), ('flip_v', 0.0)]\n",
      "selected (WDD order): ['rotate']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "————————————————————————————————————————————————————————————————————————\n",
      "prompt: rotate the grid 90 degrees clockwise, then flip it horizontally\n",
      "mapper top: [('rotate', 1.0), ('flip_h', 0.0), ('flip_v', 0.0)]\n",
      "selected (WDD order): ['rotate', 'flip_v']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "————————————————————————————————————————————————————————————————————————\n",
      "prompt: flip the grid horizontally\n",
      "mapper top: [('flip_h', 0.9375), ('flip_v', 0.06250000000000001), ('rotate', 0.0)]\n",
      "selected (WDD order): ['flip_h']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "————————————————————————————————————————————————————————————————————————\n",
      "prompt: flip the grid vertically\n",
      "mapper top: [('flip_v', 0.8553240740740741), ('flip_h', 0.14467592592592593), ('rotate', 0.0)]\n",
      "selected (WDD order): ['flip_v']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "————————————————————————————————————————————————————————————————————————\n",
      "prompt: flip me around\n",
      "mapper top: [('flip_v', 0.9949792178891824), ('rotate', 0.0050207821108175935), ('flip_h', 0.0)]\n",
      "selected (WDD order): ['flip_h']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "————————————————————————————————————————————————————————————————————————\n",
      "prompt: rotate the grid 90 degrees, then flip the grid vertically\n",
      "mapper top: [('rotate', 1.0), ('flip_h', 0.0), ('flip_v', 0.0)]\n",
      "selected (WDD order): ['rotate', 'flip_v']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "————————————————————————————————————————————————————————————————————————\n",
      "prompt: recolor blue to yellow, then rotate right\n",
      "mapper top: [('rotate', 1.0), ('flip_h', 0.0), ('flip_v', 0.0)]\n",
      "selected (WDD order): ['rotate']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "————————————————————————————————————————————————————————————————————————\n",
      "prompt: reverse the board across the horizontal axis please\n",
      "mapper top: [('flip_v', 0.6239297129848761), ('flip_h', 0.37607028701512385), ('rotate', 0.0)]\n",
      "selected (WDD order): ['flip_h']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "————————————————————————————————————————————————————————————————————————\n",
      "prompt: spin the puzzle grid 90 degrees clockwise asap.\n",
      "mapper top: [('rotate', 1.0), ('flip_h', 0.0), ('flip_v', 0.0)]\n",
      "selected (WDD order): ['rotate']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "————————————————————————————————————————————————————————————————————————\n",
      "prompt: flip the grid LR thanks\n",
      "mapper top: [('flip_h', 1.0), ('flip_v', 0.0), ('rotate', 0.0)]\n",
      "selected (WDD order): ['flip_v']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "————————————————————————————————————————————————————————————————————————\n",
      "prompt: three-quarter turn counterclockwise on this grid\n",
      "mapper top: [('rotate', 1.0), ('flip_h', 0.0), ('flip_v', 0.0)]\n",
      "selected (WDD order): ['rotate']\n",
      "————————————————————————————————————————————————————————————————————————\n",
      "prompt: quarter turn clockwise on the matrix\n",
      "mapper top: [('rotate', 1.0), ('flip_h', 0.0), ('flip_v', 0.0)]\n",
      "selected (WDD order): ['rotate']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# === WDD–ARC (Tier-2, latent-space) : final cell ==============================================\n",
    "# Uses your trained ARC mapper Pipeline (SBERT → classifier). No grid traces required.\n",
    "# - Shortlists candidates from the mapper (plus text hints)\n",
    "# - Builds latent paths toward per-class prototype directions\n",
    "# - Forms per-family envelopes (rotate uses prob trace; flip/color use energy residual)\n",
    "# - Runs matched-filter + dual gates (family-local), returns kept primitives & order\n",
    "\n",
    "import numpy as np, json, math, warnings, os, re\n",
    "from collections import defaultdict\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# --------------------------- Config (edit if needed) ------------------------------------------\n",
    "ARC_MAPPER_PATH = locals().get(\"ARC_MAPPER_PATH\", \".artifacts/arc_mapper.joblib\")\n",
    "ARC_LABELS_CSV  = locals().get(\"ARC_LABELS_CSV\",  \"\")  # optional: labeled CSV to build centroids\n",
    "ARC_CLASS_NAMES = locals().get(\"ARC_CLASS_NAMES\", [])  # optional static list; else inferred\n",
    "\n",
    "# Latent-path + parser settings\n",
    "TOP_K       = 3\n",
    "TRACE_LEN   = 128\n",
    "ALPHA_MAX   = 2.2\n",
    "SMOOTH_WIN  = 9\n",
    "NULL_SHIFTS = 64\n",
    "\n",
    "# Bring-up family-local thresholds (tighten later)\n",
    "F_TAU = {\n",
    "    \"flip\":   {\"tau_rel\": 0.05, \"tau_corr\": 0.25, \"z_abs\": 0.5, \"area\": 0.0},\n",
    "    \"rotate\": {\"tau_rel\": 0.05, \"tau_corr\": 0.20, \"z_abs\": 0.30, \"area\": 0.0},\n",
    "    \"other\":  {\"tau_rel\": 0.05, \"tau_corr\": 0.25, \"z_abs\": 0.50, \"area\": 0.0},\n",
    "}\n",
    "\n",
    "# Canonical mini-templates (used only if coef_/centroids absent)\n",
    "ARC_CANON = {\n",
    "    \"flip_h\": \"flip the grid horizontally\",\n",
    "    \"flip_v\": \"flip the grid vertically\",\n",
    "    \"rotate\": \"rotate the grid 90 degrees clockwise\",\n",
    "    \"rot90\":  \"rotate the grid 90 degrees clockwise\",\n",
    "    \"rotate_cw\": \"rotate the grid 90 degrees clockwise\",\n",
    "    \"rotate_ccw\": \"rotate the grid 90 degrees counterclockwise\",\n",
    "}\n",
    "\n",
    "ARC_CLASS_NAMES = [\"rotate\", \"flip_h\", \"flip_v\"] \n",
    "\n",
    "# --------------------------------- Helpers -----------------------------------------------------\n",
    "def _normalize(v):\n",
    "    v = np.asarray(v, float).reshape(-1)\n",
    "    return v / (np.linalg.norm(v) + 1e-12)\n",
    "\n",
    "def _unwrap_mapper(mapper):\n",
    "    \"\"\"Return (encoder_step, clf_step) from a sklearn Pipeline-like mapper.\"\"\"\n",
    "    enc = clf = None\n",
    "    if hasattr(mapper, \"named_steps\"):\n",
    "        steps = dict(mapper.named_steps)\n",
    "        # guess encoder by capability & name\n",
    "        for name, step in steps.items():\n",
    "            if hasattr(step, \"transform\") and (\"sbert\" in str(step).lower() or \"sentence\" in str(step).lower()):\n",
    "                enc = step; break\n",
    "        clf = list(steps.values())[-1]\n",
    "    else:\n",
    "        # best-effort (non-pipeline)\n",
    "        if hasattr(mapper, \"transform\"): enc = mapper\n",
    "        if hasattr(mapper, \"predict_proba\") or hasattr(mapper, \"decision_function\"): clf = mapper\n",
    "    # calibrated wrappers sometimes store base_estimator\n",
    "    base = getattr(clf, \"base_estimator\", None)\n",
    "    if base is not None:\n",
    "        clf = base\n",
    "    return enc, clf\n",
    "\n",
    "def _get_classes(mapper):\n",
    "    \"\"\"\n",
    "    Safely extract class names from the final estimator in the mapper pipeline.\n",
    "    Handles numpy arrays and missing attributes; falls back to ARC_CLASS_NAMES.\n",
    "    \"\"\"\n",
    "    classes = None\n",
    "\n",
    "    # Try the last pipeline step first\n",
    "    if hasattr(mapper, \"named_steps\"):\n",
    "        last = list(mapper.named_steps.values())[-1]\n",
    "        classes = getattr(last, \"classes_\", None)\n",
    "\n",
    "    # Fallback: mapper itself\n",
    "    if classes is None:\n",
    "        classes = getattr(mapper, \"classes_\", None)\n",
    "\n",
    "    # If still missing or empty, use configured fallback\n",
    "    if classes is None or (hasattr(classes, \"__len__\") and len(classes) == 0):\n",
    "        classes = ARC_CLASS_NAMES\n",
    "\n",
    "    # Final sanity: must have something\n",
    "    if classes is None or (hasattr(classes, \"__len__\") and len(classes) == 0):\n",
    "        raise ValueError(\n",
    "            \"Could not infer class names from mapper. \"\n",
    "            \"Set ARC_CLASS_NAMES to a non-empty list (e.g., \"\n",
    "            \"['rotate','flip_h','flip_v','recolor']).\"\n",
    "        )\n",
    "\n",
    "    # Convert numpy arrays to a Python list of strings\n",
    "    if hasattr(classes, \"tolist\"):\n",
    "        classes = classes.tolist()\n",
    "    return [str(c) for c in classes]\n",
    "\n",
    "def _embed_texts(mapper, texts):\n",
    "    \"\"\"Encode texts with the SBERT encoder inside the mapper; fallback to SentenceTransformer.\"\"\"\n",
    "    enc, _ = _unwrap_mapper(mapper)\n",
    "    if enc is not None and hasattr(enc, \"transform\"):\n",
    "        E = enc.transform(list(texts))\n",
    "        return np.asarray(E)\n",
    "    # fallback\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return np.asarray(model.encode(list(texts), normalize_embeddings=True, convert_to_numpy=True, show_progress_bar=False))\n",
    "\n",
    "def _classifier_for_embed(mapper):\n",
    "    \"\"\"Return the last estimator (accepts embeddings). May be calibrated.\"\"\"\n",
    "    if hasattr(mapper, \"named_steps\"):\n",
    "        return list(mapper.named_steps.values())[-1]\n",
    "    return mapper\n",
    "\n",
    "def _classifier_coef(clf):\n",
    "    \"\"\"Try to recover coef_ matrix from linear models or wrapped models.\"\"\"\n",
    "    co = getattr(clf, \"coef_\", None)\n",
    "    if co is not None: return np.asarray(co, float)\n",
    "    # calibrated form\n",
    "    for attr in (\"estimator_\", \"estimator\", \"base_estimator_\", \"base_estimator\"):\n",
    "        inner = getattr(clf, attr, None)\n",
    "        if inner is not None and hasattr(inner, \"coef_\"):\n",
    "            return np.asarray(inner.coef_, float)\n",
    "    # OneVsRest\n",
    "    if hasattr(clf, \"estimators_\"):\n",
    "        rows = []\n",
    "        for est in clf.estimators_:\n",
    "            got = _classifier_coef(est)\n",
    "            if got is None: return None\n",
    "            rows.append(got.ravel())\n",
    "        return np.vstack(rows) if rows else None\n",
    "    # CalibratedClassifierCV (list of per-class calibrators)\n",
    "    if hasattr(clf, \"calibrated_classifiers_\"):\n",
    "        rows = []\n",
    "        for cc in clf.calibrated_classifiers_:\n",
    "            est = getattr(cc, \"estimator\", None)\n",
    "            if est is not None and hasattr(est, \"coef_\"):\n",
    "                rows.append(est.coef_.ravel())\n",
    "        return np.vstack(rows) if rows else None\n",
    "    return None\n",
    "\n",
    "def _class_centroids_from_csv(mapper, labels_csv, class_names):\n",
    "    \"\"\"Optional: build SBERT centroids per class from a labeled CSV.\"\"\"\n",
    "    if not labels_csv or not os.path.exists(labels_csv): return None\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(labels_csv)\n",
    "    if not {\"prompt\",\"label\"}.issubset(df.columns): return None\n",
    "    df = df.dropna(subset=[\"prompt\",\"label\"]).copy()\n",
    "    df[\"prompt\"] = df[\"prompt\"].astype(str).str.strip()\n",
    "    df[\"label\"]  = df[\"label\"].astype(str).str.strip()\n",
    "    if df.empty: return None\n",
    "    # embed in chunks\n",
    "    B = 256; Es = []\n",
    "    for i in range(0, len(df), B):\n",
    "        Es.append(_embed_texts(mapper, df[\"prompt\"].iloc[i:i+B].tolist()))\n",
    "    X = np.vstack(Es)\n",
    "    y = df[\"label\"].tolist()\n",
    "    cents = {}\n",
    "    for c in class_names:\n",
    "        idx = [i for i, yy in enumerate(y) if yy == c]\n",
    "        if idx:\n",
    "            m = X[idx].mean(axis=0)\n",
    "            cents[c] = _normalize(m)\n",
    "    return cents if cents else None\n",
    "\n",
    "def _proto_directions(mapper, class_names):\n",
    "    \"\"\"Return dict label->unit direction. Try coef_, else CSV centroids, else canonical templates.\"\"\"\n",
    "    enc, clf = _unwrap_mapper(mapper)\n",
    "    # 1) coef_\n",
    "    co = _classifier_coef(clf)\n",
    "    if co is not None:\n",
    "        cls = getattr(clf, \"classes_\", class_names)\n",
    "        cls = [str(c) for c in cls]\n",
    "        P = {}\n",
    "        for c in class_names:\n",
    "            if c in cls:\n",
    "                j = cls.index(c)\n",
    "                P[c] = _normalize(co[j])\n",
    "        if len(P) == len(class_names):\n",
    "            return P\n",
    "    # 2) centroids\n",
    "    cents = _class_centroids_from_csv(mapper, ARC_LABELS_CSV, class_names)\n",
    "    if cents:\n",
    "        mu = _normalize(np.mean(np.vstack(list(cents.values())), axis=0))\n",
    "        return {c: _normalize(cents[c] - mu) for c in class_names if c in cents}\n",
    "    # 3) canonical templates\n",
    "    T = [ARC_CANON.get(c, c) for c in class_names]\n",
    "    E = _embed_texts(mapper, T)\n",
    "    mu = _normalize(E.mean(axis=0))\n",
    "    return {c: _normalize(E[i] - mu) for i, c in enumerate(class_names)}\n",
    "\n",
    "def _half_sine_path(e0, p_hat, T=TRACE_LEN, alpha_max=ALPHA_MAX):\n",
    "    p_hat = _normalize(p_hat)\n",
    "    alphas = np.sin(np.linspace(0.0, np.pi, T)) * alpha_max\n",
    "    return e0[None, :] + alphas[:, None] * p_hat[None, :]\n",
    "\n",
    "def _smooth(x, w=SMOOTH_WIN):\n",
    "    if w <= 1: return x\n",
    "    k = np.ones(w, float) / float(w)\n",
    "    return np.convolve(np.asarray(x, float), k, mode=\"same\")\n",
    "\n",
    "def _energies(Y, p, c=None):\n",
    "    p = _normalize(p)\n",
    "    D = Y.shape[1]\n",
    "    if c is None: c = np.zeros(D, float)\n",
    "    Z = Y - c[None, :]\n",
    "    s = Z @ p\n",
    "    E_par  = s**2\n",
    "    E_tot  = np.sum(Z*Z, axis=1)\n",
    "    E_perp = E_tot - E_par\n",
    "    return _smooth(E_par), _smooth(E_perp)\n",
    "\n",
    "def _nxcorr(env, q):\n",
    "    env = np.asarray(env, float)\n",
    "    q   = np.asarray(q, float)\n",
    "    q   = (q - q.mean()); qn = np.linalg.norm(q) + 1e-12\n",
    "    L   = len(q)\n",
    "    pad = L // 2\n",
    "    xx  = np.pad(env, (pad, pad), mode=\"edge\")\n",
    "    C   = np.zeros_like(env)\n",
    "    for t in range(len(env)):\n",
    "        seg = xx[t:t+L]; seg = seg - seg.mean()\n",
    "        C[t] = float(np.dot(seg, q) / ((np.linalg.norm(seg) + 1e-12) * qn))\n",
    "    return C\n",
    "\n",
    "def _matched_filter_score(env):\n",
    "    L = max(50, min(80, len(env)-4))\n",
    "    t = np.arange(L)\n",
    "    q = np.sin(np.pi * (t / (L-1)))\n",
    "    C = _nxcorr(env, q)\n",
    "    t_star = int(np.argmax(C))\n",
    "    # half-max window\n",
    "    xm = float(np.max(env)); half = 0.5 * xm\n",
    "    a = t_star\n",
    "    while a > 0 and env[a] > half: a -= 1\n",
    "    b = t_star\n",
    "    while b < len(env)-1 and env[b] > half: b += 1\n",
    "    area = float(np.sum(env[max(0,a):min(len(env)-1,b)+1]))\n",
    "    return float(C[t_star]), t_star, area, (int(a), int(b)), C\n",
    "\n",
    "def _circular_shifts(x, K=NULL_SHIFTS):\n",
    "    n = len(x)\n",
    "    if n <= 2: return []\n",
    "    shifts = np.random.choice(np.arange(1, n), size=min(K, n-1), replace=False)\n",
    "    return [np.roll(x, s) for s in shifts]\n",
    "\n",
    "def _absolute_null_z(env):\n",
    "    nulls = _circular_shifts(env, NULL_SHIFTS)\n",
    "    if not nulls: return 0.0\n",
    "    peaks = [_matched_filter_score(ne)[0] for ne in nulls]\n",
    "    mu, sd = float(np.mean(peaks)), float(np.std(peaks) + 1e-9)\n",
    "    cm, *_ = _matched_filter_score(env)\n",
    "    return (cm - mu) / sd\n",
    "\n",
    "def _exclusive_residual(E_list):\n",
    "    \"\"\"Exclusive positive residual across siblings (QR projection; fallback to mean subtraction).\"\"\"\n",
    "    if len(E_list) == 1:\n",
    "        return [np.clip((E_list[0] - np.mean(E_list[0])) / (np.std(E_list[0]) + 1e-9), 0, None)]\n",
    "    Z = np.vstack([ (e - np.mean(e)) / (np.std(e) + 1e-9) for e in E_list ])  # K x T\n",
    "    Rex = []\n",
    "    for k in range(Z.shape[0]):\n",
    "        others = [j for j in range(Z.shape[0]) if j != k]\n",
    "        A = Z[others].T  # T x (K-1)\n",
    "        try:\n",
    "            Q, _ = np.linalg.qr(A, mode=\"reduced\")\n",
    "            proj = Q @ (Q.T @ Z[k])\n",
    "            ex = Z[k] - proj\n",
    "        except Exception:\n",
    "            ex = Z[k] - np.mean(Z[others], axis=0)\n",
    "        Rex.append(np.clip(ex, 0, None))\n",
    "    return Rex\n",
    "\n",
    "def _family(lbl: str) -> str:\n",
    "    s = lbl.lower()\n",
    "    if s.startswith(\"flip_\"):                 return \"flip\"\n",
    "    if \"rot\" in s or s in {\"rotate\",\"rot90\",\"rotate_cw\",\"rotate_ccw\"}: return \"rotate\"\n",
    "    if s in {\"recolor\",\"color_map\",\"color\"}: return \"color\"\n",
    "    return \"other\"\n",
    "\n",
    "def _parse_hints(prompt: str):\n",
    "    s = prompt.lower()\n",
    "    want_flip   = (\"flip\" in s) or (\"mirror\" in s)\n",
    "    want_rotate = (\"rotate\" in s) or (\"cw\" in s) or (\"clockwise\" in s) or (\"counterclockwise\" in s) or (\"ccw\" in s) or (\"right\" in s) or (\"left\" in s)\n",
    "    want_color  = (\"color\" in s) or (\"recolor\" in s) or (\"colour\" in s) or (\"recolour\" in s) or (\"map\" in s)\n",
    "    orient = None\n",
    "    # add synonyms in _parse_hints\n",
    "    if re.search(r\"\\b(hori(zontal)?|mirror\\s*(left|right)|left\\s*↔\\s*right)\\b\", s): orient = \"flip_h\"\n",
    "    elif re.search(r\"\\b(vert(ical)?|top\\s*↕\\s*bottom|up\\s*down)\\b\", s): orient = \"flip_v\"\n",
    "    return {\"flip\": want_flip, \"rotate\": want_rotate, \"color\": want_color}, orient\n",
    "\n",
    "# --------------------------------- Main audit ---------------------------------\n",
    "def wdd_arc_audit(prompt: str):\n",
    "    import joblib\n",
    "    mapper = joblib.load(ARC_MAPPER_PATH)\n",
    "    class_names = _get_classes(mapper)\n",
    "\n",
    "    # 0) text hints\n",
    "    hints, orient_hint = _parse_hints(prompt)\n",
    "\n",
    "    # 1) embed prompt (start point e0)\n",
    "    e0 = _normalize(_embed_texts(mapper, [prompt])[0])\n",
    "\n",
    "    # 2) mapper probabilities (for shortlist + tiny flip tie-break)\n",
    "    if hasattr(mapper, \"predict_proba\"):\n",
    "        probs = mapper.predict_proba([prompt])[0]\n",
    "        smap  = {c: float(p) for c, p in zip(class_names, probs)}\n",
    "    elif hasattr(mapper, \"decision_function\"):\n",
    "        z = np.asarray(mapper.decision_function([prompt])[0], float).ravel()\n",
    "        ex = np.exp(z - z.max()); pr = ex / (ex.sum() + 1e-12)\n",
    "        smap = {c: float(p) for c, p in zip(class_names, pr)}\n",
    "    else:\n",
    "        pred = str(mapper.predict([prompt])[0])\n",
    "        smap = {c: (1.0 if c==pred else 0.0) for c in class_names}\n",
    "    mapper_top = max(smap.items(), key=lambda kv: kv[1])[0]\n",
    "\n",
    "    # 3) route: text-gated shortlist\n",
    "    MIN_CAND_PROB = 0.05     # tiny floor to consider a class from mapper\n",
    "    STRICT_TEXT_GATING = True\n",
    "    \n",
    "    def _family(lbl: str) -> str:\n",
    "        s = lbl.lower()\n",
    "        if s.startswith(\"flip_\"):                 return \"flip\"\n",
    "        if \"rot\" in s or s in {\"rotate\",\"rot90\",\"rotate_cw\",\"rotate_ccw\"}: return \"rotate\"\n",
    "        if s in {\"recolor\",\"color_map\",\"color\"}:  return \"color\"\n",
    "        return \"other\"\n",
    "    \n",
    "    # families explicitly mentioned in the prompt\n",
    "    allowed_fams = set()\n",
    "    if hints[\"flip\"]:   allowed_fams.add(\"flip\")\n",
    "    if hints[\"rotate\"]: allowed_fams.add(\"rotate\")\n",
    "    if hints[\"color\"]:  allowed_fams.add(\"color\")\n",
    "    \n",
    "    # base set: keep top-k classes with non-trivial probability\n",
    "    topk = sorted(smap.items(), key=lambda kv: kv[1], reverse=True)[:max(1, TOP_K)]\n",
    "    base = {c for c, p in topk if p >= MIN_CAND_PROB}\n",
    "    base.add(mapper_top)  # always include mapper top-1\n",
    "    \n",
    "    # text inclusions: make sure families explicitly requested are represented\n",
    "    text_added = set()\n",
    "    for fam in allowed_fams:\n",
    "        for c in class_names:\n",
    "            if _family(c) == fam:\n",
    "                text_added.add(c)\n",
    "    \n",
    "    # strict text gating: if any families are mentioned, restrict to them,\n",
    "    # but still allow any class with prob >= MIN_CAND_PROB\n",
    "    shortlist = (base | text_added)\n",
    "    if STRICT_TEXT_GATING and allowed_fams:\n",
    "        shortlist = {c for c in shortlist if (_family(c) in allowed_fams) or (smap.get(c, 0.0) >= MIN_CAND_PROB)}\n",
    "    \n",
    "    labs = [c for c in class_names if c in shortlist]\n",
    "    if not labs:\n",
    "        return {\"prompt\": prompt, \"selected\": [], \"why\": \"no candidates\", \"scores\": smap}\n",
    "\n",
    "\n",
    "    # 4) prototypes & anchors\n",
    "    P = _proto_directions(mapper, class_names)\n",
    "    D = e0.shape[0]\n",
    "    for k,v in list(P.items()):\n",
    "        v = np.asarray(v, float).reshape(-1)\n",
    "        if v.shape[0] != D:\n",
    "            v = (v[:D] if v.shape[0] > D else np.pad(v, (0, D - v.shape[0])))\n",
    "        P[k] = _normalize(v)\n",
    "    anchors = defaultdict(lambda: np.zeros(D, float))\n",
    "    cents = _class_centroids_from_csv(mapper, ARC_LABELS_CSV, class_names)\n",
    "    if cents:\n",
    "        for c, m in cents.items():\n",
    "            mv = np.asarray(m, float).reshape(-1)\n",
    "            mv = (mv[:D] if mv.shape[0] > D else np.pad(mv, (0, D - mv.shape[0])))\n",
    "            anchors[c] = _normalize(mv)\n",
    "\n",
    "    # 5) latent paths + energy/prob envelopes\n",
    "    enc, clf = _unwrap_mapper(mapper)\n",
    "    clf_embed = _classifier_for_embed(mapper)\n",
    "    clf_cls   = getattr(clf_embed, \"classes_\", class_names)\n",
    "    idx_map   = {str(c): i for i, c in enumerate(clf_cls)}\n",
    "\n",
    "    per_class = {}\n",
    "    fam2labs  = defaultdict(list)\n",
    "    for lab in labs:\n",
    "        if lab not in P: continue\n",
    "        Y = _half_sine_path(e0, P[lab], TRACE_LEN, ALPHA_MAX)      # (T, D)\n",
    "        E_par, E_perp = _energies(Y, P[lab], anchors[lab])\n",
    "        # prob envelope along path (if classifier exposes proba over embeddings)\n",
    "        if hasattr(clf_embed, \"predict_proba\"):\n",
    "            P_path = clf_embed.predict_proba(Y)                    # (T, C)\n",
    "            j = idx_map.get(lab, None)\n",
    "            prob_env = _smooth(P_path[:, j], SMOOTH_WIN) if j is not None else None\n",
    "        else:\n",
    "            prob_env = None\n",
    "        per_class[lab] = {\"E_par\": E_par, \"E_perp\": E_perp, \"prob\": prob_env}\n",
    "        fam2labs[_family(lab)].append(lab)\n",
    "\n",
    "    if not per_class:\n",
    "        return {\"prompt\": prompt, \"selected\": [], \"why\": \"no envelopes\", \"scores\": smap}\n",
    "\n",
    "    # 6) family-local residuals & scoring (rotate uses prob; flip/color use energy residual)\n",
    "    for fam, labs_f in fam2labs.items():\n",
    "        Elist = [per_class[k][\"E_perp\"] for k in labs_f]\n",
    "        Rex_f = _exclusive_residual(Elist) if len(labs_f) > 0 else []\n",
    "        for idx, k in enumerate(labs_f):\n",
    "            if fam == \"rotate\" and per_class[k][\"prob\"] is not None:\n",
    "                env = _smooth(np.clip(per_class[k][\"prob\"], 0, None), SMOOTH_WIN)\n",
    "            else:\n",
    "                ex = Rex_f[idx] if idx < len(Rex_f) else per_class[k][\"E_perp\"]\n",
    "                env = _smooth(np.clip(ex, 0, None), SMOOTH_WIN)\n",
    "            cm, t_star, area, (a,b), C = _matched_filter_score(env)\n",
    "            z_abs = _absolute_null_z(env)\n",
    "            per_class[k].update(dict(env=env, corr_max=cm, t_star=int(t_star),\n",
    "                                     area=float(area), window=(int(a), int(b)),\n",
    "                                     corr_trace=C, z_abs=float(z_abs)))\n",
    "\n",
    "    # 7) family-wise gating then merge\n",
    "    kept = []\n",
    "    for fam, labs_f in fam2labs.items():\n",
    "        thr = F_TAU.get(fam, F_TAU[\"other\"])\n",
    "        ranked_f = sorted(labs_f, key=lambda k: (per_class[k][\"corr_max\"], per_class[k][\"area\"]), reverse=True)\n",
    "        best_cm = per_class[ranked_f[0]][\"corr_max\"]\n",
    "        decided_f = []\n",
    "        for k in ranked_f:\n",
    "            r = per_class[k]\n",
    "            rel_ok  = (r[\"corr_max\"] >= (1.0 - thr[\"tau_rel\"]) * best_cm)\n",
    "            corr_ok = (r[\"corr_max\"] >= thr[\"tau_corr\"])\n",
    "            abs_ok  = (r[\"z_abs\"]    >= thr[\"z_abs\"])\n",
    "            area_ok = (r[\"area\"]     >= thr[\"area\"])\n",
    "            accept  = (abs_ok or (corr_ok and area_ok))\n",
    "            if accept:\n",
    "                decided_f.append((k, r[\"t_star\"]))\n",
    "        if len(decided_f) > 1:\n",
    "            decided_f = [(k,t) for (k,t) in decided_f if per_class[k][\"corr_max\"] >= (1.0 - thr[\"tau_rel\"]) * best_cm]\n",
    "        kept.extend(decided_f)\n",
    "\n",
    "    # 8) orientation pin + tiny mapper nudge inside flip family\n",
    "    flip_kept = [k for (k,_) in kept if k in (\"flip_h\",\"flip_v\")]\n",
    "    hints_flip_orient = orient_hint in (\"flip_h\",\"flip_v\")\n",
    "    if hints[\"flip\"] and hints_flip_orient and orient_hint not in flip_kept:\n",
    "        rh = per_class.get(orient_hint)\n",
    "        if rh and (rh[\"z_abs\"] >= 0.0 or rh[\"corr_max\"] >= 0.15):\n",
    "            kept = [(k,t) for (k,t) in kept if k not in (\"flip_h\",\"flip_v\")]\n",
    "            kept.append((orient_hint, rh[\"t_star\"]))\n",
    "    flip_kept = [k for (k,_) in kept if k in (\"flip_h\",\"flip_v\")]\n",
    "    if hints[\"flip\"] and mapper_top in (\"flip_h\",\"flip_v\") and flip_kept:\n",
    "        chosen = flip_kept[0]\n",
    "        if chosen != mapper_top:\n",
    "            r_bad = per_class[chosen]; r_good = per_class.get(mapper_top)\n",
    "            if r_good and (r_bad[\"corr_max\"] - r_good[\"corr_max\"] <= 0.05):\n",
    "                kept = [(k,t) for (k,t) in kept if k not in (\"flip_h\",\"flip_v\")]\n",
    "                kept.append((mapper_top, r_good[\"t_star\"]))\n",
    "\n",
    "    # 9) order by peak time\n",
    "    kept.sort(key=lambda kv: kv[1])\n",
    "    sequence = [k for k,_ in kept]\n",
    "\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"scores\": dict(sorted(smap.items(), key=lambda kv: kv[1], reverse=True)),\n",
    "        \"candidates\": labs,\n",
    "        \"selected\": sequence,\n",
    "        \"per_class\": {k: {kk: (vv.tolist() if hasattr(vv, \"tolist\") else vv)\n",
    "                          for kk, vv in v.items()\n",
    "                          if kk in [\"corr_max\",\"t_star\",\"area\",\"window\",\"z_abs\"]}\n",
    "                      for k, v in per_class.items()},\n",
    "    }\n",
    "\n",
    "# ------------------------------- Quick demo -----------------------------------\n",
    "EXAMPLES = [\n",
    "    \"rotate the grid 90 degrees clockwise, then flip it horizontally\",\n",
    "    \"flip the grid horizontally\",\n",
    "    \"flip the grid vertically\",\n",
    "    \"mirror left-to-right\",\n",
    "    \"rotate the grid 90 degrees, then flip the grid vertically\",\n",
    "    \"recolor blue to yellow, then rotate right\",\n",
    "    \"rotate the grid 90 degrees clockwise, then flip it horizontally\",\n",
    "    \"flip the grid horizontally\",\n",
    "    \"flip the grid vertically\",\n",
    "    \"flip me around\",\n",
    "    \"rotate the grid 90 degrees, then flip the grid vertically\",\n",
    "    \"recolor blue to yellow, then rotate right\",\n",
    "    \"reverse the board across the horizontal axis please\",\n",
    "    \"spin the puzzle grid 90 degrees clockwise asap.\",\n",
    "    \"flip the grid LR thanks\",\n",
    "    \"three-quarter turn counterclockwise on this grid\",\n",
    "    \"quarter turn clockwise on the matrix\",\n",
    "]\n",
    "\n",
    "for s in EXAMPLES:\n",
    "    out = wdd_arc_audit(s)\n",
    "    print(\"—\"*72)\n",
    "    print(\"prompt:\", s)\n",
    "    print(\"mapper top:\", list(out[\"scores\"].items())[:3])\n",
    "    print(\"selected (WDD order):\", out[\"selected\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3aa76d4-a78b-416e-89c7-47202d4780b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prompt': 'quarter turn clockwise on the matrix',\n",
       " 'scores': {'rotate': 1.0, 'flip_h': 0.0, 'flip_v': 0.0},\n",
       " 'candidates': ['rotate'],\n",
       " 'selected': ['rotate'],\n",
       " 'per_class': {'rotate': {'corr_max': 0.547179951108328,\n",
       "   't_star': 0,\n",
       "   'area': 6.265300263505406,\n",
       "   'window': (0, 14),\n",
       "   'z_abs': -1.4115122082150304}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = wdd_arc_audit('quarter turn clockwise on the matrix')\n",
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
