{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd8b9603-d4a1-4737-bc96-5a285915c2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "End-to-end script for training and evaluating a DeFi mapper.\n",
    "Combines simplified training from train_mapper_embed.py and evaluation/prediction from defi_milestone8.py,\n",
    "in a style similar to mapper_simple.py for easy Jupyter adaptation.\n",
    "\n",
    "Usage example:\n",
    "python3 end_to_end_mapper.py \\\n",
    "  --train_labels_csv tests/fixtures/defi_mapper_labeled_mini.csv \\\n",
    "  --sbert sentence-transformers/all-mpnet-base-v2 \\\n",
    "  --C 8 --max_iter 2000 --calibrate \\\n",
    "  --prompts_jsonl tests/fixtures/defi_mapper_5k_prompts.jsonl \\\n",
    "  --eval_labels_csv tests/fixtures/defi_mapper_labeled_5k.csv \\\n",
    "  --thresholds \"0.5,0.6,0.7,0.8,0.9\" \\\n",
    "  --out_dir .artifacts \\\n",
    "  --out_model defi_mapper_embed.joblib \\\n",
    "  --out_summary defi_milestone8_summary.json \\\n",
    "  --out_csv defi_milestone8_metrics.csv \\\n",
    "  --out_rows_csv defi_milestone8_rows.csv\n",
    "\n",
    "In Jupyter, you can copy-paste sections or override args programmatically.\n",
    "\"\"\"\n",
    "\n",
    "import argparse, csv, json, os, sys, time\n",
    "from collections import Counter\n",
    "from typing import Any, Dict, List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import joblib\n",
    "import os\n",
    "cwd =  os.getcwd().replace(\"/notebooks\",\"\")\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dce06ec5-32e8-47e2-93e5-3f105728f90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SBERTEncoder class (from defi_milestone8.py shim)\n",
    "class SBERTEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                 batch_size=64, normalize=True):\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.normalize = normalize\n",
    "        self._model = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        self._model = SentenceTransformer(self.model_name)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self._model is None:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            self._model = SentenceTransformer(self.model_name)\n",
    "        embs = self._model.encode(\n",
    "            list(X),\n",
    "            batch_size=self.batch_size,\n",
    "            show_progress_bar=False,\n",
    "            normalize_embeddings=self.normalize,\n",
    "        )\n",
    "        return np.asarray(embs)\n",
    "\n",
    "DEFAULT_CLASSES = [\"deposit_asset\", \"withdraw_asset\", \"swap_asset\", \"check_balance\"]\n",
    "\n",
    "# -------------------------- Training (from train_mapper_embed.py) --------------------------\n",
    "\n",
    "def train_mapper(args):\n",
    "    df = pd.read_csv(args.train_labels_csv)\n",
    "    need = {\"prompt\", \"label\"}\n",
    "    if not need.issubset(df.columns):\n",
    "        sys.exit(f\"[train] labels_csv must have columns {need}, got {df.columns.tolist()}\")\n",
    "    df = df.copy()\n",
    "    df[\"prompt\"] = df[\"prompt\"].astype(str).str.strip()\n",
    "    df[\"label\"] = df[\"label\"].astype(str).str.strip()\n",
    "    df = df.dropna(subset=[\"prompt\", \"label\"])\n",
    "    df = df[df[\"prompt\"].str.len() > 0]\n",
    "    if df.empty:\n",
    "        sys.exit(\"[train] No non-empty prompts after cleaning.\")\n",
    "    \n",
    "    X = df[\"prompt\"].tolist()\n",
    "    y = df[\"label\"].tolist()\n",
    "    \n",
    "    # Base classifier\n",
    "    base = LogisticRegression(max_iter=int(args.max_iter), C=float(args.C), class_weight=\"balanced\", random_state=0)\n",
    "    \n",
    "    # Calibration if requested\n",
    "    model = base\n",
    "    if args.calibrate:\n",
    "        cnt = Counter(y)\n",
    "        m = min(cnt.values())\n",
    "    \n",
    "        method = args.calibration_method\n",
    "        cv = args.calibration_cv\n",
    "        if method == \"auto\":\n",
    "            if m >= max(3, cv):\n",
    "                method, cv = \"isotonic\", max(3, cv)\n",
    "            elif m >= 2:\n",
    "                method, cv = \"sigmoid\", max(2, min(m, cv))\n",
    "            else:\n",
    "                print(\"[train] Not enough samples per class for calibration; skipping.\", file=sys.stderr)\n",
    "        if method in (\"isotonic\", \"sigmoid\"):\n",
    "            try:\n",
    "                model = CalibratedClassifierCV(estimator=base, method=method, cv=cv)\n",
    "            except TypeError:\n",
    "                model = CalibratedClassifierCV(base_estimator=base, method=method, cv=cv)\n",
    "    \n",
    "        # Pipeline\n",
    "        pipe = make_pipeline(SBERTEncoder(args.sbert), model)\n",
    "        pipe.fit(X, y)\n",
    "    \n",
    "        out_path = os.path.join(args.out_dir, args.out_model)\n",
    "        joblib.dump(pipe, out_path)\n",
    "        print(f\"[train] Wrote mapper to {out_path} (n={len(X)})\")\n",
    "    return pipe, out_path\n",
    "\n",
    "# -------------------------- I/O Helpers (from defi_milestone8.py) --------------------------\n",
    "\n",
    "def read_prompts_jsonl(path: str) -> List[str]:\n",
    "    prompts: List[str] = []\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(path)\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                rec = json.loads(line)\n",
    "                p = rec.get(\"prompt\", \"\").strip()\n",
    "                if p:\n",
    "                    prompts.append(p)\n",
    "            except Exception:\n",
    "                continue\n",
    "    return prompts\n",
    "\n",
    "def read_labels_csv(path: str) -> Dict[str, str]:\n",
    "    gold: Dict[str, str] = {}\n",
    "    if not path or not os.path.exists(path):\n",
    "        return gold\n",
    "    with open(path, newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for rec in reader:\n",
    "            p = str(rec.get(\"prompt\", \"\")).strip()\n",
    "            y = str(rec.get(\"label\", \"\")).strip()\n",
    "            if p:\n",
    "                gold[p] = y\n",
    "    return gold\n",
    "\n",
    "# -------------------------- Scoring & Metrics (simplified from defi_milestone8.py) --------------------------\n",
    "\n",
    "def mapper_predict_scores(mapper, prompts: List[str], class_names: List[str]) -> List[Dict[str, float]]:\n",
    "    import numpy as np\n",
    "    scores: List[Dict[str, float]] = []\n",
    "    if hasattr(mapper, \"predict_proba\"):\n",
    "        probs = mapper.predict_proba(prompts)\n",
    "        classes = list(getattr(mapper, \"classes_\", class_names))\n",
    "        for row in probs:\n",
    "            row_map = {str(c): float(p) for c, p in zip(classes, row)}\n",
    "            for cname in class_names:\n",
    "                row_map.setdefault(cname, 0.0)\n",
    "            scores.append(row_map)\n",
    "        return scores\n",
    "    # Fallback to predict (simplified, no decision_function for brevity)\n",
    "    preds = mapper.predict(prompts)\n",
    "    for y in preds:\n",
    "        row_map = {c: 0.0 for c in class_names}\n",
    "        row_map[str(y)] = 1.0\n",
    "        scores.append(row_map)\n",
    "    return scores\n",
    "\n",
    "def metrics_for_threshold(\n",
    "    prompts: List[str],\n",
    "    scores: List[Dict[str, float]],\n",
    "    class_names: List[str],\n",
    "    thr: float,\n",
    "    gold: Dict[str, str],\n",
    ") -> Dict[str, Any]:\n",
    "    total = len(prompts)\n",
    "    abstain = 0\n",
    "    correct = 0\n",
    "    fired = 0\n",
    "    for p, smap in zip(prompts, scores):\n",
    "        top = max(class_names, key=lambda c: smap.get(c, 0.0))\n",
    "        score = smap.get(top, 0.0)\n",
    "        if score < thr:\n",
    "            abstain += 1\n",
    "        else:\n",
    "            fired += 1\n",
    "            if p in gold and gold[p] == top:\n",
    "                correct += 1\n",
    "    abstain_rate = abstain / max(1, total)\n",
    "    coverage = fired / max(1, total)\n",
    "    acc_on_fired = (correct / max(1, fired)) if fired else None\n",
    "    overall_acc = correct / max(1, total)\n",
    "    return {\n",
    "        \"threshold\": thr,\n",
    "        \"total\": total,\n",
    "        \"abstain\": abstain,\n",
    "        \"abstain_rate\": abstain_rate,\n",
    "        \"coverage\": coverage,\n",
    "        \"fired\": fired,\n",
    "        \"correct_on_fired\": correct,\n",
    "        \"accuracy_on_fired\": acc_on_fired,\n",
    "        \"overall_correct\": correct,\n",
    "        \"overall_accuracy\": overall_acc,\n",
    "    }\n",
    "\n",
    "def write_rows_csv(path: str, prompts, scores, class_names, thr, gold):\n",
    "    with open(path, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"prompt\", \"gold_label\", \"predicted\", \"confidence\", \"abstain\", \"threshold\"])\n",
    "        for p, smap in zip(prompts, scores):\n",
    "            top = max(class_names, key=lambda c: smap.get(c, 0.0))\n",
    "            conf = float(smap.get(top, 0.0))\n",
    "            fire = conf >= thr\n",
    "            pred = top if fire else \"\"\n",
    "            w.writerow([p, gold.get(p, \"\"), pred, f\"{conf:.4f}\", (not fire), thr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1455b94c-cbe1-4532-96a2-6cee5b1ad216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e3e8773-8595-4f6b-aad9-70d72ebbc291",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # -------------------------- Main --------------------------\n",
    "\n",
    "# def main():\n",
    "#     ap = argparse.ArgumentParser(description=\"End-to-end DeFi Mapper Training & Evaluation\")\n",
    "#     # Training args\n",
    "#     ap.add_argument(\"--train_labels_csv\", required=True, help=\"CSV with prompt,label for training\")\n",
    "#     ap.add_argument(\"--sbert\", default=\"sentence-transformers/all-MiniLM-L6-v2\", help=\"SentenceTransformer model\")\n",
    "#     ap.add_argument(\"--C\", type=float, default=8.0, help=\"LogisticRegression C\")\n",
    "#     ap.add_argument(\"--max_iter\", type=int, default=2000, help=\"LogisticRegression max_iter\")\n",
    "#     ap.add_argument(\"--calibrate\", action=\"store_true\", help=\"Calibrate probabilities\")\n",
    "#     ap.add_argument(\"--calibration_method\", choices=[\"auto\", \"isotonic\", \"sigmoid\"], default=\"auto\")\n",
    "#     ap.add_argument(\"--calibration_cv\", type=int, default=3)\n",
    "#     # Evaluation args\n",
    "#     ap.add_argument(\"--prompts_jsonl\", required=True, help=\"JSONL with prompts for evaluation\")\n",
    "#     ap.add_argument(\"--eval_labels_csv\", default=\"\", help=\"Optional CSV with labels for evaluation\")\n",
    "#     ap.add_argument(\"--class_names\", type=str, default=\",\".join(DEFAULT_CLASSES), help=\"Comma-separated class names\")\n",
    "#     ap.add_argument(\"--thresholds\", type=str, default=\"0.5,0.6,0.7,0.8,0.9\", help=\"Comma-separated thresholds\")\n",
    "#     ap.add_argument(\"--max_abstain_rate\", type=float, default=0.10)\n",
    "#     ap.add_argument(\"--min_overall_acc\", type=float, default=0.85)\n",
    "#     ap.add_argument(\"--choose_by\", type=str, default=\"abstain_then_acc\", choices=[\"abstain_then_acc\", \"utility\"])\n",
    "#     # Output args\n",
    "#     ap.add_argument(\"--out_dir\", default=\".artifacts\", help=\"Output directory\")\n",
    "#     ap.add_argument(\"--out_model\", default=\"defi_mapper_embed.joblib\", help=\"Trained model filename\")\n",
    "#     ap.add_argument(\"--out_summary\", default=\"defi_milestone8_summary.json\", help=\"Summary JSON\")\n",
    "#     ap.add_argument(\"--out_csv\", default=\"defi_milestone8_metrics.csv\", help=\"Metrics CSV\")\n",
    "#     ap.add_argument(\"--out_rows_csv\", default=\"defi_milestone8_rows.csv\", help=\"Per-prompt rows CSV\")\n",
    "#     args = ap.parse_args()\n",
    "\n",
    "#     # Ensure output dir\n",
    "#     os.makedirs(args.out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3d87acdb-c9a8-4895-8ef8-a1d6ea307a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--backend\",       default=\"wordmap\", help=\"wordmap|sbert\")\n",
    "    ap.add_argument(\"--model_path\",    default=\".artifacts/defi_mapper.joblib\")\n",
    "    ap.add_argument(\"--prompts_jsonl\", default=\"tests/fixtures/defi/defi_mapper_5k_prompts.json\")\n",
    "    ap.add_argument(\"--test_labels_csv\", default=\"tests/fixtures/defi/defi_mapper_labeled_5k.csv\")\n",
    "    ap.add_argument(\"--thresholds\",    default=\"0.5,0.55,0.6,0.65,0.7\")\n",
    "    ap.add_argument(\"--max_iter\",    default=\"2000\")\n",
    "    ap.add_argument(\"--C\",    default=\"8\")\n",
    "    ap.add_argument(\"--calibrate\",    default=\"True\")\n",
    "    ap.add_argument(\"--calibration_method\", choices=[\"auto\",\"isotonic\",\"sigmoid\"], default=\"auto\")\n",
    "    ap.add_argument(\"--calibration_cv\", type=int, default=3)\n",
    "    ap.add_argument(\"--train_labels_csv\",    default=\"tests/fixtures/defi/defi_mapper_labeled_large.csv\")\n",
    "    ap.add_argument(\"--sbert\", default=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    ap.add_argument(\"--out_model\", default=\"defi_mapper_embed.joblib\")\n",
    "    ap.add_argument(\"--out_dir\",       default=\"\")\n",
    "    ap.add_argument(\"--min_overall_acc\", default=None)\n",
    "    \n",
    "    notebook_args = [\n",
    "        \"--backend\", \"sbert\",\n",
    "        \"--model_path\", \".artifacts/defi_mapper.joblib\",\n",
    "        \"--prompts_jsonl\", \"tests/fixtures/defi/defi_mapper_5k_prompts.jsonl\",\n",
    "        \"--test_labels_csv\",    \"tests/fixtures/defi/defi_mapper_labeled_5k.csv\",\n",
    "        \"--train_labels_csv\", \"tests/fixtures/defi/defi_mapper_labeled_large.csv\",\n",
    "        \"--thresholds\", \"0.5,0.55,0.6,0.65,0.7\",\n",
    "        \"--max_iter\", \"2000\",\n",
    "        \"--C\", \"8\",\n",
    "        \"--calibrate\", \"True\",\n",
    "        \"--calibration_method\", \"auto\",\n",
    "        \"--calibration_cv\", \"3\",\n",
    "        \"--min_overall_acc\", \"0.75\",\n",
    "        \"--min_overall_acc\", \"0.75\",\n",
    "        \"--sbert\", \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        \"--out_model\", \"defi_mapper_embed.joblib\",\n",
    "        \"--out_dir\", \".artifacts/defi/mapper_bench\",\n",
    "    ]\n",
    "    \n",
    "    return ap.parse_args(notebook_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e1b1807c-1b1d-47d5-a87c-6756f7e89e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = ['claim_rewards','withdraw_asset','stake_asset','deposit_asset','swap_asset','repay_asset','unstake_asset','repay_asset'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "595c3ea0-8add-4145-8919-2c541464f333",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Wrote mapper to .artifacts/defi/mapper_bench/defi_mapper_embed.joblib (n=1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "args = get_args()\n",
    "mapper, model_path = train_mapper(args)\n",
    "\n",
    "# Load eval data\n",
    "class_names = CLASS_NAMES\n",
    "prompts = read_prompts_jsonl(args.prompts_jsonl)\n",
    "if not prompts:\n",
    "    sys.exit(\"No prompts loaded.\")\n",
    "gold = read_labels_csv(args.labels_csv)\n",
    "scores = mapper_predict_scores(mapper, prompts, class_names)\n",
    "thr_list = [float(x.strip()) for x in args.thresholds.split(\",\") if x.strip()]\n",
    "metrics = [metrics_for_threshold(prompts, scores, class_names, thr, gold) for thr in thr_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7ca6ebfe-d6b2-4860-8774-0aaecea01154",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5e91dc9d-9225-427d-a80f-cbedc2441b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Wrote mapper to .artifacts/defi/mapper_bench/defi_mapper_embed.joblib (n=1000)\n"
     ]
    }
   ],
   "source": [
    "args = get_args()\n",
    "\n",
    "df = pd.read_csv(args.train_labels_csv)\n",
    "need = {\"prompt\", \"label\"}\n",
    "if not need.issubset(df.columns):\n",
    "    sys.exit(f\"[train] labels_csv must have columns {need}, got {df.columns.tolist()}\")\n",
    "df = df.copy()\n",
    "df[\"prompt\"] = df[\"prompt\"].astype(str).str.strip()\n",
    "df[\"label\"] = df[\"label\"].astype(str).str.strip()\n",
    "df = df.dropna(subset=[\"prompt\", \"label\"])\n",
    "df = df[df[\"prompt\"].str.len() > 0]\n",
    "if df.empty:\n",
    "    sys.exit(\"[train] No non-empty prompts after cleaning.\")\n",
    "    \n",
    "X = df[\"prompt\"].tolist()\n",
    "y = df[\"label\"].tolist()\n",
    "\n",
    "# Base classifier\n",
    "base = LogisticRegression(max_iter=int(args.max_iter), C=float(args.C), class_weight=\"balanced\", random_state=0)\n",
    "\n",
    "# Tiny-data friendly calibration\n",
    "model = base\n",
    "if args.calibrate:\n",
    "    cnt = Counter(y); m = min(cnt.values())\n",
    "    method = args.calibration_method; cv = args.calibration_cv\n",
    "    if method == \"auto\":\n",
    "        if m >= max(3, cv):\n",
    "            method, cv = \"isotonic\", max(3, cv)\n",
    "        elif m >= 2:\n",
    "            method, cv = \"sigmoid\", max(2, min(m, cv))\n",
    "        else:\n",
    "            print(\"[train_mapper_embed] Not enough samples per class for calibration; skipping.\", file=sys.stderr)\n",
    "    if method in (\"isotonic\",\"sigmoid\"):\n",
    "        try:\n",
    "            # sklearn >= 1.3 uses 'estimator'\n",
    "            model = CalibratedClassifierCV(estimator=base, method=method, cv=cv)\n",
    "        except TypeError:\n",
    "            # older sklearn used 'base_estimator'\n",
    "            model = CalibratedClassifierCV(base_estimator=base, method=method, cv=cv)\n",
    "    \n",
    "\n",
    "pipe = make_pipeline(SBERTEncoder(args.sbert), model)\n",
    "pipe.fit(X, y)\n",
    "\n",
    "out_path = os.path.join(args.out_dir, args.out_model)\n",
    "joblib.dump(pipe, out_path)\n",
    "print(f\"[train] Wrote mapper to {out_path} (n={len(X)})\")\n",
    "\n",
    "mapper, model_path = pipe, out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a5685523-bcf5-49e1-8c53-b3eb3e1feb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Load eval data\n",
    "class_names = CLASS_NAMES\n",
    "prompts = read_prompts_jsonl(args.prompts_jsonl)\n",
    "if not prompts:\n",
    "    sys.exit(\"No prompts loaded.\")\n",
    "gold = read_labels_csv(args.test_labels_csv)\n",
    "scores = mapper_predict_scores(mapper, prompts, class_names)\n",
    "thr_list = [float(x.strip()) for x in args.thresholds.split(\",\") if x.strip()]\n",
    "metrics = [metrics_for_threshold(prompts, scores, class_names, thr, gold) for thr in thr_list]\n",
    "#metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b35a2673-0b86-4255-b592-f53be31971b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['claim staking rewards on aave base',\n",
       " 'pull out 2752.8264 ARB from lido',\n",
       " 'restake 33.8529 MATIC with balancer on solana â€” this minute, use normal gas',\n",
       " 'deposit 4697 USDC into uniswap on base',\n",
       " 'supply 7.0245 SOL to maker']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76b7943-e019-455d-86a0-a981d3a85192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "    scores = mapper_predict_scores(mapper, prompts, class_names)\n",
    "\n",
    "    # Sweep thresholds\n",
    "    thr_list = [float(x.strip()) for x in args.thresholds.split(\",\") if x.strip()]\n",
    "    metrics = [metrics_for_threshold(prompts, scores, class_names, thr, gold) for thr in thr_list]\n",
    "\n",
    "    # Choose operating point (simplified: highest thr with abstain_rate <= max_abstain_rate)\n",
    "    admissible = [m for m in metrics if m[\"abstain_rate\"] <= args.max_abstain_rate]\n",
    "    if admissible:\n",
    "        admissible.sort(key=lambda m: m[\"threshold\"], reverse=True)\n",
    "        chosen = admissible[0]\n",
    "    else:\n",
    "        metrics.sort(key=lambda m: m[\"abstain_rate\"])\n",
    "        chosen = metrics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a64ba11-434b-4f6a-af45-23507f5d5ef7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Namespace' object has no attribute 'train_labels_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m args \u001b[38;5;241m=\u001b[39m get_args()\n\u001b[0;32m----> 3\u001b[0m mapper, model_path \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_mapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load eval data\u001b[39;00m\n\u001b[1;32m      6\u001b[0m class_names \u001b[38;5;241m=\u001b[39m [c\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39mclass_names\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m c\u001b[38;5;241m.\u001b[39mstrip()]\n",
      "Cell \u001b[0;32mIn[2], line 32\u001b[0m, in \u001b[0;36mtrain_mapper\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_mapper\u001b[39m(args):\n\u001b[0;32m---> 32\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_labels_csv\u001b[49m)\n\u001b[1;32m     33\u001b[0m     need \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m need\u001b[38;5;241m.\u001b[39missubset(df\u001b[38;5;241m.\u001b[39mcolumns):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Namespace' object has no attribute 'train_labels_csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load eval data\n",
    "class_names = [c.strip() for c in args.class_names.split(\",\") if c.strip()]\n",
    "prompts = read_prompts_jsonl(args.prompts_jsonl)\n",
    "if not prompts:\n",
    "    sys.exit(\"No prompts loaded.\")\n",
    "gold = read_labels_csv(args.eval_labels_csv)\n",
    "\n",
    "# Score\n",
    "scores = mapper_predict_scores(mapper, prompts, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8ea125-4cc4-4542-8cb5-fbcef44d9fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Score\n",
    "    scores = mapper_predict_scores(mapper, prompts, class_names)\n",
    "\n",
    "    # Sweep thresholds\n",
    "    thr_list = [float(x.strip()) for x in args.thresholds.split(\",\") if x.strip()]\n",
    "    metrics = [metrics_for_threshold(prompts, scores, class_names, thr, gold) for thr in thr_list]\n",
    "\n",
    "    # Choose operating point (simplified: highest thr with abstain_rate <= max_abstain_rate)\n",
    "    admissible = [m for m in metrics if m[\"abstain_rate\"] <= args.max_abstain_rate]\n",
    "    if admissible:\n",
    "        admissible.sort(key=lambda m: m[\"threshold\"], reverse=True)\n",
    "        chosen = admissible[0]\n",
    "    else:\n",
    "        metrics.sort(key=lambda m: m[\"abstain_rate\"])\n",
    "        chosen = metrics[0]\n",
    "\n",
    "    # Status\n",
    "    has_labels = bool(gold)\n",
    "    pass_abstain = (chosen[\"abstain_rate\"] <= args.max_abstain_rate)\n",
    "    pass_accuracy = True if not has_labels else (chosen[\"overall_accuracy\"] >= args.min_overall_acc)\n",
    "    status = \"pass\" if (pass_abstain and pass_accuracy) else \"fail\"\n",
    "\n",
    "    # Write summary\n",
    "    summary = {\n",
    "        \"ok\": True,\n",
    "        \"timestamp\": int(time.time()),\n",
    "        \"prompts\": len(prompts),\n",
    "        \"has_labels\": has_labels,\n",
    "        \"thresholds\": thr_list,\n",
    "        \"metrics\": metrics,\n",
    "        \"chosen\": {\n",
    "            \"threshold\": chosen[\"threshold\"],\n",
    "            \"abstain_rate\": chosen[\"abstain_rate\"],\n",
    "            \"coverage\": chosen[\"coverage\"],\n",
    "            \"accuracy_on_fired\": chosen.get(\"accuracy_on_fired\"),\n",
    "            \"overall_accuracy\": chosen.get(\"overall_accuracy\"),\n",
    "        },\n",
    "        \"status\": status,\n",
    "    }\n",
    "    out_summary = os.path.join(args.out_dir, args.out_summary)\n",
    "    with open(out_summary, \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    # Write metrics CSV\n",
    "    out_csv = os.path.join(args.out_dir, args.out_csv)\n",
    "    with open(out_csv, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        header = [\"threshold\", \"total\", \"abstain\", \"abstain_rate\", \"coverage\", \"fired\", \"correct_on_fired\", \"accuracy_on_fired\", \"overall_correct\", \"overall_accuracy\"]\n",
    "        writer.writerow(header)\n",
    "        for m in metrics:\n",
    "            writer.writerow([m.get(h) for h in header])\n",
    "\n",
    "    # Write rows CSV at chosen threshold\n",
    "    out_rows_csv = os.path.join(args.out_dir, args.out_rows_csv)\n",
    "    write_rows_csv(out_rows_csv, prompts, scores, class_names, chosen[\"threshold\"], gold)\n",
    "\n",
    "    # Print summary\n",
    "    print(json.dumps({\"ok\": True, \"model\": model_path, \"summary\": out_summary, \"csv\": out_csv, \"rows_csv\": out_rows_csv, \"status\": status}, indent=2))\n",
    "\n",
    "    # Train\n",
    "    mapper, model_path = train_mapper(args)\n",
    "\n",
    "    # Load eval data\n",
    "    class_names = [c.strip() for c in args.class_names.split(\",\") if c.strip()]\n",
    "    prompts = read_prompts_jsonl(args.prompts_jsonl)\n",
    "    if not prompts:\n",
    "        sys.exit(\"No prompts loaded.\")\n",
    "    gold = read_labels_csv(args.eval_labels_csv)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
