{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40abe650-86e1-488a-a40c-1b33c72e1997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd =  os.getcwd().replace(\"notebooks/research\",\"\")\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d56a8575-f1ef-4e45-ad1a-03cd75becaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook cell: Build ARC Stage-11 priors from grid↔label CSV\n",
    "# Input   : /path/to/arc_grid_mapper_labeled.csv  with columns: grid,label\n",
    "# Outputs : stage11_funnel_priors_arc.npz  (proto, sigma, labels, whitener)\n",
    "#           arc_mapper_lr_whitened.joblib  (optional quick sanity-check mapper)\n",
    "\n",
    "import ast, json, numpy as np, pandas as pd, joblib, os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Tuple, Dict, Any\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "CSV_PATH = \"tests/fixtures/arc/arc_grid_mapper_labeled.csv\"   # <— adjust if needed\n",
    "OUT_PRIORS = \".artifacts/stage11_funnel_priors_arc.npz\"\n",
    "OUT_MAPPER = \".artifacts/arc_mapper_lr_whitened.joblib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "216c66ab-63a4-4117-b571-dbf511406965",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, Optional, Tuple, List\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# ----------------------------\n",
    "# Primitives / shared utils\n",
    "# ----------------------------\n",
    "PRIMS = [\"flip_h\",\"flip_v\",\"rotate\"]\n",
    "\n",
    "@dataclass\n",
    "class WellParams:\n",
    "    whiten: bool = True\n",
    "    tau: float = 0.25\n",
    "    isotropize_xy: bool = True\n",
    "    sigma_scale: float = 0.80\n",
    "    depth_scale: float = 1.35\n",
    "    mix_z: float = 0.12\n",
    "    inhibit_k: int = 10\n",
    "    inhibit_strength: float = 0.55\n",
    "    point_alpha: float = 0.85\n",
    "    trisurf_alpha: float = 0.65\n",
    "\n",
    "def build_H_E_from_traces(args) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    rng = np.random.default_rng(args.seed)\n",
    "    H_rows, E_vals = [], []\n",
    "    for _ in range(args.samples):\n",
    "        traces, _ = make_synthetic_traces(\n",
    "            rng, T=args.T, noise=args.noise, cm_amp=args.cm_amp,\n",
    "            overlap=args.overlap, amp_jitter=args.amp_jitter, distractor_prob=args.distractor_prob,\n",
    "            tasks_k=(args.min_tasks, args.max_tasks)\n",
    "        )\n",
    "        E_perp = perpendicular_energy(traces)\n",
    "        S = {p: moving_average(E_perp[p], k=args.sigma) for p in PRIMS}\n",
    "        feats = np.concatenate([_z(S[p]) for p in PRIMS], axis=0)\n",
    "        H_rows.append(feats)\n",
    "        E_vals.append(float(sum(np.trapz(S[p]) for p in PRIMS)))\n",
    "    H = np.vstack(H_rows)\n",
    "    E = np.asarray(E_vals, float)\n",
    "    E = (E - E.min()) / (E.ptp() + 1e-9)\n",
    "    return H, E\n",
    "\n",
    "def _phantom_metrics(X2: np.ndarray, z: np.ndarray) -> Dict[str, float]:\n",
    "    nb = max(12, int(np.sqrt(len(X2)) / 2))\n",
    "    xi = np.digitize(X2[:,0], np.linspace(X2[:,0].min(), X2[:,0].max(), nb))\n",
    "    yi = np.digitize(X2[:,1], np.linspace(X2[:,1].min(), X2[:,1].max(), nb))\n",
    "    grid_min = {}\n",
    "    for i in range(len(X2)):\n",
    "        key = (xi[i], yi[i])\n",
    "        grid_min[key] = min(grid_min.get(key, np.inf), z[i])\n",
    "    mins = sorted(grid_min.values())\n",
    "    if len(mins) < 2:\n",
    "        return {\"phantom_index\": 0.0, \"margin\": 0.0}\n",
    "    z0, z1 = mins[0], mins[1]\n",
    "    span = np.percentile(z, 95) - np.percentile(z, 5) + 1e-9\n",
    "    phantom_index = (z1 - z0) / span\n",
    "    margin = z1 - z0\n",
    "    return {\"phantom_index\": float(phantom_index), \"margin\": float(margin)}\n",
    "\n",
    "def pca3_and_warp(H: np.ndarray,\n",
    "                  energy: Optional[np.ndarray] = None,\n",
    "                  params: WellParams = WellParams()):\n",
    "    if PCA is None:\n",
    "        raise RuntimeError(\"scikit-learn not available; PCA required for manifold warp\")\n",
    "    pca = PCA(n_components=3, whiten=params.whiten, random_state=0)\n",
    "    X3 = pca.fit_transform(H)\n",
    "    X2 = X3[:, :2]\n",
    "    z  = X3[:, 2].copy()\n",
    "\n",
    "    c, _ = _softmin_center(X2, energy, params.tau)\n",
    "\n",
    "    if params.isotropize_xy:\n",
    "        X2_iso, _ = _isotropize(X2 - c)\n",
    "    else:\n",
    "        X2_iso = X2 - c\n",
    "\n",
    "    r = np.linalg.norm(X2_iso, axis=1)\n",
    "    sigma = np.median(r) * params.sigma_scale + 1e-9\n",
    "    X2_new, z_new = _radial_funnel(X2_iso, z, sigma, params.depth_scale, params.mix_z)\n",
    "    z_new = _lateral_inhibition(z_new, X2_new, k=params.inhibit_k, strength=params.inhibit_strength)\n",
    "\n",
    "    metrics = _phantom_metrics(X2_new, z_new)\n",
    "    out = np.column_stack([X2_new + c, z_new])\n",
    "    return out, metrics, dict(center=c, sigma=sigma)\n",
    "\n",
    "def analytic_core_template(r_grid: np.ndarray, D: float, p: float, r0_frac: float) -> np.ndarray:\n",
    "    r_max = float(r_grid[-1] + 1e-12)\n",
    "    r0 = r0_frac * r_max\n",
    "    invp = 1.0 / (np.sqrt(r_grid**2 + r0**2) + 1e-12)**p\n",
    "    invR = 1.0 / (np.sqrt(r_max**2 + r0**2) + 1e-12)**p\n",
    "    return -D * (invp - invR)\n",
    "\n",
    "def blend_profiles(z_data: np.ndarray, z_template: np.ndarray, alpha: float) -> np.ndarray:\n",
    "    alpha = np.clip(alpha, 0.0, 1.0)\n",
    "    return (1.0 - alpha) * z_data + alpha * z_template\n",
    "\n",
    "def priors_from_profile(r_grid: np.ndarray, z_prof: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "    phi_raw = (z_prof[-1] - z_prof)\n",
    "    phi = phi_raw / (phi_raw.max() + 1e-12)\n",
    "    dz = np.gradient(z_prof, r_grid + 1e-12)\n",
    "    g_raw = np.maximum(0.0, -dz)\n",
    "    g = g_raw / (g_raw.max() + 1e-12)\n",
    "    r_norm = r_grid / (r_grid[-1] + 1e-12)\n",
    "    return dict(r=r_norm, phi=phi, g=g)\n",
    "\n",
    "def gaussian_bump(T, center, width, amp=1.0):\n",
    "    t = np.arange(T)\n",
    "    sig2 = (width/2.355)**2  # FWHM→σ\n",
    "    return amp * np.exp(-(t-center)**2 / (2*sig2))\n",
    "\n",
    "def perpendicular_energy(traces: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "    mu = common_mode(traces)\n",
    "    return {p: np.clip(traces[p] - mu, 0, None) for p in PRIMS}\n",
    "\n",
    "def common_mode(traces: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "    return np.stack([traces[p] for p in PRIMS], 0).mean(0)\n",
    "\n",
    "def _z(x: np.ndarray) -> np.ndarray:\n",
    "    x = np.asarray(x, float)\n",
    "    return (x - x.mean()) / (x.std() + 1e-8)\n",
    "\n",
    "def _softmin_center(X2: np.ndarray, energy: Optional[np.ndarray], tau: float):\n",
    "    n = len(X2)\n",
    "    if energy is None:\n",
    "        w = np.ones(n) / n\n",
    "    else:\n",
    "        e = (energy - energy.min()) / (energy.std() + 1e-8)\n",
    "        w = np.exp(-e / max(tau, 1e-6))\n",
    "        w = w / (w.sum() + 1e-12)\n",
    "    c = (w[:, None] * X2).sum(axis=0)\n",
    "    return c, w\n",
    "\n",
    "def _isotropize(X2: np.ndarray):\n",
    "    mu = X2.mean(axis=0)\n",
    "    Y = X2 - mu\n",
    "    C = (Y.T @ Y) / max(len(Y)-1, 1)\n",
    "    evals, evecs = np.linalg.eigh(C)\n",
    "    T = evecs @ np.diag(1.0 / np.sqrt(np.maximum(evals, 1e-8))) @ evecs.T\n",
    "    return (Y @ T), (mu, T)\n",
    "\n",
    "def _radial_funnel(X2_iso: np.ndarray, z: np.ndarray, sigma: float, depth_scale: float, mix_z: float):\n",
    "    r = np.linalg.norm(X2_iso, axis=1) + 1e-9\n",
    "    u = X2_iso / r[:, None]\n",
    "    z_funnel = -np.exp(-(r**2) / (2 * sigma**2))  # [-1,0]\n",
    "    z_new = depth_scale * z_funnel + mix_z * (z - z.mean())\n",
    "    X2_new = (r[:, None] * u)\n",
    "    return X2_new, z_new\n",
    "\n",
    "def _lateral_inhibition(z: np.ndarray, X2: np.ndarray, k:int, strength: float) -> np.ndarray:\n",
    "    if NearestNeighbors is None:\n",
    "        return z\n",
    "    k = min(max(3, k), len(X2))\n",
    "    nbrs = NearestNeighbors(n_neighbors=k).fit(X2)\n",
    "    idx = nbrs.kneighbors(return_distance=False)\n",
    "    ranks = np.argsort(np.argsort(z[idx], axis=1), axis=1)[:,0]\n",
    "    boost = (ranks > 0).astype(float)\n",
    "    z_adj = z + strength * 0.5 * (boost - 0.5) * (np.std(z) + 1e-6)\n",
    "    return z_adj\n",
    "\n",
    "def fit_radial_profile(X3: np.ndarray, center: np.ndarray, r_grid: np.ndarray,\n",
    "                       h: float, q: float, r0_frac: float,\n",
    "                       core_k: float, core_p: float) -> np.ndarray:\n",
    "    x, y, z = X3[:,0], X3[:,1], X3[:,2]\n",
    "    r = np.linalg.norm(np.c_[x-center[0], y-center[1]], axis=1)\n",
    "    z_fit = np.zeros_like(r_grid)\n",
    "    for i, rg in enumerate(r_grid):\n",
    "        w = np.exp(-((r - rg)**2) / (2*h*h + 1e-12))\n",
    "        if w.sum() < 1e-8:\n",
    "            idx = np.argsort(np.abs(r - rg))[:8]\n",
    "            z_fit[i] = float(np.median(z[idx]))\n",
    "        else:\n",
    "            z_fit[i] = weighted_quantile(z, w, q)\n",
    "    last = z_fit[-1]\n",
    "    for i in range(len(z_fit)-2, -1, -1):\n",
    "        if z_fit[i] > last:\n",
    "            z_fit[i] = last\n",
    "        else:\n",
    "            last = z_fit[i]\n",
    "    r_max = float(r_grid[-1] + 1e-12)\n",
    "    r0 = r0_frac * r_max\n",
    "    core = core_k * (1.0 / (np.sqrt(r_grid**2 + r0**2) + 1e-12)**core_p)\n",
    "    core -= core[-1]\n",
    "    return z_fit - core\n",
    "\n",
    "def weighted_quantile(values: np.ndarray, weights: np.ndarray, q: float) -> float:\n",
    "    values = np.asarray(values, float)\n",
    "    weights = np.asarray(weights, float)\n",
    "    if values.size == 0:\n",
    "        return float(\"nan\")\n",
    "    idx = np.argsort(values)                 # <-- fix: sort by values to get indices\n",
    "    v, w = values[idx], weights[idx]\n",
    "    cum = np.cumsum(w)\n",
    "    if cum[-1] <= 0:\n",
    "        return float(np.median(v))\n",
    "    t = q * cum[-1]\n",
    "    j = int(np.searchsorted(cum, t, side=\"left\"))\n",
    "    j = min(max(j, 0), len(v) - 1)\n",
    "    return float(v[j])\n",
    "\n",
    "def moving_average(x, k=9):\n",
    "    if k <= 1: return x.copy()\n",
    "    pad = k // 2\n",
    "    xp = np.pad(x, (pad, pad), mode=\"reflect\")\n",
    "    return np.convolve(xp, np.ones(k)/k, mode=\"valid\")\n",
    "\n",
    "def make_synthetic_traces(rng, T=720, noise=0.02, cm_amp=0.02, overlap=0.5,\n",
    "                          amp_jitter=0.4, distractor_prob=0.4,\n",
    "                          tasks_k: Tuple[int,int]=(1,3)) -> Tuple[Dict[str,np.ndarray], List[str]]:\n",
    "    k = int(rng.integers(tasks_k[0], tasks_k[1]+1))\n",
    "    tasks = list(rng.choice(PRIMS, size=k, replace=False))\n",
    "    rng.shuffle(tasks)\n",
    "    base = np.array([0.20, 0.50, 0.80]) * T\n",
    "    centers = ((1.0 - overlap) * base + overlap * (T * 0.50)).astype(int)\n",
    "    width = int(max(12, T * 0.10))\n",
    "    t = np.arange(T)\n",
    "    cm = cm_amp * (1.0 + 0.2 * np.sin(2*np.pi * t / max(30, T//6)))\n",
    "\n",
    "    traces = {p: np.zeros(T, float) for p in PRIMS}\n",
    "    for i, prim in enumerate(tasks):\n",
    "        c = centers[i % len(centers)]\n",
    "        amp = max(0.25, 1.0 + rng.normal(0, amp_jitter))\n",
    "        c_jit = int(np.clip(c + rng.integers(-width//5, width//5 + 1), 0, T-1))\n",
    "        traces[prim] += gaussian_bump(T, c_jit, width, amp=amp)\n",
    "\n",
    "    for p in PRIMS:\n",
    "        if p not in tasks and rng.random() < distractor_prob:\n",
    "            c = int(rng.uniform(T*0.15, T*0.85))\n",
    "            amp = max(0.25, 1.0 + rng.normal(0, amp_jitter))\n",
    "            traces[p] += gaussian_bump(T, c, width, amp=0.9*amp)\n",
    "\n",
    "    for p in PRIMS:\n",
    "        traces[p] = np.clip(traces[p] + cm, 0, None)\n",
    "        traces[p] = traces[p] + rng.normal(0, noise, size=T)\n",
    "        traces[p] = np.clip(traces[p], 0, None)\n",
    "\n",
    "    return traces, tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "940ccd57-c53f-4302-97cb-601c1ff4e5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Protocol-true Stage-11 priors for ARC: writes .artifacts/arc_stage11_priors.npz\n",
    "# Uses the same method as stage11_benchmark_latest.py (r, phi, g profile).\n",
    "\n",
    "import os, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Import the Stage-11 reference functions from your repo\n",
    "import importlib.util, types\n",
    "\n",
    "# SRC = \"notebooks/research/stage11_benchmark_latest.py\"   # path in your repo (adjust if needed)\n",
    "# spec = importlib.util.spec_from_file_location(\"s11\", SRC)\n",
    "# s11 = importlib.util.module_from_spec(spec)\n",
    "# spec.loader.exec_module(s11)\n",
    "\n",
    "# 2) Make (or load) an H/E cloud to fit the single-well + radial profile\n",
    "#    Option A (quick): synthetic ARC-like traces (protocol-identical, fast)\n",
    "class Args: pass\n",
    "args = Args()\n",
    "args.samples = 600        # more samples → smoother profile\n",
    "args.seed = 42\n",
    "args.T = 720\n",
    "args.noise = 0.02\n",
    "args.cm_amp = 0.02\n",
    "args.overlap = 0.5\n",
    "args.amp_jitter = 0.4\n",
    "args.distractor_prob = 0.4\n",
    "args.min_tasks = 1\n",
    "args.max_tasks = 3\n",
    "args.sigma = 9            # residual-energy smoother window\n",
    "\n",
    "# Build H/E exactly like Stage-11 does\n",
    "H, E = build_H_E_from_traces(args)\n",
    "\n",
    "# 3) Warp to single-well manifold (PCA3 + funnel warp), get center/sigma\n",
    "base_params = WellParams(\n",
    "    whiten=True, tau=0.25, isotropize_xy=True,\n",
    "    sigma_scale=0.80, depth_scale=1.35, mix_z=0.12,\n",
    "    inhibit_k=10, inhibit_strength=0.55\n",
    ")\n",
    "X3_warp, m, info = pca3_and_warp(H, energy=E, params=base_params)\n",
    "\n",
    "# 4) Fit radial funnel profile z(r) then convert to {r, phi, g}\n",
    "#    (these hyperparams mirror the Stage-11 benchmark defaults)\n",
    "n_r = 220\n",
    "r_cloud = np.linalg.norm((X3_warp[:,:2] - info[\"center\"]), axis=1)\n",
    "r_max = float(np.quantile(r_cloud, 0.98))\n",
    "r_grid = np.linspace(0.0, r_max, n_r)\n",
    "\n",
    "h = max(1e-6, 0.30 * r_max)     # rbf_bw\n",
    "z_data = fit_radial_profile(\n",
    "    X3_warp, info[\"center\"], r_grid, h=h, q=0.65,     # fit_quantile\n",
    "    r0_frac=0.14,                                     # core_r0_frac\n",
    "    core_k=0.18, core_p=1.7                           # core template strength/shape\n",
    ")\n",
    "z_tmpl = analytic_core_template(\n",
    "    r_grid, D=1.2, p=1.6, r0_frac=0.14                # template_D, template_p\n",
    ")\n",
    "z_prof = blend_profiles(z_data, z_tmpl, alpha=0.25)  # blend_core\n",
    "\n",
    "pri = priors_from_profile(r_grid, z_prof)  # → dict(r, phi, g) on [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf38d6d7-8b6f-47cd-9578-b2db1db214d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 5) Save exactly what geodesic_parse_with_prior() expects\n",
    "# Path(\".artifacts\").mkdir(exist_ok=True, parents=True)\n",
    "# np.savez(\".artifacts/arc_stage11_priors.npz\", **pri)\n",
    "# print(\"Wrote .artifacts/arc_stage11_priors.npz with keys:\", list(pri.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86d1ac8b-96f4-449a-a5c9-38f09395dec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote .artifacts/arc_stage11_priors.npz with keys: ['r', 'phi', 'g']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, os\n",
    "# os.makedirs(\".artifacts\", exist_ok=True)\n",
    "\n",
    "# Cover radius range [0, Rmax]; if unsure, start with 1.0\n",
    "r_grid = np.linspace(0.0, 1.0, 64)\n",
    "# A convex funnel: high score near the center, tapering with radius\n",
    "z_grid = (1.0 - r_grid)**2  # any smooth, monotone-decreasing shape works\n",
    "\n",
    "np.savez(\".artifacts/arc_stage11_priors.npz\", r_grid=r_grid, z_grid=z_grid)\n",
    "print(\"Wrote .artifacts/arc_stage11_priors.npz with keys:\", list(pri.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89433759-a634-4e6b-b2a9-7710e1e0cc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Save exactly what geodesic_parse_with_prior() expects\n",
    "Path(\".artifacts\").mkdir(exist_ok=True, parents=True)\n",
    "np.savez(\".artifacts/arc_stage11_priors.npz\", **pri)\n",
    "print(\"Wrote .artifacts/arc_stage11_priors.npz with keys:\", list(pri.keys()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
