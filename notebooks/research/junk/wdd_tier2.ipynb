{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05546405-4532-4cbb-a141-92858a357a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd =  os.getcwd().replace(\"notebooks/research\",\"\")\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d43cbe9b-e578-47cb-adb4-fae0dbbdf69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoders shim ready (SBERTEncoder + SBERTFeaturizer) and sys.path configured\n"
     ]
    }
   ],
   "source": [
    "# --- Robust notebook shim for legacy joblib artifacts expecting `encoders.*` ---\n",
    "import sys, types, numpy as np\n",
    "\n",
    "# Create/replace a lightweight 'encoders' module in sys.modules\n",
    "enc_mod = types.ModuleType(\"encoders\")\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except Exception as e:\n",
    "    SentenceTransformer = None\n",
    "    print(\"NOTE: sentence-transformers not available:\", e)\n",
    "\n",
    "class _SBERTBase:\n",
    "    \"\"\"\n",
    "    Compat shim implementing the sklearn Transformer API expected by saved Pipelines.\n",
    "    Handles pickles that don't call __init__ and are missing attributes.\n",
    "    Provides both class names: SBERTEncoder and SBERTFeaturizer.\n",
    "    \"\"\"\n",
    "    # NOTE: __init__ might not be called during unpickle; use _ensure_attrs() everywhere.\n",
    "    def __init__(self, model=\"sentence-transformers/all-MiniLM-L6-v2\", **kwargs):\n",
    "        self.model_name = model\n",
    "        self._enc = None\n",
    "        self._kwargs = kwargs\n",
    "\n",
    "    def _ensure_attrs(self):\n",
    "        # Add any attributes that might be missing from legacy pickles\n",
    "        if not hasattr(self, \"model_name\") or self.model_name is None:\n",
    "            self.model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        if not hasattr(self, \"_enc\"):\n",
    "            self._enc = None\n",
    "        if not hasattr(self, \"_kwargs\"):\n",
    "            self._kwargs = {}\n",
    "\n",
    "    def _ensure_encoder(self):\n",
    "        self._ensure_attrs()\n",
    "        if self._enc is None:\n",
    "            if SentenceTransformer is None:\n",
    "                raise RuntimeError(\n",
    "                    \"sentence-transformers not installed in this kernel; \"\n",
    "                    \"pip install sentence-transformers && restart kernel\"\n",
    "                )\n",
    "            self._enc = SentenceTransformer(self.model_name)\n",
    "\n",
    "    # sklearn API\n",
    "    def fit(self, X, y=None):\n",
    "        self._ensure_attrs()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        self._ensure_encoder()\n",
    "        return np.asarray(self._enc.encode(list(X), show_progress_bar=False))\n",
    "\n",
    "    # some older code may call .encode directly; alias it\n",
    "    def encode(self, X):\n",
    "        return self.transform(X)\n",
    "\n",
    "# Expose both legacy names on the encoders module\n",
    "class SBERTEncoder(_SBERTBase): ...\n",
    "class SBERTFeaturizer(_SBERTBase): ...\n",
    "\n",
    "enc_mod.SBERTEncoder = SBERTEncoder\n",
    "enc_mod.SBERTFeaturizer = SBERTFeaturizer\n",
    "sys.modules[\"encoders\"] = enc_mod\n",
    "\n",
    "# Make sure your package code is importable too (if needed)\n",
    "import pathlib\n",
    "if str(pathlib.Path(\"src\").resolve()) not in sys.path:\n",
    "    sys.path.append(str(pathlib.Path(\"src\").resolve()))\n",
    "print(\"encoders shim ready (SBERTEncoder + SBERTFeaturizer) and sys.path configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "add5fe41-7f49-4b22-a559-56f2bbd389af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: /Users/ian_moore/repos/micro-lm/.artifacts/defi_mapper.joblib\n",
      "Pipeline(steps=[('sbertencoder', <__main__.SBERTEncoder object at 0x318dfb9d0>),\n",
      "                ('calibratedclassifiercv',\n",
      "                 CalibratedClassifierCV(cv=3,\n",
      "                                        estimator=LogisticRegression(C=8.0,\n",
      "                                                                     class_weight='balanced',\n",
      "                                                                     max_iter=2000,\n",
      "                                                                     random_state=0),\n",
      "                                        method='isotonic'))])\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "def load_mapper():\n",
    "    for name in [\".artifacts/defi_mapper.joblib\", \".artifacts/defi_mapper_embed.joblib\"]:\n",
    "        p = Path(name).resolve()\n",
    "        if p.exists():\n",
    "            print(\"Loading:\", p.as_posix())\n",
    "            return joblib.load(p.as_posix())\n",
    "    raise FileNotFoundError(\"No mapper artifact found in .artifacts/\")\n",
    "\n",
    "pipe = load_mapper()\n",
    "print(pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b83c4e60-2c75-4703-8cdf-6b9c9e215a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: deposit_asset\n",
      "Top-3: [('deposit_asset', 1.0), ('borrow_asset', 0.0), ('claim_rewards', 0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "prompt = \"supply 7.0245 SOL to maker\"\n",
    "pred  = pipe.predict([prompt])[0]\n",
    "probs = pipe.predict_proba([prompt])[0]\n",
    "print(\"Predicted:\", pred)\n",
    "print(\"Top-3:\", sorted(zip(pipe.classes_, probs), key=lambda t: t[1], reverse=True)[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d64dc9-3144-450f-8220-719cb378e5bd",
   "metadata": {},
   "source": [
    "### Get token-time traces from the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d43297d1-21a0-4fe0-b825-bf08fd2bddd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "BASE = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tok = AutoTokenizer.from_pretrained(BASE)\n",
    "mdl = AutoModel.from_pretrained(BASE, output_hidden_states=True)\n",
    "mdl.eval();\n",
    "\n",
    "def get_hidden_states(text: str, layer_offset: int = -9):\n",
    "    \"\"\"Returns [T, H] float32 tensor for a single prompt from a chosen layer.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        batch = tok(text, return_tensors=\"pt\")\n",
    "        out = mdl(**batch)\n",
    "        # hidden_states: tuple(len=L+1) of [1, T, H]; choose a stable mid/earlier layer (e.g., -9)\n",
    "        hs = out.hidden_states[layer_offset].squeeze(0).float()  # [T, H]\n",
    "        # strip CLS if you prefer token-only; optional:\n",
    "        return hs  # [T, H]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9490bbf7-1b5b-4237-95ee-0e8d572df714",
   "metadata": {},
   "source": [
    "### Build PCA channels (3D) as the WDD signal space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b40bb32e-09df-4575-bff9-28fea05f6bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 384])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "BASE = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tok = AutoTokenizer.from_pretrained(BASE)\n",
    "mdl = AutoModel.from_pretrained(BASE, output_hidden_states=True).eval()\n",
    "\n",
    "def get_hidden_states(text: str, layer_offset: int = -4):\n",
    "    \"\"\"\n",
    "    Returns a [T, H] tensor from a valid layer.\n",
    "    For MiniLM-L6, valid offsets are: -1..-7 (where -7 is embeddings).\n",
    "    We avoid embeddings and clamp to the range automatically.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        batch = tok(text, return_tensors=\"pt\")\n",
    "        out = mdl(**batch)\n",
    "        hs = out.hidden_states\n",
    "        n = len(hs)                 # e.g., 7 for L6 (0..6)\n",
    "        # clamp offset to [- (n-1), -1] so we never pick embeddings or go OOR\n",
    "        lo = -(n-1)                 # e.g., -6\n",
    "        hi = -1\n",
    "        k = max(lo, min(layer_offset, hi))\n",
    "        return hs[k].squeeze(0).float()   # [T, H]\n",
    "\n",
    "# quick probe\n",
    "h = get_hidden_states(\"supply 7.0245 SOL to maker\", layer_offset=-4)\n",
    "h.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac8540b2-e1aa-417d-954a-af0b9b5e1d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def build_pca_channels(texts, layer_offset=-4, d=3, whiten=True):\n",
    "    mats = [get_hidden_states(t, layer_offset).numpy() for t in texts]  # each [T,H]\n",
    "    all_tokens = np.vstack(mats)  # [sum_T, H]\n",
    "\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "    Xn = scaler.fit_transform(all_tokens)\n",
    "\n",
    "    pca = PCA(n_components=d, whiten=whiten, random_state=42)\n",
    "    Z = pca.fit_transform(Xn)\n",
    "\n",
    "    seqs, i = [], 0\n",
    "    for m in mats:\n",
    "        T = m.shape[0]\n",
    "        seqs.append(Z[i:i+T])\n",
    "        i += T\n",
    "    return seqs, pca, scaler\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     out = mdl(**tok(\"test\", return_tensors=\"pt\"))\n",
    "#     print(len(out.hidden_states))  # embeddings + num_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ea5c3b9-4524-4bf5-9c7a-2b1e629afa77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, (10, 3))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example batch (you can feed your benchmark prompts)\n",
    "texts = [\n",
    "    \"supply 7.0245 SOL to maker\",\n",
    "    \"swap 10 ETH to USDC on uniswap\",\n",
    "    \"attempt a borrow with low health factor\"\n",
    "]\n",
    "pca_seqs, pca_model, pca_scaler = build_pca_channels(texts, d=3, whiten=True)\n",
    "len(pca_seqs), pca_seqs[0].shape  # → (3, [T,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e1d8d9-fa7a-43bc-b591-b14013e3d974",
   "metadata": {},
   "source": [
    "### 4) Compute WDD scores (margin) + NGF matched-filter (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d84204a9-3b6f-4234-986b-dda366d8f63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngeodesic not available; falling back to simple np.convolve matcher\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5843617692371803, 0.5322664036930501, 0.5372362585588677]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Try to import ngeodesic’s matched filtering / denoising\n",
    "try:\n",
    "    from ngeodesic.dsp.filters import gaussian_matched_filter_1d\n",
    "    from ngeodesic.dsp.denoise import hybrid_smoother\n",
    "    HAVE_NG = True\n",
    "except Exception:\n",
    "    HAVE_NG = False\n",
    "    print(\"ngeodesic not available; falling back to simple np.convolve matcher\")\n",
    "\n",
    "def matched_filter_channel(x: np.ndarray, sigma: float = 5.0) -> np.ndarray:\n",
    "    \"\"\"Apply 1D matched filter per channel.\"\"\"\n",
    "    if HAVE_NG:\n",
    "        return gaussian_matched_filter_1d(x, sigma=sigma)\n",
    "    # Fallback: crude gaussian kernel\n",
    "    L = int(6*sigma)+1\n",
    "    t = np.arange(L) - L//2\n",
    "    k = np.exp(-(t**2)/(2*sigma*sigma)); k /= k.sum()\n",
    "    return np.convolve(x, k, mode=\"same\")\n",
    "\n",
    "def wdd_score_sequence(seq_3d: np.ndarray, sigma: float = 5.0) -> float:\n",
    "    \"\"\"\n",
    "    Simple WDD score for one prompt:\n",
    "    - take 3 PCA channels over tokens [T,3],\n",
    "    - matched filter each channel,\n",
    "    - compute max energy and a margin-like separation across channels,\n",
    "    - return a scalar score (higher = stronger detection).\n",
    "    \"\"\"\n",
    "    # matched filter each channel independently\n",
    "    mf = np.stack([matched_filter_channel(seq_3d[:,i], sigma) for i in range(seq_3d.shape[1])], axis=1)  # [T,3]\n",
    "    # channel energy\n",
    "    ch_energy = mf.max(axis=0)  # [3]\n",
    "    # margin: best - second_best\n",
    "    srt = np.sort(ch_energy)\n",
    "    margin = float(srt[-1] - srt[-2]) if len(srt) >= 2 else float(srt[-1])\n",
    "    return margin\n",
    "\n",
    "scores = [wdd_score_sequence(z, sigma=5.0) for z in pca_seqs]\n",
    "scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c606e9-8634-4277-8869-76c3fd79531a",
   "metadata": {},
   "source": [
    "### 5) Null calibration (circular shifts) → threshold → abstain/fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c0c963a-6866-4f5e-99d4-48707b00e556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('supply 7.0245 SOL to maker', 0.584, 1.73, True),\n",
       " ('swap 10 ETH to USDC on uniswap', 0.532, -0.47, False),\n",
       " ('attempt a borrow with low health factor', 0.537, -4.0, False)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "\n",
    "def circular_shift(x, k):  # shift tokens by k\n",
    "    k = k % len(x); \n",
    "    return np.concatenate([x[-k:], x[:-k]], axis=0)\n",
    "\n",
    "def wdd_null_scores(seq_3d, n_null=128, sigma=5.0):\n",
    "    # Build null by circularly shifting channels independently\n",
    "    T = seq_3d.shape[0]\n",
    "    nulls = []\n",
    "    for _ in range(n_null):\n",
    "        z = np.stack([circular_shift(seq_3d[:,i], int(rng.integers(1, max(2, T//2)))) \n",
    "                      for i in range(seq_3d.shape[1])], axis=1)\n",
    "        nulls.append(wdd_score_sequence(z, sigma=sigma))\n",
    "    return np.array(nulls)\n",
    "\n",
    "# Calibrate threshold on each example’s null (you can also pool nulls)\n",
    "nulls = [wdd_null_scores(z, n_null=256, sigma=5.0) for z in pca_seqs]\n",
    "mu = np.array([n.mean() for n in nulls]); sd = np.array([n.std() + 1e-9 for n in nulls])\n",
    "\n",
    "# z-scores for observed sequences\n",
    "z_scores = (np.array(scores) - mu) / sd\n",
    "\n",
    "# Decision: pass if z >= z_thr (e.g., 1.5), else abstain\n",
    "z_thr = 1.5\n",
    "decisions = (z_scores >= z_thr)\n",
    "list(zip(texts, np.round(scores,3), np.round(z_scores,2), decisions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2462382-b827-466b-becd-6a988dc7c827",
   "metadata": {},
   "source": [
    "### 6) (Optional) Dual-gate + denoise (closer to NGF Stage-11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dbd7ed9-2431-482c-a2eb-2a3057c55f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True {'rel': 0.5843617692371803, 'z': 1.5777207699924611} ← supply 7.0245 SOL to maker\n",
      "False {'rel': 0.5322664036930501, 'z': -0.6078239725797517} ← swap 10 ETH to USDC on uniswap\n",
      "False {'rel': 0.5372362585588677, 'z': -3.9480696151284724} ← attempt a borrow with low health factor\n"
     ]
    }
   ],
   "source": [
    "# Build per-token “energy” streams (post-filter) for visualization & denoising\n",
    "def channel_streams(seq_3d, sigma=5.0):\n",
    "    return np.stack([matched_filter_channel(seq_3d[:,i], sigma) for i in range(3)], axis=1)  # [T,3]\n",
    "\n",
    "streams = [channel_streams(z, sigma=5.0) for z in pca_seqs]\n",
    "\n",
    "if HAVE_NG:\n",
    "    smooth = [hybrid_smoother(s, ema_alpha=0.15, med_k=3) for s in streams]  # each [T,3]\n",
    "else:\n",
    "    smooth = streams\n",
    "\n",
    "# Dual-gate decision (toy example): \n",
    "# - relative: margin between best and second channel ≥ τ_rel\n",
    "# - absolute: best channel z-score ≥ τ_abs (using null on that best channel)\n",
    "def dual_gate_decision(seq_3d, null_ns=256, sigma=5.0, tau_rel=0.05, tau_abs=1.5):\n",
    "    s = channel_streams(seq_3d, sigma=sigma)\n",
    "    ch_max = s.max(axis=0)  # [3]\n",
    "    rel = np.sort(ch_max)[-1] - np.sort(ch_max)[-2]\n",
    "    # absolute gate via per-example pooled null\n",
    "    n = wdd_null_scores(seq_3d, n_null=null_ns, sigma=sigma)\n",
    "    z = (wdd_score_sequence(seq_3d, sigma=sigma) - n.mean()) / (n.std() + 1e-9)\n",
    "    return (rel >= tau_rel) and (z >= tau_abs), {\"rel\": float(rel), \"z\": float(z)}\n",
    "\n",
    "for t, z in zip(texts, pca_seqs):\n",
    "    ok, stats = dual_gate_decision(z, sigma=5.0)\n",
    "    print(ok, stats, \"←\", t)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
