{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad9b562a-6ea1-440c-8be1-f67d677b7c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd =  os.getcwd().replace(\"notebooks/research\",\"\")\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7a8e459-25b4-4d16-bbc7-434a4c3eea97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoders shim ready (SBERTEncoder + SBERTFeaturizer) and sys.path configured\n"
     ]
    }
   ],
   "source": [
    "# --- Robust notebook shim for legacy joblib artifacts expecting `encoders.*` ---\n",
    "import sys, types, numpy as np\n",
    "\n",
    "# Create/replace a lightweight 'encoders' module in sys.modules\n",
    "enc_mod = types.ModuleType(\"encoders\")\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except Exception as e:\n",
    "    SentenceTransformer = None\n",
    "    print(\"NOTE: sentence-transformers not available:\", e)\n",
    "\n",
    "class _SBERTBase:\n",
    "    \"\"\"\n",
    "    Compat shim implementing the sklearn Transformer API expected by saved Pipelines.\n",
    "    Handles pickles that don't call __init__ and are missing attributes.\n",
    "    Provides both class names: SBERTEncoder and SBERTFeaturizer.\n",
    "    \"\"\"\n",
    "    # NOTE: __init__ might not be called during unpickle; use _ensure_attrs() everywhere.\n",
    "    def __init__(self, model=\"sentence-transformers/all-MiniLM-L6-v2\", **kwargs):\n",
    "        self.model_name = model\n",
    "        self._enc = None\n",
    "        self._kwargs = kwargs\n",
    "\n",
    "    def _ensure_attrs(self):\n",
    "        # Add any attributes that might be missing from legacy pickles\n",
    "        if not hasattr(self, \"model_name\") or self.model_name is None:\n",
    "            self.model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        if not hasattr(self, \"_enc\"):\n",
    "            self._enc = None\n",
    "        if not hasattr(self, \"_kwargs\"):\n",
    "            self._kwargs = {}\n",
    "\n",
    "    def _ensure_encoder(self):\n",
    "        self._ensure_attrs()\n",
    "        if self._enc is None:\n",
    "            if SentenceTransformer is None:\n",
    "                raise RuntimeError(\n",
    "                    \"sentence-transformers not installed in this kernel; \"\n",
    "                    \"pip install sentence-transformers && restart kernel\"\n",
    "                )\n",
    "            self._enc = SentenceTransformer(self.model_name)\n",
    "\n",
    "    # sklearn API\n",
    "    def fit(self, X, y=None):\n",
    "        self._ensure_attrs()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        self._ensure_encoder()\n",
    "        return np.asarray(self._enc.encode(list(X), show_progress_bar=False))\n",
    "\n",
    "    # some older code may call .encode directly; alias it\n",
    "    def encode(self, X):\n",
    "        return self.transform(X)\n",
    "\n",
    "# Expose both legacy names on the encoders module\n",
    "class SBERTEncoder(_SBERTBase): ...\n",
    "class SBERTFeaturizer(_SBERTBase): ...\n",
    "\n",
    "enc_mod.SBERTEncoder = SBERTEncoder\n",
    "enc_mod.SBERTFeaturizer = SBERTFeaturizer\n",
    "sys.modules[\"encoders\"] = enc_mod\n",
    "\n",
    "# Make sure your package code is importable too (if needed)\n",
    "import pathlib\n",
    "if str(pathlib.Path(\"src\").resolve()) not in sys.path:\n",
    "    sys.path.append(str(pathlib.Path(\"src\").resolve()))\n",
    "print(\"encoders shim ready (SBERTEncoder + SBERTFeaturizer) and sys.path configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de25993f-7a09-4fe8-b0b5-f3ceb9179820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: /Users/ian_moore/repos/micro-lm/.artifacts/arc_mapper.joblib\n",
      "Pipeline(steps=[('sbertencoder', <__main__.SBERTEncoder object at 0x36382b040>),\n",
      "                ('calibratedclassifiercv',\n",
      "                 CalibratedClassifierCV(cv=3,\n",
      "                                        estimator=LogisticRegression(C=8.0,\n",
      "                                                                     class_weight='balanced',\n",
      "                                                                     max_iter=2000,\n",
      "                                                                     random_state=0),\n",
      "                                        method='isotonic'))])\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "def load_mapper():\n",
    "    for name in [\".artifacts/arc_mapper.joblib\"]:\n",
    "        p = Path(name).resolve()\n",
    "        if p.exists():\n",
    "            print(\"Loading:\", p.as_posix())\n",
    "            return joblib.load(p.as_posix())\n",
    "    raise FileNotFoundError(\"No mapper artifact found in .artifacts/\")\n",
    "\n",
    "mapper = load_mapper()\n",
    "print(mapper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "c42a4e19-babd-461f-9c05-1d391809128c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: rotate\n",
      "Top-3: [('rotate', 1.0), ('flip_h', 0.0), ('flip_v', 0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "prompt = \"flip the grid horizontally\"\n",
    "prompt = \"Rotate the grid 90 degrees clockwise, then flip it horizontally\"\n",
    "pred  = mapper.predict([prompt])[0]\n",
    "probs = mapper.predict_proba([prompt])[0]\n",
    "print(\"Predicted:\", pred)\n",
    "print(\"Top-3:\", sorted(zip(mapper.classes_, probs), key=lambda t: t[1], reverse=True)[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4a0b5f62-f3c9-4b77-b77e-ba2ca02c75db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper_fn(prompt, labels=(\"flip_h\",\"flip_v\",\"rot90\"), mapper=mapper):\n",
    "    probs = mapper.predict_proba([prompt])[0]\n",
    "    res = sorted(zip(mapper.classes_, probs), key=lambda t: t[1], reverse=True)\n",
    "    return {res[0][0]:res[0][1], res[1][0]:res[1][1], res[2][0]:res[2][1]}\n",
    "\n",
    "# def mapper_fn(prompt, labels=(\"flip_h\",\"flip_v\",\"rot90\"), temperature=0.0):\n",
    "#     return {'flip_h': 0.9375, 'flip_v': 0.06250000000000001, 'rot90': 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "17997443-e74a-4eda-a12e-071fe6bbd3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "PROTO_WIDTH = 160  # keep consistent with WDD_CFG[\"proto_width\"]\n",
    "\n",
    "def _upsample(x, L):\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    if len(x) == L: return x\n",
    "    xp = np.linspace(0, len(x)-1, num=len(x))\n",
    "    xq = np.linspace(0, len(x)-1, num=L)\n",
    "    return np.interp(xq, xp, x)\n",
    "\n",
    "def _smooth_norm(x, sigma=3.5):\n",
    "    x = gaussian_filter1d(x.astype(np.float32), sigma=sigma, mode=\"nearest\")\n",
    "    x -= x.min()\n",
    "    x /= (x.max() - x.min() + 1e-8)\n",
    "    return x\n",
    "\n",
    "def _scan_cols(A, B):\n",
    "    # per-column Hamming match\n",
    "    return np.array([np.mean(A[:, j] == B[:, j]) for j in range(A.shape[1])], np.float32)\n",
    "\n",
    "def _scan_rows(A, B):\n",
    "    # per-row Hamming match\n",
    "    return np.array([np.mean(A[i, :] == B[i, :]) for i in range(A.shape[0])], np.float32)\n",
    "\n",
    "def _rot90_proxy(G):\n",
    "    # diagonal-band match against 90° rotation (CCW)\n",
    "    R = np.rot90(G, 1)\n",
    "    H, W = G.shape\n",
    "    vals = []\n",
    "    for k in range(-(H-1), W):\n",
    "        d1 = np.diagonal(G, offset=k)\n",
    "        d2 = np.diagonal(R, offset=k)\n",
    "        m = min(len(d1), len(d2))\n",
    "        if m > 0:\n",
    "            vals.append(np.mean(d1[:m] == d2[:m]))\n",
    "    return np.array(vals if vals else [0.0], np.float32)\n",
    "\n",
    "def traces_from_grid(grid, L=PROTO_WIDTH):\n",
    "    G = np.asarray(grid, dtype=int)\n",
    "    Gh = G[:, ::-1]      # flip_h hypothesis (left↔right)\n",
    "    Gv = G[::-1, :]      # flip_v hypothesis (top↕bottom)\n",
    "\n",
    "    # axis-specific matches\n",
    "    hc = _scan_cols(G, Gh); hr = _scan_rows(G, Gh)  # matches under flip_h\n",
    "    vc = _scan_cols(G, Gv); vr = _scan_rows(G, Gv)  # matches under flip_v\n",
    "\n",
    "    # CONTRASTIVE traces:\n",
    "    #  - flip_h should be strong when column-wise evidence for h-flip is high\n",
    "    #    AND row-wise evidence for v-flip is low → hc - vr\n",
    "    #  - flip_v should be strong when row-wise evidence for v-flip is high\n",
    "    #    AND col-wise evidence for h-flip is low → vr - hc\n",
    "    raw_h = hc - vr\n",
    "    raw_v = vr - hc\n",
    "\n",
    "    # rot90 proxy\n",
    "    raw_r = _rot90_proxy(G)\n",
    "\n",
    "    # upsample → smooth → normalize\n",
    "    th = _smooth_norm(_upsample(raw_h, L))\n",
    "    tv = _smooth_norm(_upsample(raw_v, L))\n",
    "    tr = _smooth_norm(_upsample(raw_r, L))\n",
    "\n",
    "    # tiny ε patterns to avoid pathological flat ties on micro-grids\n",
    "    eps = 1e-3\n",
    "    lin = np.linspace(0, 1, L, dtype=np.float32)\n",
    "    th = (1-eps)*th + eps*lin\n",
    "    tv = (1-eps)*tv + eps*(1-lin)\n",
    "    tr = (1-eps)*tr + eps*(0.5 + 0.5*np.sin(2*np.pi*lin))\n",
    "\n",
    "    return {\"flip_h\": th, \"flip_v\": tv, \"rot90\": tr}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7d6de8cb-975b-431b-9258-dca9f56a4331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional, Tuple, List\n",
    "\n",
    "PRIMS = [\"flip_h\",\"flip_v\",\"rotate\"]\n",
    "\n",
    "def gaussian_bump(T, center, width, amp=1.0):\n",
    "    t = np.arange(T)\n",
    "    sig2 = (width/2.355)**2  # FWHM→σ\n",
    "    return amp * np.exp(-(t-center)**2 / (2*sig2))\n",
    "\n",
    "# ----------------------------\n",
    "# Synthetic ARC-like generator\n",
    "# ----------------------------\n",
    "def make_synthetic_traces(rng, T=720, noise=0.02, cm_amp=0.02, overlap=0.5,\n",
    "                          amp_jitter=0.4, distractor_prob=0.4,\n",
    "                          tasks_k: Tuple[int,int]=(1,3)) -> Tuple[Dict[str,np.ndarray], List[str]]:\n",
    "    k = int(rng.integers(tasks_k[0], tasks_k[1]+1))\n",
    "    tasks = list(rng.choice(PRIMS, size=k, replace=False))\n",
    "    rng.shuffle(tasks)\n",
    "    base = np.array([0.20, 0.50, 0.80]) * T\n",
    "    centers = ((1.0 - overlap) * base + overlap * (T * 0.50)).astype(int)\n",
    "    width = int(max(12, T * 0.10))\n",
    "    t = np.arange(T)\n",
    "    cm = cm_amp * (1.0 + 0.2 * np.sin(2*np.pi * t / max(30, T//6)))\n",
    "\n",
    "    traces = {p: np.zeros(T, float) for p in PRIMS}\n",
    "    for i, prim in enumerate(tasks):\n",
    "        c = centers[i % len(centers)]\n",
    "        amp = max(0.25, 1.0 + rng.normal(0, amp_jitter))\n",
    "        c_jit = int(np.clip(c + rng.integers(-width//5, width//5 + 1), 0, T-1))\n",
    "        traces[prim] += gaussian_bump(T, c_jit, width, amp=amp)\n",
    "\n",
    "    for p in PRIMS:\n",
    "        if p not in tasks and rng.random() < distractor_prob:\n",
    "            c = int(rng.uniform(T*0.15, T*0.85))\n",
    "            amp = max(0.25, 1.0 + rng.normal(0, amp_jitter))\n",
    "            traces[p] += gaussian_bump(T, c, width, amp=0.9*amp)\n",
    "\n",
    "    for p in PRIMS:\n",
    "        traces[p] = np.clip(traces[p] + cm, 0, None)\n",
    "        traces[p] = traces[p] + rng.normal(0, noise, size=T)\n",
    "        traces[p] = np.clip(traces[p], 0, None)\n",
    "\n",
    "    return traces, tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b50ec7fe-ce8c-48fc-b20d-c480ccfa338d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traces_from_synthetic(label: str, T: int = 160):\n",
    "    \"\"\"Deterministic single-task traces (no noise, no distractors) for a given primitive label.\"\"\"\n",
    "    rng = np.random.default_rng(0)\n",
    "    # 1 task, zero noise/common-mode, no distractors\n",
    "    traces, tasks = make_synthetic_traces(\n",
    "        rng, T=T, noise=0.0, cm_amp=0.0, overlap=0.5,\n",
    "        amp_jitter=0.0, distractor_prob=0.0, tasks_k=(1,1)\n",
    "    )\n",
    "    # Force the task to the requested label by moving the bump to that channel\n",
    "    if label in PRIMS:\n",
    "        # clear all channels then copy the nonzero bump into target channel\n",
    "        # find the single active channel the generator used\n",
    "        active = [p for p,v in traces.items() if np.max(v) > 0.0]\n",
    "        src = active[0] if active else PRIMS[0]\n",
    "        bump = traces[src].copy()\n",
    "        for p in PRIMS: traces[p] = np.zeros_like(bump)\n",
    "        traces[label] = bump\n",
    "    return traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "23407f6d-b1a3-4fd3-aa2f-0d4f1bacab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#traces_from_grid(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7f466181-2e9e-46b1-bc14-9a1a870d39b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUE: ['flip_v', 'flip_h', 'rotate']\n"
     ]
    }
   ],
   "source": [
    "traces2, true_tasks2 = make_synthetic_traces(\n",
    "    rng, T=160, noise=0.0, cm_amp=0.0, overlap=0.0, amp_jitter=0.0, distractor_prob=0.0,\n",
    "    tasks_k=(3,3)\n",
    ")\n",
    "print(\"TRUE:\", true_tasks2)   # e.g. ['flip_v','rotate'] in time order\n",
    "# print(\"REPORT:\", *geodesic_parse_report(traces2, sigma=9, proto_width=160))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4f1aaa91-f6e6-43d4-a8f2-278655eec5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) make deterministic, noise-free synthetic traces (single task)\n",
    "rng = np.random.default_rng(123)\n",
    "traces, true_tasks = make_synthetic_traces(\n",
    "    rng,\n",
    "    T=160,               # match your proto_width for convenience\n",
    "    noise=0.0,           # <-- zero noise\n",
    "    cm_amp=0.0,          # <-- kill common-mode drift\n",
    "    overlap=0.0,         # keep bumps separated if you choose >1 task\n",
    "    amp_jitter=0.0,      # remove amplitude jitter\n",
    "    distractor_prob=0.0, # no distractor bumps\n",
    "    tasks_k=(1,1)        # exactly one primitive\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e5df35e4-9ab3-42cc-b9b4-fcf7ba5fe490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 3, 6],\n",
       "       [3, 0, 3, 6],\n",
       "       [3, 2, 1, 3],\n",
       "       [3, 2, 0, 5]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "f1d7e387-fce9-4d16-b5c0-f1b62bf5b877",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'priors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[177], line 13\u001b[0m\n\u001b[1;32m      7\u001b[0m grid \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m6\u001b[39m],\n\u001b[1;32m      8\u001b[0m                  [\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m6\u001b[39m],\n\u001b[1;32m      9\u001b[0m                  [\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m],\n\u001b[1;32m     10\u001b[0m                  [\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m5\u001b[39m]], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# run without priors first\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m res \u001b[38;5;241m=\u001b[39m run_arc_wdd(prompt, grid, mapper_fn, traces_from_grid, \u001b[43mpriors\u001b[49m, use_priors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverdict\u001b[39m\u001b[38;5;124m\"\u001b[39m], res\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m), res\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeep\u001b[39m\u001b[38;5;124m\"\u001b[39m), res\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     15\u001b[0m res\n",
      "\u001b[0;31mNameError\u001b[0m: name 'priors' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# from wdd_arc_impl_patched import load_priors_npz, run_arc_wdd\n",
    "from notebooks.research.wdd_arc_impl_patched import  load_priors_npz, run_arc_wdd\n",
    "\n",
    "prompt = \"flip the grid horizontally\"\n",
    "grid = np.array([[3,0,3,],[3,2,1],[3,2,0]], dtype=int)\n",
    "\n",
    "# run without priors first\n",
    "res = run_arc_wdd(prompt, grid, mapper_fn, traces_from_grid, priors, use_priors=True)\n",
    "print(res[\"verdict\"], res.get(\"label\"), res.get(\"keep\"), res.get(\"order\"))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c309502c-e9f6-424a-b5dd-2795cfaf02ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'flip_v': 0.5515621633268692, 'flip_h': 0.4484378366731308, 'rotate': 0.0}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapper_fn(\"flip the grid vertcially\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "89c31cc1-2a7b-496f-8e70-7b80c9f6068f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verdict': 'PASS', 'label': 'flip_h', 'keep': ['flip_h'], 'order': ['flip_h'], 'out': array([[1, 0, 3],\n",
      "       [1, 2, 3],\n",
      "       [0, 2, 3]]), 'mapper': {'flip_h': 0.9375, 'flip_v': 0.06250000000000001, 'rot90': 0.0}, 'route': ['flip_h', 'flip_v']}\n"
     ]
    }
   ],
   "source": [
    "res = run_arc_wdd(prompt, grid, mapper_fn, traces_from_grid,\n",
    "                  priors=None, use_priors=False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ac63e7b-c208-4676-87b9-bc33d374fb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapper probs: {'flip_h': 0.9375, 'flip_v': 0.06250000000000001, 'rot90': 0.0} top1: flip_h\n",
      "trace keys: ['flip_h', 'flip_v', 'rot90']\n",
      "flip_h len 160 nan? False std 0.21490313 min/max (0.0010000000474974513, 0.9991006255149841)\n",
      "flip_v len 160 nan? False std 0.22710231 min/max (0.0, 1.0)\n",
      "rot90 len 160 nan? False std 0.25631213 min/max (0.4998902380466461, 1.4987125396728516)\n"
     ]
    }
   ],
   "source": [
    "mp = mapper_fn(prompt)\n",
    "print(\"mapper probs:\", mp, \"top1:\", max(mp, key=mp.get))\n",
    "\n",
    "tr = traces_from_grid(grid)\n",
    "print(\"trace keys:\", list(tr))\n",
    "for k,v in tr.items():\n",
    "    print(k, \"len\", len(v), \"nan?\", np.isnan(v).any(), \"std\", np.std(v), \"min/max\", (float(np.min(v)), float(np.max(v))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c0550371-ab47-4630-91b6-54993fb959df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]]\n",
      "flip_h (L↔R):\n",
      " [[ 3  2  1  0]\n",
      " [ 7  6  5  4]\n",
      " [11 10  9  8]]\n",
      "flip_v (T↕B):\n",
      " [[ 8  9 10 11]\n",
      " [ 4  5  6  7]\n",
      " [ 0  1  2  3]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.arange(12).reshape(3,4)\n",
    "flip_h_exec = A[:, ::-1]\n",
    "flip_v_exec = A[::-1, :]\n",
    "print(\"A:\\n\", A)\n",
    "print(\"flip_h (L↔R):\\n\", flip_h_exec)\n",
    "print(\"flip_v (T↕B):\\n\", flip_v_exec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd45194d-a473-4ccb-a294-e8d930345e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.arange(12).reshape(3,4)\n",
    "assert np.allclose(A[:, ::-1], np.array([[3,2,1,0],[7,6,5,4],[11,10,9,8]]))\n",
    "assert np.allclose(A[::-1, :], np.array([[8,9,10,11],[4,5,6,7],[0,1,2,3]]))\n",
    "\n",
    "# mapper sanity:\n",
    "mp = mapper_fn(\"flip the grid horizontally\")\n",
    "assert \"flip_h\" in mp and \"flip_v\" in mp, \"mapper must expose both\"\n",
    "# if your policy is 'horizontal' == left-right:\n",
    "assert max(mp, key=mp.get) == \"flip_h\", \"mapper text->label semantics mismatch\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e51808-d9d6-4c73-887f-da697114c577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3637e5-b78a-442b-a4d9-9c18c01e8bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
