{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77502adf-d010-4789-9083-32e5c44b9119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd =  os.getcwd().replace(\"notebooks\",\"\")\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b670f0fa-8d84-40e6-8380-123f52d71a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoders shim ready (SBERTEncoder + SBERTFeaturizer) and sys.path configured\n"
     ]
    }
   ],
   "source": [
    "# --- Robust notebook shim for legacy joblib artifacts expecting `encoders.*` ---\n",
    "import sys, types, numpy as np\n",
    "\n",
    "# Create/replace a lightweight 'encoders' module in sys.modules\n",
    "enc_mod = types.ModuleType(\"encoders\")\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except Exception as e:\n",
    "    SentenceTransformer = None\n",
    "    print(\"NOTE: sentence-transformers not available:\", e)\n",
    "\n",
    "class _SBERTBase:\n",
    "    \"\"\"\n",
    "    Compat shim implementing the sklearn Transformer API expected by saved Pipelines.\n",
    "    Handles pickles that don't call __init__ and are missing attributes.\n",
    "    Provides both class names: SBERTEncoder and SBERTFeaturizer.\n",
    "    \"\"\"\n",
    "    # NOTE: __init__ might not be called during unpickle; use _ensure_attrs() everywhere.\n",
    "    def __init__(self, model=\"sentence-transformers/all-MiniLM-L6-v2\", **kwargs):\n",
    "        self.model_name = model\n",
    "        self._enc = None\n",
    "        self._kwargs = kwargs\n",
    "\n",
    "    def _ensure_attrs(self):\n",
    "        # Add any attributes that might be missing from legacy pickles\n",
    "        if not hasattr(self, \"model_name\") or self.model_name is None:\n",
    "            self.model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        if not hasattr(self, \"_enc\"):\n",
    "            self._enc = None\n",
    "        if not hasattr(self, \"_kwargs\"):\n",
    "            self._kwargs = {}\n",
    "\n",
    "    def _ensure_encoder(self):\n",
    "        self._ensure_attrs()\n",
    "        if self._enc is None:\n",
    "            if SentenceTransformer is None:\n",
    "                raise RuntimeError(\n",
    "                    \"sentence-transformers not installed in this kernel; \"\n",
    "                    \"pip install sentence-transformers && restart kernel\"\n",
    "                )\n",
    "            self._enc = SentenceTransformer(self.model_name)\n",
    "\n",
    "    # sklearn API\n",
    "    def fit(self, X, y=None):\n",
    "        self._ensure_attrs()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        self._ensure_encoder()\n",
    "        return np.asarray(self._enc.encode(list(X), show_progress_bar=False))\n",
    "\n",
    "    # some older code may call .encode directly; alias it\n",
    "    def encode(self, X):\n",
    "        return self.transform(X)\n",
    "\n",
    "# Expose both legacy names on the encoders module\n",
    "class SBERTEncoder(_SBERTBase): ...\n",
    "class SBERTFeaturizer(_SBERTBase): ...\n",
    "\n",
    "enc_mod.SBERTEncoder = SBERTEncoder\n",
    "enc_mod.SBERTFeaturizer = SBERTFeaturizer\n",
    "sys.modules[\"encoders\"] = enc_mod\n",
    "\n",
    "# Make sure your package code is importable too (if needed)\n",
    "import pathlib\n",
    "if str(pathlib.Path(\"src\").resolve()) not in sys.path:\n",
    "    sys.path.append(str(pathlib.Path(\"src\").resolve()))\n",
    "print(\"encoders shim ready (SBERTEncoder + SBERTFeaturizer) and sys.path configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b448ea2f-00a7-4020-9b4a-d6d35afed36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: /Users/ian_moore/repos/micro-lm/.artifacts/defi_mapper.joblib\n",
      "Pipeline(steps=[('sbertencoder', <__main__.SBERTEncoder object at 0x3116fada0>),\n",
      "                ('calibratedclassifiercv',\n",
      "                 CalibratedClassifierCV(cv=3,\n",
      "                                        estimator=LogisticRegression(C=8.0,\n",
      "                                                                     class_weight='balanced',\n",
      "                                                                     max_iter=2000,\n",
      "                                                                     random_state=0),\n",
      "                                        method='isotonic'))])\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "def load_mapper():\n",
    "    for name in [\".artifacts/defi_mapper.joblib\", \".artifacts/defi_mapper_embed.joblib\"]:\n",
    "        p = Path(name).resolve()\n",
    "        if p.exists():\n",
    "            print(\"Loading:\", p.as_posix())\n",
    "            return joblib.load(p.as_posix())\n",
    "    raise FileNotFoundError(\"No mapper artifact found in .artifacts/\")\n",
    "\n",
    "pipe = load_mapper()\n",
    "print(pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a8aefa4-cfe0-444f-bd8f-a3425490a167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: deposit_asset\n",
      "Top-3: [('deposit_asset', 1.0), ('borrow_asset', 0.0), ('claim_rewards', 0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "prompt = \"supply 7.0245 SOL to maker\"\n",
    "pred  = pipe.predict([prompt])[0]\n",
    "probs = pipe.predict_proba([prompt])[0]\n",
    "print(\"Predicted:\", pred)\n",
    "print(\"Top-3:\", sorted(zip(pipe.classes_, probs), key=lambda t: t[1], reverse=True)[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4df37c-27bf-421d-b988-c7a99eb3109c",
   "metadata": {},
   "source": [
    "### WDD Tier 2: Kick-the-Tires Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d13a91-aaa6-4932-9bff-429320fbd1ce",
   "metadata": {},
   "source": [
    "#### Cell 1 — Imports & helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "031df970-b998-4e7d-8f87-d3b3c15e0cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, make sure your src/ is importable\n",
    "import os, json, time, hashlib, random, pathlib, gc\n",
    "from typing import Dict, Any, List\n",
    "import dataclasses\n",
    "import numpy as np\n",
    "os.environ.setdefault(\"PYTHONPATH\", \"src:.\")\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "# Import the project runner\n",
    "try:\n",
    "    from micro_lm.pipelines.runner import run_micro\n",
    "except Exception:\n",
    "    from micro_lm.core.runner import run_micro\n",
    "\n",
    "ARTDIR = pathlib.Path(\".artifacts/notebook_stage8_defi\")\n",
    "ARTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def seq_hash(seq: List[str]) -> str:\n",
    "    return hashlib.sha256((\"|\".join(seq) if seq else \"∅\").encode()).hexdigest()[:12]\n",
    "\n",
    "def _json_default(o):\n",
    "    # dataclasses\n",
    "    if dataclasses.is_dataclass(o):\n",
    "        return dataclasses.asdict(o)\n",
    "    # numpy scalars\n",
    "    if isinstance(o, (np.integer,)):\n",
    "        return int(o)\n",
    "    if isinstance(o, (np.floating,)):\n",
    "        return float(o)\n",
    "    if isinstance(o, (np.ndarray,)):\n",
    "        return o.tolist()\n",
    "    # try a structured view for \"plan step\"-like objects\n",
    "    for attrset in ((\"op\",\"args\"), (\"name\",\"args\"), (\"label\",\"args\")):\n",
    "        if all(hasattr(o, a) for a in attrset):\n",
    "            return {a: getattr(o, a) for a in attrset}\n",
    "    # last resort\n",
    "    return repr(o)\n",
    "\n",
    "def sanitize_output(out: dict) -> dict:\n",
    "    \"\"\"Make the run_micro() result JSON-serializable (deep).\"\"\"\n",
    "    def _walk(x):\n",
    "        if isinstance(x, dict):\n",
    "            return {k: _walk(v) for k, v in x.items()}\n",
    "        if isinstance(x, (list, tuple)):\n",
    "            return [_walk(v) for v in x]\n",
    "        try:\n",
    "            json.dumps(x)  # fast path\n",
    "            return x\n",
    "        except Exception:\n",
    "            return _json_default(x)\n",
    "    return _walk(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ef3ce9-7865-4849-a737-1ee623f30020",
   "metadata": {},
   "source": [
    "#### Cell 2 — Strict “no-fallback” runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b91eb3d3-c19f-42ea-a8d8-563d16c42a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "# Try hard to disable legacy/shim paths (honored by your runner if wired)\n",
    "os.environ[\"MICROLM_DISABLE_SHIM\"]   = \"1\"\n",
    "os.environ[\"MICROLM_DISABLE_MAPPER\"] = \"1\"\n",
    "\n",
    "STRICT_POLICY = {\n",
    "    \"audit\": {\n",
    "        \"backend\": \"wdd\",\n",
    "        \"mode\": \"family\",   # key for DeFi execs\n",
    "        \"profile\": True,\n",
    "        \"max_null\": 64,\n",
    "        \"batch\": 32,\n",
    "    },\n",
    "    \"ltv_max\": 0.75,\n",
    "    \"hf_min\": 1.0,\n",
    "    # Keep mapper config around but set threshold >1 to prohibit “OK via mapper”\n",
    "    \"mapper\": {\"confidence_threshold\": 1.01},\n",
    "}\n",
    "\n",
    "def _fail_if_mapper_or_shim(res: dict, prompt: str):\n",
    "    v = (res.get(\"verify\") or {})\n",
    "    reason = (v.get(\"reason\") or \"\").lower()\n",
    "    arts = (res.get(\"artifacts\") or {})\n",
    "    if \"shim:\" in reason:\n",
    "        raise AssertionError(f\"Shim path detected via verify.reason='{reason}' for: {prompt!r}\")\n",
    "    # catch mapper artifacts even if flags are empty\n",
    "    a = arts if isinstance(arts, dict) else {}\n",
    "    if \"mapper\" in a or (\"artifacts\" in a and isinstance(a[\"artifacts\"], dict) and \"mapper\" in a[\"artifacts\"]):\n",
    "        raise AssertionError(f\"Mapper artifact present (WDD-only mode) for: {prompt!r}\")\n",
    "\n",
    "STRICT_CONTEXT = {\n",
    "    \"oracle\": {\"age_sec\": 5, \"max_age_sec\": 30}\n",
    "}\n",
    "\n",
    "def run_once_strict(prompt: str, *, T: int = 180, rails: str = \"stage11\") -> dict:\n",
    "    out = run_micro(domain=\"defi\", prompt=prompt, context=dict(STRICT_CONTEXT),\n",
    "                    policy=json.loads(json.dumps(STRICT_POLICY)), rails=rails, T=T)\n",
    "    _fail_if_mapper_or_shim(out, prompt)  # <--- new guard\n",
    "\n",
    "    plan = out.get(\"plan\") or {}\n",
    "    seq = plan.get(\"sequence\") or []\n",
    "    if (out.get(\"verify\") or {}).get(\"ok\") is True and not seq:\n",
    "        raise AssertionError(f\"verify.ok True but no sequence for prompt: {prompt!r}\")\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05934a69-023b-47ad-83fd-2585c25aca43",
   "metadata": {},
   "source": [
    "#### Cell 3 — Minimal DeFi test suite (exec + edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4fc1745-66b6-48c8-9489-35194d0c02fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exec (allowed) path\n",
    "DEFI_EXEC = [\n",
    "    {\"name\": \"ok_deposit\", \"prompt\": \"deposit 10 ETH into aave\", \"expect_top1\": \"deposit_asset\"},\n",
    "    {\"name\": \"ok_swap\",    \"prompt\": \"swap 2 ETH for USDC\",      \"expect_top1\": \"swap_asset\"},\n",
    "]\n",
    "\n",
    "# Edge/blocked path\n",
    "DEFI_EDGES = [\n",
    "    {\"name\": \"edge_ltv_withdraw_unsafe\", \"prompt\": \"withdraw 5 ETH\",        \"expect_ok\": False, \"reason_any\": [\"ltv\"]},\n",
    "    {\"name\": \"edge_hf_health_breach\",    \"prompt\": \"increase borrow to the maximum\", \"expect_ok\": False, \"reason_any\": [\"hf\", \"health\"]},\n",
    "    {\"name\": \"edge_oracle_stale\",        \"prompt\": \"borrow 1000 USDC\",      \"expect_ok\": False, \"reason_any\": [\"oracle\", \"stale\"]},\n",
    "    {\"name\": \"edge_low_conf_nonexec\",    \"prompt\": \"stake xyz\",              \"expect_ok\": False, \"reason_any\": [\"abstain\", \"low_conf\"]},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc340c6a-da75-4565-9dc0-a7097e8eac4e",
   "metadata": {},
   "source": [
    "#### Cell 4 — Determinism + perturb harness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61b3346c-bf3c-4ad6-a983-4e5f6935a0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOKS = [\"0.1\",\"0.2\",\"0.3\",\"1\",\"2\",\"5\",\"10\",\"25\",\"50\",\"100\",\"1000\"]\n",
    "\n",
    "def open_jsonl(path: pathlib.Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    f = path.open(\"w\", buffering=1)\n",
    "    def write(obj: Dict[str, Any]):\n",
    "        f.write(json.dumps(obj, default=_json_default) + \"\\n\")\n",
    "    def close():\n",
    "        try:\n",
    "            f.flush(); f.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "    return write, close\n",
    "\n",
    "\n",
    "def perturb_prompt(p: str) -> str:\n",
    "    if any(ch.isdigit() for ch in p):\n",
    "        # swap out the first numeric token found\n",
    "        for tok in NUM_TOKS:\n",
    "            if tok in p:\n",
    "                return p.replace(tok, random.choice(NUM_TOKS), 1)\n",
    "        return p\n",
    "    return p + random.choice([\" — asap\", \" — minimize gas\", \" — safe mode\", \" please\"])\n",
    "\n",
    "def tokens_from_output(out: Dict[str, Any]) -> str:\n",
    "    v = out.get(\"verify\") or {}\n",
    "    reason = str(v.get(\"reason\") or \"\").lower()\n",
    "    tags = v.get(\"tags\") or []\n",
    "    if isinstance(tags, list): reason += \" \" + \" \".join(str(t).lower() for t in tags)\n",
    "    flags = out.get(\"flags\") or {}\n",
    "    reason += \" \" + \" \".join(k.lower() for k in flags.keys())\n",
    "    return reason.strip()\n",
    "\n",
    "def run_suite(cases, *, runs=3, do_perturb=True, perturb_k=1):\n",
    "    rows = []\n",
    "    failures = []\n",
    "    for case in cases:\n",
    "        prompt = case[\"prompt\"]\n",
    "        variants = [prompt]\n",
    "        if do_perturb:\n",
    "            variants += [perturb_prompt(prompt) for _ in range(perturb_k)]\n",
    "\n",
    "        for idx, p in enumerate(variants):\n",
    "            outs = [run_once_strict(p) for _ in range(runs)]\n",
    "\n",
    "\n",
    "            # --- extra shim detection guards ---\n",
    "            v = (out.get(\"verify\") or {})\n",
    "            reason = (v.get(\"reason\") or \"\").lower()\n",
    "            # 1) Reason says shim\n",
    "            if \"shim:\" in reason:\n",
    "                raise AssertionError(f\"Shim path detected via reason='{reason}' for: {prompt!r}\")\n",
    "            \n",
    "            # 2) Artifacts show mapper route\n",
    "            arts = (out.get(\"artifacts\") or {})\n",
    "            if \"mapper\" in arts or (\"artifacts\" in arts and isinstance(arts[\"artifacts\"], dict) and \"mapper\" in arts[\"artifacts\"]):\n",
    "                raise AssertionError(f\"Mapper artifact detected for: {prompt!r}\")\n",
    "            \n",
    "            # 3) Label surfaced but no plan.sequence (mapper-only accept)\n",
    "            plan = out.get(\"plan\") or {}\n",
    "            if (out.get(\"label\") and not plan.get(\"sequence\")):\n",
    "                raise AssertionError(f\"Label without plan.sequence (likely mapper-only): {prompt!r}\")\n",
    "            # --- end guards ---\n",
    "\n",
    "            \n",
    "            hashes = [seq_hash((o.get(\"plan\") or {}).get(\"sequence\") or []) for o in outs]\n",
    "            stable = len(set(hashes)) == 1\n",
    "            out0 = outs[0]\n",
    "\n",
    "            # Expectations\n",
    "            ok = True\n",
    "            why = \"\"\n",
    "            if \"expect_top1\" in case and case[\"expect_top1\"] is not None:\n",
    "                top1 = ((out0.get(\"plan\") or {}).get(\"sequence\") or [None])[0]\n",
    "                if top1 != case[\"expect_top1\"]:\n",
    "                    ok, why = False, f\"top1 expected {case['expect_top1']} got {top1}\"\n",
    "            if case.get(\"expect_ok\") is False:\n",
    "                if (out0.get(\"verify\") or {}).get(\"ok\") is True:\n",
    "                    ok, why = False, f\"expected verify.ok False, got True\"\n",
    "                else:\n",
    "                    blob = tokens_from_output(out0)\n",
    "                    if case.get(\"reason_any\") and not any(r in blob for r in case[\"reason_any\"]):\n",
    "                        ok, why = False, f\"expected reason to contain one of {case['reason_any']}; got '{blob}'\"\n",
    "\n",
    "            row = {\n",
    "                \"name\": case[\"name\"] + (f\"_p{idx}\" if idx else \"\"),\n",
    "                \"prompt\": p,\n",
    "                \"stable\": stable,\n",
    "                \"hashes\": hashes,\n",
    "                \"ok\": ok,\n",
    "                \"why\": why,\n",
    "                \"output\": sanitize_output(out0),   # <-- use the sanitizer here\n",
    "            }\n",
    "            rows.append(row)\n",
    "\n",
    "            if not ok:\n",
    "                failures.append(row[\"name\"] + \": \" + why)\n",
    "\n",
    "    # Write lightweight artifacts\n",
    "    (ARTDIR / \"rows.jsonl\").write_text(\n",
    "        \"\\n\".join(json.dumps(r, default=_json_default) for r in rows)\n",
    "    )\n",
    "    summary = {\n",
    "        \"time\": int(time.time()),\n",
    "        \"total\": len(rows),\n",
    "        \"failures\": failures,\n",
    "        \"stable_frac\": sum(1 for r in rows if r[\"stable\"]) / max(1, len(rows)),\n",
    "    }\n",
    "    (ARTDIR / \"summary.json\").write_text(json.dumps(summary, indent=2))\n",
    "    print(json.dumps(summary, indent=2))\n",
    "    if failures:\n",
    "        print(\"Failures:\")\n",
    "        for f in failures:\n",
    "            print(\" -\", f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa59e89e-1cb1-4b6e-9900-328c31625439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verify: None\n",
      "flags : None\n",
      "plan  : dict_keys([]) None\n",
      "aux   : []\n",
      "{\"ok\": false, \"label\": \"abstain\", \"score\": 0.9983419190704078, \"reason\": \"low_confidence\", \"artifacts\": {\"mapper\": {\"intent\": null, \"score\": 0.9983419190704078, \"topk\": [[\"deposit_asset\", 0.9983419190704078], [\"borrow_asset\", 0.001658080929592077], [\"claim_rewards\", 0.0], [\"repay_asset\", 0.0], [\"stake_asset\", 0.0], [\"swap_asset\", 0.0], [\"unstake_asset\", 0.0], [\"withdraw_asset\", 0.0]], \"reason\": \"low_confidence\", \"aux\": null}}} ...\n"
     ]
    }
   ],
   "source": [
    "dbg = run_once_strict(\"deposit 10 ETH into aave\", T=180, rails=\"stage11\")\n",
    "print(\"verify:\", dbg.get(\"verify\"))\n",
    "print(\"flags :\", dbg.get(\"flags\"))\n",
    "print(\"plan  :\", (dbg.get(\"plan\") or {}).keys(), (dbg.get(\"plan\") or {}).get(\"sequence\"))\n",
    "print(\"aux   :\", list((dbg.get(\"aux\") or {}).keys()))\n",
    "print(json.dumps(sanitize_output(dbg), default=_json_default)[:800], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64799ed4-a6dc-4b18-91d2-04d7bc2dbbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DeFi exec ===\n",
      "{\n",
      "  \"time\": 1758580901,\n",
      "  \"total\": 4,\n",
      "  \"failures\": [\n",
      "    \"ok_deposit: top1 expected deposit_asset got None\",\n",
      "    \"ok_deposit_p1: top1 expected deposit_asset got None\",\n",
      "    \"ok_swap: top1 expected swap_asset got None\",\n",
      "    \"ok_swap_p1: top1 expected swap_asset got None\"\n",
      "  ],\n",
      "  \"stable_frac\": 1.0\n",
      "}\n",
      "Failures:\n",
      " - ok_deposit: top1 expected deposit_asset got None\n",
      " - ok_deposit_p1: top1 expected deposit_asset got None\n",
      " - ok_swap: top1 expected swap_asset got None\n",
      " - ok_swap_p1: top1 expected swap_asset got None\n",
      "\n",
      "Artifacts written to: /Users/ian_moore/repos/micro-lm/.artifacts/notebook_stage8_defi\n"
     ]
    }
   ],
   "source": [
    "import os, warnings\n",
    "os.environ.setdefault(\"TRANSFORMERS_NO_ADVISORY_WARNINGS\", \"1\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "print(\"=== DeFi exec ===\")\n",
    "run_suite(DEFI_EXEC, runs=3, do_perturb=True, perturb_k=1)\n",
    "\n",
    "# print(\"\\n=== DeFi edges ===\")\n",
    "# run_suite(DEFI_EDGES, runs=3, do_perturb=True, perturb_k=1)\n",
    "\n",
    "print(\"\\nArtifacts written to:\", ARTDIR.resolve())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
