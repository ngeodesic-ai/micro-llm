{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b16f8897-94df-4c5b-a445-b8368867953c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal M8: load → score → sweep thresholds → pick → inspect\n",
    "# Paste this whole cell into a Jupyter notebook.\n",
    "\n",
    "import json, csv, os, math, joblib, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "cwd =  os.getcwd().replace(\"/notebooks\",\"\")\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "90df5632-0aff-4ea9-9bcc-9b27b886128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- I/O ----------------\n",
    "\n",
    "def read_prompts_jsonl(fp: str) -> List[str]:\n",
    "    rows = []\n",
    "    with open(fp, \"r\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                J = json.loads(line)\n",
    "                p = (J.get(\"prompt\") if isinstance(J, dict) else None) or line.strip()\n",
    "                p = str(p).strip()\n",
    "                if p:\n",
    "                    rows.append(p)\n",
    "            except Exception:\n",
    "                s = line.strip()\n",
    "                if s:\n",
    "                    rows.append(s)\n",
    "    return rows\n",
    "\n",
    "def read_labels_csv(fp: str) -> Dict[str, str]:\n",
    "    if not fp:\n",
    "        return {}\n",
    "    gold = {}\n",
    "    try:\n",
    "        df = pd.read_csv(fp)\n",
    "        for _, r in df.iterrows():\n",
    "            p = str(r[\"prompt\"]).strip()\n",
    "            y = str(r[\"label\"]).strip()\n",
    "            if p:\n",
    "                gold[p] = y\n",
    "    except Exception:\n",
    "        with open(fp, newline=\"\") as f:\n",
    "            r = csv.DictReader(f)\n",
    "            for row in r:\n",
    "                p = str(row.get(\"prompt\",\"\")).strip()\n",
    "                y = str(row.get(\"label\",\"\")).strip()\n",
    "                if p:\n",
    "                    gold[p] = y\n",
    "    return gold\n",
    "\n",
    "# ------------- Scoring helpers -------------\n",
    "\n",
    "def predict_scores(mapper, prompts: List[str], class_names: List[str]) -> List[Dict[str,float]]:\n",
    "    \"\"\"\n",
    "    Returns per-prompt score dictionaries (class -> probability-like score).\n",
    "    Logic matches the robust fallbacks used in your M8 script.\n",
    "    \"\"\"\n",
    "    scores: List[Dict[str,float]] = []\n",
    "    # Try predict_proba first\n",
    "    if hasattr(mapper, \"predict_proba\"):\n",
    "        probs = mapper.predict_proba(prompts)\n",
    "        classes = list(getattr(mapper, \"classes_\", class_names))\n",
    "        for row in probs:\n",
    "            row_map = {str(c): float(p) for c, p in zip(classes, row)}\n",
    "            for cname in class_names: row_map.setdefault(cname, 0.0)\n",
    "            scores.append(row_map)\n",
    "        return scores\n",
    "    # Then decision_function → softmax\n",
    "    if hasattr(mapper, \"decision_function\"):\n",
    "        logits = mapper.decision_function(prompts)\n",
    "        logits = np.array(logits, dtype=float)\n",
    "        if logits.ndim == 1: logits = logits.reshape(-1, 1)\n",
    "        classes = list(getattr(mapper, \"classes_\", class_names))\n",
    "        for row in logits:\n",
    "            ex = np.exp(row - row.max())\n",
    "            prob = ex / (ex.sum() + 1e-12)\n",
    "            row_map = {str(c): float(p) for c, p in zip(classes, prob)}\n",
    "            for cname in class_names: row_map.setdefault(cname, 0.0)\n",
    "            scores.append(row_map)\n",
    "        return scores\n",
    "    # Fallback: predict-only\n",
    "    preds = mapper.predict(prompts)\n",
    "    for y in preds:\n",
    "        row_map = {c: 0.0 for c in class_names}\n",
    "        row_map[str(y)] = 1.0\n",
    "        scores.append(row_map)\n",
    "    return scores\n",
    "\n",
    "def metrics_for_threshold(prompts: List[str],\n",
    "                          scores: List[Dict[str,float]],\n",
    "                          class_names: List[str],\n",
    "                          thr: float,\n",
    "                          gold: Dict[str,str]) -> Dict[str,Any]:\n",
    "    total = len(prompts); fired = 0; abstain = 0; correct = 0\n",
    "    for p, smap in zip(prompts, scores):\n",
    "        top = max(class_names, key=lambda c: smap.get(c, 0.0))\n",
    "        conf = smap.get(top, 0.0)\n",
    "        if conf >= thr:\n",
    "            fired += 1\n",
    "            if gold and gold.get(p) == top:\n",
    "                correct += 1\n",
    "        else:\n",
    "            abstain += 1\n",
    "    coverage = fired / max(1, total)\n",
    "    abstain_rate = abstain / max(1, total)\n",
    "    acc_on_fired = (correct / max(1, fired)) if fired else None\n",
    "    overall_acc = correct / max(1, total)\n",
    "    return dict(\n",
    "        threshold=thr, total=total,\n",
    "        abstain=abstain, abstain_rate=abstain_rate,\n",
    "        coverage=coverage, fired=fired,\n",
    "        correct_on_fired=correct,\n",
    "        accuracy_on_fired=acc_on_fired,\n",
    "        overall_correct=correct, overall_accuracy=overall_acc\n",
    "    )\n",
    "\n",
    "def choose_operating_point(metrics: List[Dict[str,Any]],\n",
    "                           max_abstain_rate: float = 0.10,\n",
    "                           choose_by: str = \"abstain_then_acc\") -> Dict[str,Any]:\n",
    "    admissible = [m for m in metrics if m[\"abstain_rate\"] <= max_abstain_rate]\n",
    "    if admissible:\n",
    "        # prefer higher thresholds; break ties by acc_on_fired\n",
    "        admissible.sort(key=lambda m: (m[\"threshold\"], m.get(\"accuracy_on_fired\") or 0.0), reverse=True)\n",
    "        return admissible[0]\n",
    "    if choose_by == \"utility\":\n",
    "        def util(m):\n",
    "            a = (m[\"accuracy_on_fired\"] or 0.0)\n",
    "            c = m[\"coverage\"]\n",
    "            return 0.0 if a == 0 or c == 0 else (2*a*c)/(a+c)\n",
    "        return max(metrics, key=util)\n",
    "    # default: minimize abstain, prefer higher acc_on_fired\n",
    "    metrics.sort(key=lambda m: (m[\"abstain_rate\"], -(m.get(\"accuracy_on_fired\") or 0.0)))\n",
    "    return metrics[0]\n",
    "\n",
    "def rows_at_threshold(prompts: List[str],\n",
    "                      scores: List[Dict[str,float]],\n",
    "                      class_names: List[str],\n",
    "                      thr: float,\n",
    "                      gold: Dict[str,str]) -> pd.DataFrame:\n",
    "    rows=[]\n",
    "    for p, smap in zip(prompts, scores):\n",
    "        top = max(class_names, key=lambda c: smap.get(c, 0.0))\n",
    "        conf = float(smap.get(top, 0.0))\n",
    "        fire = conf >= thr\n",
    "        rows.append(dict(prompt=p,\n",
    "                         gold_label=gold.get(p,\"\"),\n",
    "                         predicted=(top if fire else \"\"),\n",
    "                         confidence=conf,\n",
    "                         abstain=(not fire),\n",
    "                         threshold=thr))\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# ------------- One-call runner for notebooks -------------\n",
    "\n",
    "def m8_notebook_simple(\n",
    "    mapper_path: str,\n",
    "    prompts_jsonl: str,\n",
    "    labels_csv: str = \"\",\n",
    "    class_names: List[str] = (\"deposit_asset\",\"withdraw_asset\",\"swap_asset\",\"check_balance\"),\n",
    "    thresholds: List[float] = (0.5,0.6,0.7,0.8,0.9),\n",
    "    max_abstain_rate: float = 0.10,\n",
    "    min_overall_acc: float | None = 0.85,  # ignored if no labels\n",
    "    choose_by: str = \"abstain_then_acc\",\n",
    "):\n",
    "    # Load artifacts\n",
    "    mapper = joblib.load(mapper_path)\n",
    "    prompts = read_prompts_jsonl(prompts_jsonl)\n",
    "    gold = read_labels_csv(labels_csv) if labels_csv else {}\n",
    "\n",
    "    # Score once\n",
    "    scores = predict_scores(mapper, prompts, list(class_names))\n",
    "\n",
    "    # Sweep\n",
    "    per_thr = [metrics_for_threshold(prompts, scores, list(class_names), float(t), gold)\n",
    "               for t in thresholds]\n",
    "    metrics_df = pd.DataFrame(per_thr).sort_values(\"threshold\")\n",
    "\n",
    "    # Choose operating point\n",
    "    chosen = choose_operating_point(per_thr, max_abstain_rate=max_abstain_rate, choose_by=choose_by)\n",
    "\n",
    "    # Gate status (if labels provided and min_overall_acc is set)\n",
    "    has_labels = bool(gold)\n",
    "    pass_abstain = (chosen[\"abstain_rate\"] <= max_abstain_rate)\n",
    "    pass_accuracy = True if (not has_labels or min_overall_acc is None) else \\\n",
    "                    ((chosen.get(\"overall_accuracy\") or 0.0) >= float(min_overall_acc))\n",
    "    status = \"pass\" if (pass_abstain and pass_accuracy) else \"fail\"\n",
    "\n",
    "    # Per-row table at the chosen threshold (easy drilldown)\n",
    "    rows_df = rows_at_threshold(prompts, scores, list(class_names), chosen[\"threshold\"], gold)\n",
    "\n",
    "    # Pretty print summary\n",
    "    display_cols = [\"threshold\",\"abstain_rate\",\"coverage\",\"accuracy_on_fired\",\"overall_accuracy\",\"fired\",\"abstain\",\"total\"]\n",
    "    print(\"[M8] chosen:\", {k: chosen.get(k) for k in display_cols})\n",
    "    print(\"[M8] status:\", status)\n",
    "\n",
    "    return dict(\n",
    "        status=status,\n",
    "        metrics=metrics_df,\n",
    "        rows=rows_df,\n",
    "        chosen=chosen,\n",
    "        has_labels=has_labels,\n",
    "    )\n",
    "\n",
    "# ---------------- Example call (edit paths as needed) ----------------\n",
    "# result = m8_notebook_simple(\n",
    "#     mapper_path=\".artifacts/defi_mapper_embed.joblib\",\n",
    "#     prompts_jsonl=\"tests/fixtures/defi/defi_mapper_5k_prompts.jsonl\",\n",
    "#     labels_csv=\"tests/fixtures/defi/defi_mapper_labeled_5k.csv\",  # or \"\" if unlabeled\n",
    "#     thresholds=[0.2,0.25,0.3,0.35,0.4],\n",
    "#     max_abstain_rate=0.20,\n",
    "#     min_overall_acc=0.85,\n",
    "#     choose_by=\"utility\",  # or \"abstain_then_acc\"\n",
    "# )\n",
    "# display(result[\"metrics\"].head())\n",
    "# display(result[\"rows\"].head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
