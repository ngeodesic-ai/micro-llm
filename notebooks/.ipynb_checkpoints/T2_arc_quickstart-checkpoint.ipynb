{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "739dd57a-6ddf-43cd-b33a-64f96c73df45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root: /Users/ian_moore/repos/micro-lm\n",
      "Using src: /Users/ian_moore/repos/micro-lm/src\n",
      "micro_lm loaded from: /Users/ian_moore/repos/micro-lm/src/micro_lm/__init__.py\n",
      "quickstart loaded OK from src.\n"
     ]
    }
   ],
   "source": [
    "import os, sys, pathlib, importlib, glob\n",
    "\n",
    "# 1) Point to repo root and src/\n",
    "ROOT = pathlib.Path.cwd()\n",
    "if not (ROOT / \"src\" / \"micro_lm\").exists():\n",
    "    for _ in range(6):\n",
    "        ROOT = ROOT.parent\n",
    "        if (ROOT / \"src\" / \"micro_lm\").exists():\n",
    "            break\n",
    "SRC = ROOT / \"src\"\n",
    "print(\"Repo root:\", ROOT)\n",
    "print(\"Using src:\", SRC)\n",
    "\n",
    "# 2) Ensure src/ is first on sys.path\n",
    "if str(SRC) in sys.path:\n",
    "    sys.path.remove(str(SRC))\n",
    "sys.path.insert(0, str(SRC))\n",
    "\n",
    "# 3) Make sure packages are *real* packages (create __init__.py if missing)\n",
    "PKG = SRC / \"micro_lm\"\n",
    "CLI = PKG / \"cli\"\n",
    "for p in (PKG, CLI):\n",
    "    if p.exists() and not (p / \"__init__.py\").exists():\n",
    "        (p / \"__init__.py\").write_text(\"\")  # minimal package\n",
    "        print(\"Created:\", p / \"__init__.py\")\n",
    "\n",
    "# 4) Purge any previously-imported installed micro_lm modules\n",
    "for k in list(sys.modules.keys()):\n",
    "    if k == \"micro_lm\" or k.startswith(\"micro_lm.\"):\n",
    "        del sys.modules[k]\n",
    "importlib.invalidate_caches()\n",
    "\n",
    "# 5) Import micro_lm *from src* and then the CLI module\n",
    "import micro_lm\n",
    "print(\"micro_lm loaded from:\", micro_lm.__file__)\n",
    "\n",
    "from micro_lm.cli.defi_quickstart import quickstart\n",
    "print(\"quickstart loaded OK from src.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4129a549-389d-456f-ac3e-245622b6ddd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDD] prompt='deposit 10 ETH into aave' | seq=['deposit_asset']\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "[WDD] act=deposit layer=-5 sigma=4 proto_w=13 mf_peak=6.953530481900707 keep_len=0 decision=PASS\n",
      "[WDD] fallback: MF_peak=6.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"prompt\": \"deposit 10 ETH into aave\",\n",
      "  \"domain\": \"defi\",\n",
      "  \"rails\": \"stage11\",\n",
      "  \"T\": 180,\n",
      "  \"top1\": \"deposit_asset\",\n",
      "  \"sequence\": [\n",
      "    \"deposit_asset\"\n",
      "  ],\n",
      "  \"plan\": {\n",
      "    \"sequence\": [\n",
      "      \"deposit_asset\"\n",
      "    ]\n",
      "  },\n",
      "  \"verify\": {\n",
      "    \"ok\": true,\n",
      "    \"reason\": \"shim:accept:stage-4\",\n",
      "    \"tags\": [\n",
      "      \"rails:stage11\",\n",
      "      \"wdd:on\",\n",
      "      \"audit:wdd\"\n",
      "    ]\n",
      "  },\n",
      "  \"flags\": {},\n",
      "  \"aux\": {\n",
      "    \"stage11\": {\n",
      "      \"wdd\": {\n",
      "        \"decision\": \"PASS\",\n",
      "        \"sigma\": 4,\n",
      "        \"proto_w\": 13,\n",
      "        \"which_prior\": \"deposit(L-5)\",\n",
      "        \"mf_peak\": 6.953530481900707,\n",
      "        \"keep\": []\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"det_hash\": \"f1378c645f25\",\n",
      "  \"wdd_summary\": {\n",
      "    \"decision\": \"PASS\",\n",
      "    \"keep\": [],\n",
      "    \"sigma\": 4,\n",
      "    \"proto_w\": 13,\n",
      "    \"which_prior\": \"deposit(L-5)\",\n",
      "    \"note\": \"fallback: MF_peak=6.953530481900707\"\n",
      "  },\n",
      "  \"abstained\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "\n",
    "# Show WDD debug lines (same as setting MICRO_LM_WDD_DEBUG=1 on the CLI)\n",
    "os.environ[\"MICRO_LM_WDD_DEBUG\"] = \"1\"\n",
    "\n",
    "from micro_lm.cli.defi_quickstart import quickstart\n",
    "\n",
    "prompt = \"deposit 10 ETH into aave\"\n",
    "policy = {\"audit\": {\"backend\": \"wdd\"}}   # same as --policy '{\"audit\":{\"backend\":\"wdd\"}}'\n",
    "rails  = \"stage11\"\n",
    "\n",
    "# If your build requires the explicit switch for detector mode, set use_wdd=True.\n",
    "# Otherwise you can omit it.\n",
    "out = quickstart(\n",
    "    prompt,\n",
    "    policy=policy,\n",
    "    rails=rails,\n",
    "    T=180,\n",
    "    use_wdd=True,       # <- set to True if WDD needs to be explicitly invoked\n",
    "    verbose=True,       # matches --verbose\n",
    ")\n",
    "\n",
    "print(json.dumps(out, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
