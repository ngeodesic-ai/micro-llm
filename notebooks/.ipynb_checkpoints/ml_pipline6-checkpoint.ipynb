{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "915a4539-cf38-4919-84d0-cda9c362e18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import json, csv, sys, os, time\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- sklearn bits\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# --- sentence-transformers for embeddings\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except Exception as e:\n",
    "    SentenceTransformer = None\n",
    "    print(\"[WARN] sentence-transformers not importable right now. Install it to train/encode.\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0973c2e9-6069-4c6e-91e7-7841cb39522a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- SBERT encoder (pipeline compatible) ----------------\n",
    "class SBERTEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Lightweight sentence-embedding transformer for sklearn pipelines.\"\"\"\n",
    "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\", batch_size: int = 64, normalize: bool = True):\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.normalize = normalize\n",
    "        self._model = None\n",
    "\n",
    "    def _ensure_model(self):\n",
    "        if self._model is None:\n",
    "            if SentenceTransformer is None:\n",
    "                raise RuntimeError(\"sentence-transformers is required to encode prompts.\")\n",
    "            self._model = SentenceTransformer(self.model_name)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self._ensure_model()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        self._ensure_model()\n",
    "        embs = self._model.encode(\n",
    "            list(X),\n",
    "            batch_size=self.batch_size,\n",
    "            show_progress_bar=False,\n",
    "            normalize_embeddings=self.normalize,\n",
    "        )\n",
    "        return np.asarray(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66c70911-57fd-40e1-9cd1-282e014e4fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ I/O helpers ------------------------\n",
    "def _read_prompts_jsonl(path: str) -> List[str]:\n",
    "    prompts: List[str] = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                rec = json.loads(line)\n",
    "                p = rec.get(\"prompt\", \"\").strip()\n",
    "                if p:\n",
    "                    prompts.append(p)\n",
    "            except Exception:\n",
    "                # tolerate occasional garbage lines\n",
    "                continue\n",
    "    return prompts\n",
    "\n",
    "def _read_labels_csv(path: str) -> Dict[str, str]:\n",
    "    gold: Dict[str, str] = {}\n",
    "    df = pd.read_csv(path)\n",
    "    if not {\"prompt\", \"label\"}.issubset(df.columns):\n",
    "        raise ValueError(f\"labels_csv must have columns ['prompt','label'], got {df.columns.tolist()}\" )\n",
    "    for _, row in df.iterrows():\n",
    "        p = str(row[\"prompt\"]).strip()\n",
    "        y = str(row[\"label\"]).strip()\n",
    "        if p:\n",
    "            gold[p] = y\n",
    "    return gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09c61fca-6301-43c7-9fb7-7143b93abcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ Training ------------------------\n",
    "def train_mapper(\n",
    "    labels_csv: str,\n",
    "    out_path: str = \".artifacts/defi_mapper.joblib\",\n",
    "    sbert_model: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    C: float = 8.0,\n",
    "    max_iter: int = 2000,\n",
    "    calibrate: bool = True,\n",
    "    calibration_method: str = \"auto\",  # 'auto' | 'isotonic' | 'sigmoid'\n",
    "    calibration_cv: int = 3,\n",
    ") -> str:\n",
    "    \"\"\"Train a SBERT + LogisticRegression pipeline, optionally calibrated, and dump to joblib.\"\"\"\n",
    "    os.makedirs(os.path.dirname(out_path) or \".\", exist_ok=True)\n",
    "\n",
    "    df = pd.read_csv(labels_csv)\n",
    "    need = {\"prompt\",\"label\"}\n",
    "    if not need.issubset(df.columns):\n",
    "        raise SystemExit(f\"[train_mapper] labels_csv must have columns {need}, got {df.columns.tolist()}\" )\n",
    "    df = df.dropna(subset=[\"prompt\",\"label\"]).copy()\n",
    "    df[\"prompt\"] = df[\"prompt\"].astype(str).str.strip()\n",
    "    df[\"label\"]  = df[\"label\"].astype(str).str.strip()\n",
    "    df = df[df[\"prompt\"].str.len() > 0]\n",
    "    if df.empty:\n",
    "        raise SystemExit(\"[train_mapper] No non-empty prompts after cleaning.\")\n",
    "\n",
    "    X = df[\"prompt\"].tolist()\n",
    "    y = df[\"label\"].tolist()\n",
    "\n",
    "    base = LogisticRegression(max_iter=max_iter, C=C, class_weight=\"balanced\", random_state=0)\n",
    "\n",
    "    model = base\n",
    "    if calibrate:\n",
    "        # pick a safe calibration automatically for tiny classes\n",
    "        from collections import Counter\n",
    "        cnt = Counter(y); m = min(cnt.values())\n",
    "        method = calibration_method; cv = calibration_cv\n",
    "        if method == \"auto\":\n",
    "            if m >= max(3, cv):\n",
    "                method, cv = \"isotonic\", max(3, cv)\n",
    "            elif m >= 2:\n",
    "                method, cv = \"sigmoid\", max(2, min(m, cv))\n",
    "            else:\n",
    "                print(\"[train_mapper] Not enough per-class samples for calibration; skipping.\", file=sys.stderr)\n",
    "                method = None\n",
    "        if method in (\"isotonic\",\"sigmoid\"):\n",
    "            try:\n",
    "                model = CalibratedClassifierCV(estimator=base, method=method, cv=cv)  # sklearn>=1.3\n",
    "            except TypeError:\n",
    "                model = CalibratedClassifierCV(base_estimator=base, method=method, cv=cv)  # older sklearn\n",
    "\n",
    "    pipe = make_pipeline(SBERTEncoder(sbert_model), model)\n",
    "    pipe.fit(X, y)\n",
    "    joblib.dump(pipe, out_path)\n",
    "    print(f\"[train_mapper] wrote: {out_path}  (n={len(X)})\")\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4059881-0d71-45ac-b59e-a9d89112786f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ Prediction & Metrics ------------------------\n",
    "@dataclass\n",
    "class PredictResult:\n",
    "    rows_csv: Optional[str]\n",
    "    metrics: Optional[dict]\n",
    "\n",
    "\n",
    "def _predict_proba(mapper, prompts: List[str]) -> Tuple[List[str], np.ndarray]:\n",
    "    if hasattr(mapper, \"classes_\"):\n",
    "        classes = list(map(str, mapper.classes_))\n",
    "    else:\n",
    "        # try to infer from predict_proba later\n",
    "        classes = None\n",
    "\n",
    "    if hasattr(mapper, \"predict_proba\"):\n",
    "        probs = mapper.predict_proba(prompts)\n",
    "        probs = np.asarray(probs, dtype=float)\n",
    "        if classes is None:\n",
    "            classes = list(map(str, getattr(mapper, \"classes_\", [])))\n",
    "        return classes, probs\n",
    "\n",
    "    # decision_function -> softmax fallback\n",
    "    if hasattr(mapper, \"decision_function\"):\n",
    "        logits = mapper.decision_function(prompts)\n",
    "        logits = np.asarray(logits, dtype=float)\n",
    "        if logits.ndim == 1:\n",
    "            logits = logits.reshape(-1, 1)\n",
    "        ex = np.exp(logits - logits.max(axis=1, keepdims=True))\n",
    "        probs = ex / (ex.sum(axis=1, keepdims=True) + 1e-12)\n",
    "        if classes is None:\n",
    "            classes = list(map(str, getattr(mapper, \"classes_\", [])))\n",
    "        return classes, probs\n",
    "\n",
    "    # predict-only fallback (degenerate probs)\n",
    "    preds = np.array(mapper.predict(prompts), dtype=object).reshape(-1, 1)\n",
    "    classes = list(sorted(set(map(str, preds.flatten().tolist()))))\n",
    "    idx = {c:i for i,c in enumerate(classes)}\n",
    "    probs = np.zeros((len(prompts), len(classes)), dtype=float)\n",
    "    for r, y in enumerate(preds.flatten().tolist()):\n",
    "        probs[r, idx[str(y)]] = 1.0\n",
    "    return classes, probs\n",
    "\n",
    "\n",
    "def predict_prompts(\n",
    "    mapper_path: str,\n",
    "    prompts_jsonl: str,\n",
    "    labels_csv_pred: Optional[str] = None,\n",
    "    threshold: float = 0.6,\n",
    "    out_rows_csv: Optional[str] = None,\n",
    ") -> PredictResult:\n",
    "    \"\"\"Load mapper, score prompts, write per-row CSV, and (optionally) compute quick metrics.\"\"\"\n",
    "    mapper = joblib.load(mapper_path)\n",
    "    prompts = _read_prompts_jsonl(prompts_jsonl)\n",
    "    classes, probs = _predict_proba(mapper, prompts)\n",
    "\n",
    "    top_idx = probs.argmax(axis=1)\n",
    "    top_conf = probs.max(axis=1)\n",
    "    fired = (top_conf >= threshold)\n",
    "    preds = np.array([classes[i] for i in top_idx], dtype=object)\n",
    "    pred_labels = np.where(fired, preds, \"\")\n",
    "\n",
    "    rows = []\n",
    "    if labels_csv_pred:\n",
    "        gold = _read_labels_csv(labels_csv_pred)\n",
    "    else:\n",
    "        gold = {}\n",
    "\n",
    "    for p, yhat, conf, fire in zip(prompts, pred_labels, top_conf, fired):\n",
    "        rows.append({\n",
    "            \"prompt\": p,\n",
    "            \"predicted\": yhat,\n",
    "            \"confidence\": float(conf),\n",
    "            \"abstain\": (not bool(fire)),\n",
    "            \"threshold\": float(threshold),\n",
    "            \"gold_label\": gold.get(p, \"\")\n",
    "        })\n",
    "\n",
    "    rows_csv_path = None\n",
    "    if out_rows_csv:\n",
    "        os.makedirs(os.path.dirname(out_rows_csv) or \".\", exist_ok=True)\n",
    "        import csv as _csv\n",
    "        with open(out_rows_csv, \"w\", newline=\"\") as f:\n",
    "            w = _csv.DictWriter(f, fieldnames=[\"prompt\",\"gold_label\",\"predicted\",\"confidence\",\"abstain\",\"threshold\"]\n",
    "            )\n",
    "            w.writeheader()\n",
    "            for r in rows:\n",
    "                w.writerow({k: r[k] for k in w.fieldnames})\n",
    "        rows_csv_path = out_rows_csv\n",
    "        print(f\"[predict] wrote rows: {rows_csv_path}  (n={len(rows)})\")\n",
    "\n",
    "    # quick metrics (if gold labels provided)\n",
    "    metrics = None\n",
    "    if gold:\n",
    "        total = len(prompts)\n",
    "        abstain_ct = sum(1 for r in rows if r[\"abstain\"])\n",
    "        fired_ct   = total - abstain_ct\n",
    "        correct_on_fired = sum(1 for r in rows if (not r[\"abstain\"]) and r[\"predicted\"] == r[\"gold_label\"])\n",
    "        overall_correct  = sum(1 for r in rows if r[\"predicted\"] == r[\"gold_label\"])  # empty pred never equals gold\n",
    "\n",
    "        accuracy_on_fired = (correct_on_fired / fired_ct) if fired_ct else None\n",
    "        overall_accuracy  = overall_correct / total if total else None\n",
    "\n",
    "        metrics = {\n",
    "            \"threshold\": float(threshold),\n",
    "            \"total\": total,\n",
    "            \"abstain\": abstain_ct,\n",
    "            \"abstain_rate\": abstain_ct / total if total else None,\n",
    "            \"coverage\": fired_ct / total if total else None,\n",
    "            \"fired\": fired_ct,\n",
    "            \"correct_on_fired\": correct_on_fired,\n",
    "            \"accuracy_on_fired\": accuracy_on_fired,\n",
    "            \"overall_correct\": overall_correct,\n",
    "            \"overall_accuracy\": overall_accuracy,\n",
    "        }\n",
    "        print(\"[metrics]\", metrics)\n",
    "\n",
    "    return PredictResult(rows_csv=rows_csv_path, metrics=metrics)\n",
    "\n",
    "# ------------------------ CLI (optional) ------------------------\n",
    "def _as_bool(x: str) -> bool:\n",
    "    return str(x).strip().lower() in {\"1\",\"true\",\"t\",\"yes\",\"y\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8bbd4f0-91df-42f9-b74c-48b4adb5d9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, csv, json, os, sys, time\n",
    "def get_args():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--backend\",       default=\"sbert\", help=\"wordmap|sbert\")\n",
    "    ap.add_argument(\"--model_path\",    default=\".artifacts/defi_mapper.joblib\")\n",
    "    ap.add_argument(\"--prompts_jsonl\", default=\"tests/fixtures/defi/defi_mapper_5k_prompts.json\")\n",
    "    ap.add_argument(\"--labels_csv_pred\", default=\"tests/fixtures/defi/defi_mapper_labeled_5k.csv\")\n",
    "    ap.add_argument(\"--train_labels_csv\", default=\"tests/fixtures/defi/defi_mapper_labeled_large.csv\")\n",
    "    ap.add_argument(\"--thresholds\",    default=\"0.5,0.55,0.6,0.65,0.7\")\n",
    "    ap.add_argument(\"--max_iter\",    default=\"2000\")\n",
    "    ap.add_argument(\"--C\",    default=\"8\")\n",
    "    ap.add_argument(\"--calibrate\",    default=\"True\")\n",
    "    ap.add_argument(\"--calibration_method\", choices=[\"auto\",\"isotonic\",\"sigmoid\"], default=\"auto\")\n",
    "    ap.add_argument(\"--calibration_cv\", type=int, default=3)\n",
    "    ap.add_argument(\"--sbert_model\", default=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    ap.add_argument(\"--threshold\", type=float, default=0.6)\n",
    "    ap.add_argument(\"--out_path\", default=\"defi_mapper_embed.joblib\")\n",
    "    ap.add_argument(\"--out_dir\",       default=\"\")\n",
    "    ap.add_argument(\"--out_rows_csv\", default=\".artifacts/m8_rows_simple.csv\")\n",
    "    ap.add_argument(\"--min_overall_acc\", default=None)\n",
    "    \n",
    "    notebook_args = [\n",
    "        \"--backend\", \"sbert\",\n",
    "        \"--model_path\", \".artifacts/defi_mapper.joblib\",\n",
    "        \"--prompts_jsonl\", \"tests/fixtures/defi/defi_mapper_5k_prompts.jsonl\",\n",
    "        \"--labels_csv_pred\",    \"tests/fixtures/defi/defi_mapper_labeled_5k.csv\",\n",
    "        \"--train_labels_csv\", \"tests/fixtures/defi/defi_mapper_labeled_large.csv\",\n",
    "        \"--thresholds\", \"0.5,0.55,0.6,0.65,.7\",\n",
    "        \"--max_iter\", \"2000\",\n",
    "        \"--C\", \"8\",\n",
    "        \"--calibrate\", \"True\",\n",
    "        \"--calibration_method\", \"auto\",\n",
    "        \"--calibration_cv\", \"3\",\n",
    "        \"--sbert_model\", \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        \"--threshold\", \"0.5\",\n",
    "        \"--out_path\", \"defi_mapper_embed.joblib\",\n",
    "        \"--min_overall_acc\", \"0.75\",\n",
    "        \"--out_dir\", \".artifacts/defi/mapper_bench\",\n",
    "        \"--out_rows_csv\", \".artifacts/m8_rows_simple.csv\"\n",
    "        \n",
    "    ]\n",
    "    \n",
    "    return ap.parse_args(notebook_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b0ffa14-fa70-4801-8b79-ae3c50eb2117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train_mapper] wrote: defi_mapper_embed.joblib  (n=1000)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cwd =  os.getcwd().replace(\"/notebooks\",\"\")\n",
    "os.chdir(cwd)\n",
    "\n",
    "args = get_args()\n",
    "\n",
    "model_path = train_mapper(\n",
    "        labels_csv=args.train_labels_csv,\n",
    "        out_path=args.out_path,\n",
    "        sbert_model=args.sbert_model,\n",
    "        C=float(args.C),\n",
    "        max_iter=int(args.max_iter),\n",
    "        calibrate=_as_bool(args.calibrate),\n",
    "        calibration_method=args.calibration_method,\n",
    "        calibration_cv=args.calibration_cv\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07f48e35-46f1-4684-8ea4-4bb478abfec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[predict] wrote rows: .artifacts/m8_rows_simple.csv  (n=5000)\n",
      "[metrics] {'threshold': 0.5, 'total': 5000, 'abstain': 37, 'abstain_rate': 0.0074, 'coverage': 0.9926, 'fired': 4963, 'correct_on_fired': 4941, 'accuracy_on_fired': 0.995567197259722, 'overall_correct': 4941, 'overall_accuracy': 0.9882}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PredictResult(rows_csv='.artifacts/m8_rows_simple.csv', metrics={'threshold': 0.5, 'total': 5000, 'abstain': 37, 'abstain_rate': 0.0074, 'coverage': 0.9926, 'fired': 4963, 'correct_on_fired': 4941, 'accuracy_on_fired': 0.995567197259722, 'overall_correct': 4941, 'overall_accuracy': 0.9882})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = get_args()\n",
    "\n",
    "predict_prompts(\n",
    "        mapper_path=model_path,\n",
    "        prompts_jsonl=args.prompts_jsonl,\n",
    "        labels_csv_pred=(args.labels_csv_pred or None),\n",
    "        threshold=float(args.threshold),\n",
    "        out_rows_csv=args.out_rows_csv\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e983e8b-3863-4923-8fb8-c43c36318d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['claim_rewards', 'withdraw_asset', 'stake_asset', 'deposit_asset',\n",
       "       'deposit_asset'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prompts = ['sing me a lullaby',\n",
    "                      'what’s the weather in NYC?',\n",
    "                      'open settings',\n",
    "                      'convert centimeters to inches',\n",
    "                      'swap seats with me',\n",
    "                      'hey wats going on']\n",
    "\n",
    "# mapper\n",
    "mapper = joblib.load(model_path)\n",
    "test_prompts = _read_prompts_jsonl(args.prompts_jsonl)\n",
    "labeled_test_prompts = _read_labels_csv(args.labels_csv_pred)\n",
    "threshold = float(args.threshold)\n",
    "\n",
    "classes, probs = _predict_proba(mapper, test_prompts[0:5])\n",
    "\n",
    "top_idx = probs.argmax(axis=1)\n",
    "top_conf = probs.max(axis=1)\n",
    "fired = (top_conf >= threshold)\n",
    "preds = np.array([classes[i] for i in top_idx], dtype=object)\n",
    "pred_labels = np.where(fired, preds, \"\")\n",
    "pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f646b20d-a95f-40b4-a92b-42d8f83a59d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['claim_rewards', 'withdraw_asset', 'stake_asset', 'deposit_asset',\n",
       "       'deposit_asset'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapper\n",
    "mapper = joblib.load(model_path)\n",
    "test_prompts = _read_prompts_jsonl(args.prompts_jsonl)\n",
    "labeled_test_prompts = _read_labels_csv(args.labels_csv_pred)\n",
    "threshold = float(args.threshold)\n",
    "\n",
    "classes, probs = _predict_proba(mapper, test_prompts[0:5])\n",
    "\n",
    "top_idx = probs.argmax(axis=1)\n",
    "top_conf = probs.max(axis=1)\n",
    "fired = (top_conf >= threshold)\n",
    "preds = np.array([classes[i] for i in top_idx], dtype=object)\n",
    "pred_labels = np.where(fired, preds, \"\")\n",
    "pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd5d4573-832d-4035-a213-9298f4162049",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRIMITIVE = \"claim_rewards\"\n",
    "\n",
    "def get_train_prompts(N_TRAIN = 1000):\n",
    "\n",
    "    labeled_train_prompts_read = _read_labels_csv(args.train_labels_csv)\n",
    "    labeled_train_prompts = {}\n",
    "    train_prompts = []\n",
    "    train_labels = []\n",
    "    for k, g in enumerate(labeled_train_prompts_read):\n",
    "        \n",
    "        # if(labeled_test_prompts_read[g] == PRIMITIVE):\n",
    "        #     labeled_test_prompts[k] = g\n",
    "        #     test_labels.append(labeled_test_prompts_read[g])\n",
    "        #     test_prompts.append(g)\n",
    "        labeled_train_prompts[k] = g\n",
    "        train_labels.append(labeled_train_prompts_read[g])\n",
    "        train_prompts.append(g)\n",
    "        \n",
    "        if (len(labeled_train_prompts) == N_TRAIN):\n",
    "            break;\n",
    "\n",
    "    return train_prompts, train_labels\n",
    "\n",
    "def get_test_prompts(N_TEST = 25):\n",
    "    # get test prompts\n",
    "    labeled_test_prompts_read = _read_labels_csv(args.labels_csv_pred)\n",
    "    labeled_test_prompts = {}\n",
    "    test_prompts = []\n",
    "    test_labels = []\n",
    "    for k, g in enumerate(labeled_test_prompts_read):\n",
    "        \n",
    "        # if(labeled_test_prompts_read[g] == PRIMITIVE):\n",
    "        #     labeled_test_prompts[k] = g\n",
    "        #     test_labels.append(labeled_test_prompts_read[g])\n",
    "        #     test_prompts.append(g)\n",
    "        labeled_test_prompts[k] = g\n",
    "        test_labels.append(labeled_test_prompts_read[g])\n",
    "        test_prompts.append(g)\n",
    "        \n",
    "        if (len(labeled_test_prompts) == N_TEST):\n",
    "            break;\n",
    "            \n",
    "    return test_prompts, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ec695c9-6bdb-4fc3-8106-3c5b0e31805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    _SBERT_OK = True\n",
    "except Exception:\n",
    "    _SBERT_OK = False\n",
    "\n",
    "class SbertEmbedding:\n",
    "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        if not _SBERT_OK:\n",
    "            raise RuntimeError(\"sentence-transformers not available; pip install sentence-transformers\")\n",
    "        self.m = SentenceTransformer(model_name)\n",
    "    def encode_one(self, text: str) -> np.ndarray:\n",
    "        v = self.m.encode([text], normalize_embeddings=True)[0]\n",
    "        return v.astype(np.float32)\n",
    "\n",
    "def load_phrases():\n",
    "    default = {\n",
    "        \"deposit_asset\":  [\"deposit\",\"supply\",\"provide\",\"add liquidity\",\"provide liquidity\",\"top up\",\"put in\",\"add funds\",\"fund\",\"allocate\",\"contribute\",\"add position\",\"add to pool\",\"supply to pool\",\"add into\",\"supply into\"],\n",
    "        \"withdraw_asset\": [\"withdraw\",\"redeem\",\"unstake\",\"remove liquidity\",\"pull out\",\"take out\",\"cash out\",\"exit\",\"remove position\",\"remove from pool\",\"take from\",\"pull from\"],\n",
    "        \"swap_asset\":     [\"swap\",\"convert\",\"trade\",\"exchange\",\"convert into\",\"swap to\",\"swap into\",\"bridge\",\"wrap\",\"unwrap\",\"swap for\"],\n",
    "        \"borrow_asset\":   [\"borrow\",\"draw\",\"open a loan for\",\"open debt\",\"draw down\",\"take a loan\",\"borrow against\"],\n",
    "        \"repay_asset\":    [\"repay\",\"pay back\",\"close out the loan for\",\"settle loan\",\"pay debt\",\"repay debt\",\"close loan\"],\n",
    "        \"stake_asset\":    [\"stake\",\"lock\",\"bond\",\"delegate\",\"lock up\",\"stake into\",\"stake to\",\"stake on\"],\n",
    "        \"unstake_asset\":  [\"unstake\",\"unlock\",\"unbond\",\"undelegate\",\"release\",\"unstake from\",\"unstake out\"],\n",
    "        \"claim_rewards\":  [\"claim\",\"harvest\",\"collect rewards\",\"claim rewards\",\"collect staking rewards\",\"collect yield\",\"claim yield\",\"harvest rewards\"],         \n",
    "    }\n",
    "    return default\n",
    "\n",
    "def build_prototypes(emb, phrases_or_thresholds: dict):\n",
    "    \"\"\"\n",
    "    Accepts either:\n",
    "      A) phrases: {prim: [(phrase, score), (phrase, score), ...]}  or  {prim: [phrase, phrase, ...]}\n",
    "      B) thresholds: {prim: float}  (per-class threshold file)\n",
    "    For (B), we fall back to embedding the primitive name itself.\n",
    "    \"\"\"\n",
    "    proto = {}\n",
    "    for p, val in phrases_or_thresholds.items():\n",
    "        vecs = []\n",
    "        if isinstance(val, (int, float)):\n",
    "            # thresholds file -> fallback: use primitive token as prototype\n",
    "            vecs = [emb.encode_one(p)]\n",
    "        else:\n",
    "            # phrases file: list of phrases or (phrase,score)\n",
    "            if isinstance(val, list) and len(val) > 0:\n",
    "                for item in val:\n",
    "                    if isinstance(item, (list, tuple)) and len(item) > 0:\n",
    "                        phrase = item[0]\n",
    "                    else:\n",
    "                        phrase = item\n",
    "                    vecs.append(emb.encode_one(str(phrase)))\n",
    "            else:\n",
    "                # empty -> fallback\n",
    "                vecs = [emb.encode_one(p)]\n",
    "        proto[p] = np.stack(vecs, axis=0).mean(axis=0)\n",
    "    return proto\n",
    "    \n",
    "def cosine(a, b):\n",
    "    an = a / (np.linalg.norm(a) + 1e-8)\n",
    "    bn = b / (np.linalg.norm(b) + 1e-8)\n",
    "    return float(np.dot(an, bn))\n",
    "\n",
    "def spans_from_prompt(prompt, prototypes, emb, tau_span=0.55):\n",
    "    toks = prompt.strip().split()\n",
    "    spans = []\n",
    "    for n in range(1, min(6, len(toks))+1):\n",
    "        for i in range(0, len(toks)-n+1):\n",
    "            s = \" \".join(toks[i:i+n])\n",
    "            e = emb.encode_one(s)\n",
    "            for k, v in prototypes.items():\n",
    "                sc = max(0.0, cosine(e, v))\n",
    "                if sc >= tau_span:\n",
    "                    t_center = (i + n/2.0) / max(1.0, len(toks))  # normalized [0,1]\n",
    "                    spans.append({\"primitive\": k, \"term\": s, \"score\": round(sc,4), \"t_center\": round(t_center,4)})\n",
    "    # keep top 3 spans per primitive\n",
    "    by_prim = {}\n",
    "    for sp in spans:\n",
    "        by_prim.setdefault(sp[\"primitive\"], []).append(sp)\n",
    "    for k in by_prim:\n",
    "        by_prim[k] = sorted(by_prim[k], key=lambda x: x[\"score\"], reverse=True)[:3]\n",
    "    return by_prim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa1e5761-fdab-466a-8e61-aefa1d3a5442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 PASS / prompt: claim staking rewards on aave base / primitives: ['stake_asset', 'claim_rewards']\n",
      "1 PASS / prompt: pull out 2752.8264 ARB from lido / primitives: ['withdraw_asset']\n",
      "2 ABSTAIN* / prompt: restake 33.8529 MATIC with balancer on solana — this minute, use normal gas / primitives: [] / truth: stake_asset\n",
      "3 PASS / prompt: deposit 4697 USDC into uniswap on base / primitives: ['deposit_asset', 'swap_asset', 'unstake_asset']\n",
      "4 PASS / prompt: supply 7.0245 SOL to maker / primitives: ['deposit_asset']\n",
      "5 PASS / prompt: convert 7064 USDT into LINK via sushiswap on arbitrum — minimize gas, safe mode / primitives: ['swap_asset', 'deposit_asset']\n",
      "6 PASS / prompt: repay 2861.82 LINK on maker (optimism) — use normal gas / primitives: ['borrow_asset', 'repay_asset']\n",
      "7 PASS / prompt: collect incentives at balancer (base) — now / primitives: ['claim_rewards']\n",
      "8 ABSTAIN* / prompt: delegate 4020.4921 OP to pendle (arbitrum) / primitives: [] / truth: stake_asset\n",
      "9 ABSTAIN* / prompt: restake 1.876441 BTC with yearn on optimism — now / primitives: [] / truth: stake_asset\n",
      "10 ABSTAIN* / prompt: restake 39.238 MATIC with lido on avalanche — immediately / primitives: [] / truth: stake_asset\n",
      "11 PASS / prompt: claim staking rewards on sushiswap arbitrum — use fast gas / primitives: ['stake_asset', 'claim_rewards']\n",
      "12 PASS / prompt: reduce loan by 4808.8879 ARB on compound (solana) — this minute / primitives: ['borrow_asset', 'repay_asset']\n",
      "13 PASS / prompt: remove 36.8239 SOL from aave / primitives: ['withdraw_asset', 'unstake_asset']\n",
      "14 PASS / prompt: harvest yield from maker on ethereum / primitives: ['claim_rewards', 'withdraw_asset']\n",
      "15 PASS / prompt: collect incentives at compound (optimism) — use fast gas, normal mode / primitives: ['claim_rewards']\n",
      "16 PASS / prompt: lock 0.668373 WBTC in pendle on avalanche / primitives: ['stake_asset']\n",
      "17 PASS / prompt: unstake 786.2037 AAVE from lido on arbitrum / primitives: ['withdraw_asset', 'unstake_asset']\n",
      "18 ABSTAIN* / prompt: withdraw staked 0.667255 WBTC from pendle optimism — right away, use normal gas / primitives: ['withdraw_asset', 'stake_asset'] / truth: unstake_asset\n",
      "19 PASS / prompt: collect incentives at maker (solana) — asap, use normal gas, normal mode / primitives: ['claim_rewards']\n",
      "20 PASS / prompt: convert 704.7582 AAVE into USDT via uniswap on arbitrum — minimize gas / primitives: ['swap_asset', 'deposit_asset', 'unstake_asset']\n",
      "21 PASS / prompt: harvest yield from balancer on solana — high yield mode / primitives: ['claim_rewards', 'withdraw_asset']\n",
      "22 PASS / prompt: borrow 19.5336 AVAX from aave on base using DAI as collateral / primitives: ['borrow_asset', 'repay_asset', 'withdraw_asset']\n",
      "23 PASS / prompt: swap 860.4607 ARB to WBTC on sushiswap arbitrum with slippage 0.1% — asap / primitives: ['swap_asset']\n",
      "24 PASS / prompt: cash out 0.195658 BTC from pendle (base) / primitives: ['withdraw_asset']\n"
     ]
    }
   ],
   "source": [
    "import json, math, hashlib\n",
    "import numpy as np\n",
    "\n",
    "# ---------- utilities ----------\n",
    "def kaiser_window(L=160, beta=8.6):\n",
    "    # smooth, MF-friendly lobe (unit norm)\n",
    "    n = np.arange(L)\n",
    "    w = np.i0(beta * np.sqrt(1 - ((2*n)/(L-1) - 1)**2))\n",
    "    w = w / (np.linalg.norm(w) + 1e-9)\n",
    "    return w.astype(\"float32\")\n",
    "\n",
    "def matched_filter_scores(traces, q):\n",
    "    scores, nulls, peaks = {}, {}, {}\n",
    "    L = len(q)\n",
    "    for k, x in traces.items():\n",
    "        if len(x) < L:  # pad if needed\n",
    "            x = np.pad(x, (0, L - len(x)))\n",
    "        # convolution as correlation (flip q)\n",
    "        r = np.convolve(x, q[::-1], mode=\"valid\")\n",
    "        peak = float(r.max()) if r.size else 0.0\n",
    "        scores[k] = peak\n",
    "        nulls[k]  = float(np.sqrt(np.sum(x**2)) * (np.linalg.norm(q))) / max(len(x),1)  # crude noise floor\n",
    "        peaks[k]  = {\"score\": peak, \"t_idx\": int(np.argmax(r)) if r.size else 0}\n",
    "    return scores, nulls, peaks\n",
    "\n",
    "def decide(scores, nulls, tau_rel=0.60, tau_abs=0.93):\n",
    "    accepted, seq = {}, []\n",
    "    for k in scores:\n",
    "        s, n = scores[k], nulls[k] + 1e-9\n",
    "        rel = s / n\n",
    "        if (rel >= tau_rel) and (s >= tau_abs):\n",
    "            accepted[k] = {\"score\": round(s, 4), \"rel\": round(rel, 3), \"null\": round(n, 4)}\n",
    "            seq.append((k, s))\n",
    "    seq.sort(key=lambda z: -z[1])\n",
    "    return [k for k, _ in seq], accepted\n",
    "\n",
    "# ---------- core: audit from spans ----------\n",
    "def audit_from_span_map(prompt:str,\n",
    "                        primitive_to_term_mapping:dict,\n",
    "                        T:int=720,\n",
    "                        tau_span:float=0.55,\n",
    "                        tau_abs:float=0.93,\n",
    "                        tau_rel:float=0.60,\n",
    "                        sigma:float=0.02,\n",
    "                        fuse_per_primitive:bool=False):\n",
    "    \"\"\"\n",
    "    primitive_to_term_mapping: {\n",
    "      \"deposit_asset\": [{\"term\":\"add\",\"score\":0.6391,\"t_center\":0.0833}, ...],\n",
    "      ...\n",
    "    }\n",
    "    \"\"\"\n",
    "    # 1) init noise traces\n",
    "    seed = int(hashlib.sha256(prompt.encode(\"utf-8\")).hexdigest()[:8], 16)\n",
    "    rng  = np.random.default_rng(seed)\n",
    "    primitives = list(primitive_to_term_mapping.keys())\n",
    "    traces = {p: rng.normal(0.0, sigma, size=T).astype(\"float32\") for p in primitives}\n",
    "\n",
    "    # 2) build a canonical lobe shape\n",
    "    q = kaiser_window(L=min(160, max(64, T//8)))\n",
    "\n",
    "    # 3) inject lobes from strong spans\n",
    "    span_hits = 0\n",
    "    for p, hits in primitive_to_term_mapping.items():\n",
    "        if not hits: \n",
    "            continue\n",
    "        # keep only strong spans\n",
    "        strong = [h for h in hits if float(h.get(\"score\",0.0)) >= tau_span]\n",
    "        if not strong:\n",
    "            continue\n",
    "\n",
    "        if fuse_per_primitive:\n",
    "            # one fused lobe per primitive (amplitude = max span score)\n",
    "            A = max(float(h[\"score\"]) for h in strong)\n",
    "            tc = np.mean([float(h.get(\"t_center\", 0.5)) for h in strong])\n",
    "            start = max(0, min(T - len(q), int(tc * (T - len(q)))))\n",
    "            traces[p][start:start+len(q)] += A * q\n",
    "            span_hits += 1\n",
    "        else:\n",
    "            # one lobe per span\n",
    "            for h in strong:\n",
    "                A  = float(h[\"score\"])\n",
    "                tc = float(h.get(\"t_center\", 0.5))\n",
    "                start = max(0, min(T - len(q), int(tc * (T - len(q)))))\n",
    "                traces[p][start:start+len(q)] += A * q\n",
    "                span_hits += 1\n",
    "\n",
    "    if span_hits == 0:\n",
    "        return {\n",
    "            \"decision\": \"ABSTAIN\",\n",
    "            \"sequence\": [],\n",
    "            \"accepted_peaks\": {},\n",
    "            \"notes\": {\"reason\": \"no_span_evidence\", \"tau_span\": tau_span, \"T\": T}\n",
    "        }\n",
    "\n",
    "    # 4) matched filter + parser\n",
    "    scores, nulls, peaks = matched_filter_scores(traces, q)\n",
    "    sequence, accepted = decide(scores, nulls, tau_rel=tau_rel, tau_abs=tau_abs)\n",
    "\n",
    "    return {\n",
    "        \"decision\": \"PASS\" if sequence else \"ABSTAIN\",\n",
    "        \"sequence\": sequence,\n",
    "        \"accepted_peaks\": accepted,\n",
    "        \"peaks\": peaks,  # optional: raw peak info\n",
    "        \"notes\": {\n",
    "            \"tau_span\": tau_span, \"tau_rel\": tau_rel, \"tau_abs\": tau_abs,\n",
    "            \"sigma\": sigma, \"T\": T, \"fused\": fuse_per_primitive\n",
    "        }\n",
    "    }\n",
    "\n",
    "# ---------- demo ----------\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    test_prompts, test_labels = get_test_prompts(25)\n",
    "\n",
    "    emb = SbertEmbedding()\n",
    "    phrases = load_phrases()\n",
    "    prototypes = build_prototypes(emb, phrases)\n",
    "    tau_span = 0.55\n",
    "\n",
    "    for k, test_prompt in enumerate(test_prompts):\n",
    "        span_map = spans_from_prompt(test_prompt, prototypes, emb, tau_span=tau_span)\n",
    "        primitive = test_labels[k]\n",
    "        if(primitive in span_map):    \n",
    "            primitive_to_term_mapping = {\n",
    "                \"deposit_asset\":  [],       \n",
    "                \"withdraw_asset\": [],\n",
    "                \"swap_asset\": [],\n",
    "                \"borrow_asset\": [],\n",
    "                \"repay_asset\": [],\n",
    "                \"stake_asset\": [],\n",
    "                \"unstake_asset\": [],\n",
    "                \"claim_rewards\": []\n",
    "            }\n",
    "\n",
    "            primitive_to_term_mapping[primitive] = span_map[primitive]\n",
    "            \n",
    "            audit = audit_from_span_map(\n",
    "                test_prompt,\n",
    "                primitive_to_term_mapping,\n",
    "                T=720, tau_span=0.55, tau_abs=0.50, tau_rel=0.60, sigma=0.02,\n",
    "                fuse_per_primitive=False\n",
    "            )\n",
    "            #print(json.dumps(audit, indent=2))\n",
    "            is_passed = audit['decision']\n",
    "            print(f'{k} {is_passed} / prompt: {test_prompt} / primitives: {list(span_map.keys())}')\n",
    "        else:\n",
    "            print(f'{k} ABSTAIN* / prompt: {test_prompt} / primitives: {list(span_map.keys())} / truth: {primitive}')\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d246b521-61f3-4277-962b-ff5f37b45911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: withdraw staked 0.667255 WBTC from pendle optimism — right away, use normal gas / TRUTH , unstake_asset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'decision': 'PASS',\n",
       " 'sequence': ['withdraw_asset'],\n",
       " 'accepted_peaks': {'withdraw_asset': {'score': 1.1491,\n",
       "   'rel': 588.601,\n",
       "   'null': 0.002}},\n",
       " 'peaks': {'deposit_asset': {'score': 0.07326878607273102, 't_idx': 32},\n",
       "  'withdraw_asset': {'score': 1.1490687131881714, 't_idx': 34},\n",
       "  'swap_asset': {'score': 0.047245629131793976, 't_idx': 383},\n",
       "  'borrow_asset': {'score': 0.03908442705869675, 't_idx': 620},\n",
       "  'repay_asset': {'score': 0.03895927220582962, 't_idx': 589},\n",
       "  'stake_asset': {'score': 0.0428708977997303, 't_idx': 457},\n",
       "  'unstake_asset': {'score': 0.04353821277618408, 't_idx': 286},\n",
       "  'claim_rewards': {'score': 0.04872598871588707, 't_idx': 28}},\n",
       " 'notes': {'tau_span': 0.55,\n",
       "  'tau_rel': 0.6,\n",
       "  'tau_abs': 0.5,\n",
       "  'sigma': 0.02,\n",
       "  'T': 720,\n",
       "  'fused': False}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 18\n",
    "test_prompt = test_prompts[K]\n",
    "primitive = test_labels[k]\n",
    "span_map = spans_from_prompt(test_prompt, prototypes, emb, tau_span=tau_span)\n",
    "primitive_to_term_mapping[primitive] = span_map[primitive]\n",
    "audit = audit_from_span_map(\n",
    "                test_prompt,\n",
    "                primitive_to_term_mapping,\n",
    "                T=720, tau_span=0.55, tau_abs=0.50, tau_rel=0.60, sigma=0.02,\n",
    "                fuse_per_primitive=False\n",
    ")\n",
    "\n",
    "print(f\"PROMPT: {test_prompt} / TRUTH , {test_labels[K]}\")\n",
    "audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3bc360-2b1d-4548-9400-d4777c591c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "primitive_to_term_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00197a57-3caa-481b-8dc5-897a12ac37e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "span_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
