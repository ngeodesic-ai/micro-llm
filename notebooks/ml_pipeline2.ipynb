{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b16f8897-94df-4c5b-a445-b8368867953c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal M8: load → score → sweep thresholds → pick → inspect\n",
    "# Paste this whole cell into a Jupyter notebook.\n",
    "\n",
    "import json, csv, os, math, joblib, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "cwd =  os.getcwd().replace(\"/notebooks\",\"\")\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "90df5632-0aff-4ea9-9bcc-9b27b886128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- I/O ----------------\n",
    "\n",
    "def read_prompts_jsonl(fp: str) -> List[str]:\n",
    "    rows = []\n",
    "    with open(fp, \"r\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                J = json.loads(line)\n",
    "                p = (J.get(\"prompt\") if isinstance(J, dict) else None) or line.strip()\n",
    "                p = str(p).strip()\n",
    "                if p:\n",
    "                    rows.append(p)\n",
    "            except Exception:\n",
    "                s = line.strip()\n",
    "                if s:\n",
    "                    rows.append(s)\n",
    "    return rows\n",
    "\n",
    "def read_labels_csv(fp: str) -> Dict[str, str]:\n",
    "    if not fp:\n",
    "        return {}\n",
    "    gold = {}\n",
    "    try:\n",
    "        df = pd.read_csv(fp)\n",
    "        for _, r in df.iterrows():\n",
    "            p = str(r[\"prompt\"]).strip()\n",
    "            y = str(r[\"label\"]).strip()\n",
    "            if p:\n",
    "                gold[p] = y\n",
    "    except Exception:\n",
    "        with open(fp, newline=\"\") as f:\n",
    "            r = csv.DictReader(f)\n",
    "            for row in r:\n",
    "                p = str(row.get(\"prompt\",\"\")).strip()\n",
    "                y = str(row.get(\"label\",\"\")).strip()\n",
    "                if p:\n",
    "                    gold[p] = y\n",
    "    return gold\n",
    "\n",
    "# ------------- Scoring helpers -------------\n",
    "\n",
    "def predict_scores(mapper, prompts: List[str], class_names: List[str]) -> List[Dict[str,float]]:\n",
    "    \"\"\"\n",
    "    Returns per-prompt score dictionaries (class -> probability-like score).\n",
    "    Logic matches the robust fallbacks used in your M8 script.\n",
    "    \"\"\"\n",
    "    scores: List[Dict[str,float]] = []\n",
    "    # Try predict_proba first\n",
    "    if hasattr(mapper, \"predict_proba\"):\n",
    "        probs = mapper.predict_proba(prompts)\n",
    "        classes = list(getattr(mapper, \"classes_\", class_names))\n",
    "        for row in probs:\n",
    "            row_map = {str(c): float(p) for c, p in zip(classes, row)}\n",
    "            for cname in class_names: row_map.setdefault(cname, 0.0)\n",
    "            scores.append(row_map)\n",
    "        return scores\n",
    "    # Then decision_function → softmax\n",
    "    if hasattr(mapper, \"decision_function\"):\n",
    "        logits = mapper.decision_function(prompts)\n",
    "        logits = np.array(logits, dtype=float)\n",
    "        if logits.ndim == 1: logits = logits.reshape(-1, 1)\n",
    "        classes = list(getattr(mapper, \"classes_\", class_names))\n",
    "        for row in logits:\n",
    "            ex = np.exp(row - row.max())\n",
    "            prob = ex / (ex.sum() + 1e-12)\n",
    "            row_map = {str(c): float(p) for c, p in zip(classes, prob)}\n",
    "            for cname in class_names: row_map.setdefault(cname, 0.0)\n",
    "            scores.append(row_map)\n",
    "        return scores\n",
    "    # Fallback: predict-only\n",
    "    preds = mapper.predict(prompts)\n",
    "    for y in preds:\n",
    "        row_map = {c: 0.0 for c in class_names}\n",
    "        row_map[str(y)] = 1.0\n",
    "        scores.append(row_map)\n",
    "    return scores\n",
    "\n",
    "def metrics_for_threshold(prompts: List[str],\n",
    "                          scores: List[Dict[str,float]],\n",
    "                          class_names: List[str],\n",
    "                          thr: float,\n",
    "                          gold: Dict[str,str]) -> Dict[str,Any]:\n",
    "    total = len(prompts); fired = 0; abstain = 0; correct = 0\n",
    "    for p, smap in zip(prompts, scores):\n",
    "        top = max(class_names, key=lambda c: smap.get(c, 0.0))\n",
    "        conf = smap.get(top, 0.0)\n",
    "        if conf >= thr:\n",
    "            fired += 1\n",
    "            if gold and gold.get(p) == top:\n",
    "                correct += 1\n",
    "        else:\n",
    "            abstain += 1\n",
    "    coverage = fired / max(1, total)\n",
    "    abstain_rate = abstain / max(1, total)\n",
    "    acc_on_fired = (correct / max(1, fired)) if fired else None\n",
    "    overall_acc = correct / max(1, total)\n",
    "    return dict(\n",
    "        threshold=thr, total=total,\n",
    "        abstain=abstain, abstain_rate=abstain_rate,\n",
    "        coverage=coverage, fired=fired,\n",
    "        correct_on_fired=correct,\n",
    "        accuracy_on_fired=acc_on_fired,\n",
    "        overall_correct=correct, overall_accuracy=overall_acc\n",
    "    )\n",
    "\n",
    "def choose_operating_point(metrics: List[Dict[str,Any]],\n",
    "                           max_abstain_rate: float = 0.10,\n",
    "                           choose_by: str = \"abstain_then_acc\") -> Dict[str,Any]:\n",
    "    admissible = [m for m in metrics if m[\"abstain_rate\"] <= max_abstain_rate]\n",
    "    if admissible:\n",
    "        # prefer higher thresholds; break ties by acc_on_fired\n",
    "        admissible.sort(key=lambda m: (m[\"threshold\"], m.get(\"accuracy_on_fired\") or 0.0), reverse=True)\n",
    "        return admissible[0]\n",
    "    if choose_by == \"utility\":\n",
    "        def util(m):\n",
    "            a = (m[\"accuracy_on_fired\"] or 0.0)\n",
    "            c = m[\"coverage\"]\n",
    "            return 0.0 if a == 0 or c == 0 else (2*a*c)/(a+c)\n",
    "        return max(metrics, key=util)\n",
    "    # default: minimize abstain, prefer higher acc_on_fired\n",
    "    metrics.sort(key=lambda m: (m[\"abstain_rate\"], -(m.get(\"accuracy_on_fired\") or 0.0)))\n",
    "    return metrics[0]\n",
    "\n",
    "def rows_at_threshold(prompts: List[str],\n",
    "                      scores: List[Dict[str,float]],\n",
    "                      class_names: List[str],\n",
    "                      thr: float,\n",
    "                      gold: Dict[str,str]) -> pd.DataFrame:\n",
    "    rows=[]\n",
    "    for p, smap in zip(prompts, scores):\n",
    "        top = max(class_names, key=lambda c: smap.get(c, 0.0))\n",
    "        conf = float(smap.get(top, 0.0))\n",
    "        fire = conf >= thr\n",
    "        rows.append(dict(prompt=p,\n",
    "                         gold_label=gold.get(p,\"\"),\n",
    "                         predicted=(top if fire else \"\"),\n",
    "                         confidence=conf,\n",
    "                         abstain=(not fire),\n",
    "                         threshold=thr))\n",
    "    return pd.DataFrame(rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d15ccc77-a53b-4820-92d6-e582b493008d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------- One-call runner for notebooks -------------\n",
    "\n",
    "def m8_notebook_simple(\n",
    "    mapper_path: str,\n",
    "    prompts_jsonl: str,\n",
    "    labels_csv: str = \"\",\n",
    "    class_names: List[str] = (\"deposit_asset\",\"withdraw_asset\",\"swap_asset\",\"check_balance\"),\n",
    "    thresholds: List[float] = (0.5,0.6,0.7,0.8,0.9),\n",
    "    max_abstain_rate: float = 0.10,\n",
    "    min_overall_acc: float | None = 0.85,  # ignored if no labels\n",
    "    choose_by: str = \"abstain_then_acc\",\n",
    "):\n",
    "    # Load artifacts\n",
    "    mapper = joblib.load(mapper_path)\n",
    "    prompts = read_prompts_jsonl(prompts_jsonl)\n",
    "    gold = read_labels_csv(labels_csv) if labels_csv else {}\n",
    "\n",
    "    # Score once\n",
    "    scores = predict_scores(mapper, prompts, list(class_names))\n",
    "\n",
    "    # Sweep\n",
    "    per_thr = [metrics_for_threshold(prompts, scores, list(class_names), float(t), gold)\n",
    "               for t in thresholds]\n",
    "    metrics_df = pd.DataFrame(per_thr).sort_values(\"threshold\")\n",
    "\n",
    "    # Choose operating point\n",
    "    chosen = choose_operating_point(per_thr, max_abstain_rate=max_abstain_rate, choose_by=choose_by)\n",
    "\n",
    "    # Gate status (if labels provided and min_overall_acc is set)\n",
    "    has_labels = bool(gold)\n",
    "    pass_abstain = (chosen[\"abstain_rate\"] <= max_abstain_rate)\n",
    "    pass_accuracy = True if (not has_labels or min_overall_acc is None) else \\\n",
    "                    ((chosen.get(\"overall_accuracy\") or 0.0) >= float(min_overall_acc))\n",
    "    status = \"pass\" if (pass_abstain and pass_accuracy) else \"fail\"\n",
    "\n",
    "    # Per-row table at the chosen threshold (easy drilldown)\n",
    "    rows_df = rows_at_threshold(prompts, scores, list(class_names), chosen[\"threshold\"], gold)\n",
    "\n",
    "    # Pretty print summary\n",
    "    display_cols = [\"threshold\",\"abstain_rate\",\"coverage\",\"accuracy_on_fired\",\"overall_accuracy\",\"fired\",\"abstain\",\"total\"]\n",
    "    print(\"[M8] chosen:\", {k: chosen.get(k) for k in display_cols})\n",
    "    print(\"[M8] status:\", status)\n",
    "\n",
    "    return dict(\n",
    "        status=status,\n",
    "        metrics=metrics_df,\n",
    "        rows=rows_df,\n",
    "        chosen=chosen,\n",
    "        has_labels=has_labels,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "221cda2c-8f63-4d34-a8c4-c5463ea66d16",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '.artifacts/defi_mapper_embed.joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ---------------- Example call (edit paths as needed) ----------------\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mm8_notebook_simple\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmapper_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.artifacts/defi_mapper_embed.joblib\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m     \u001b[49m\u001b[43mprompts_jsonl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtests/fixtures/defi/defi_mapper_5k_prompts.jsonl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlabels_csv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtests/fixtures/defi/defi_mapper_labeled_5k.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# or \"\" if unlabeled\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m     \u001b[49m\u001b[43mthresholds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.35\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_abstain_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmin_overall_acc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.85\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m     \u001b[49m\u001b[43mchoose_by\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutility\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# or \"abstain_then_acc\"\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# display(result[\"metrics\"].head())\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# display(result[\"rows\"].head())\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[90], line 14\u001b[0m, in \u001b[0;36mm8_notebook_simple\u001b[0;34m(mapper_path, prompts_jsonl, labels_csv, class_names, thresholds, max_abstain_rate, min_overall_acc, choose_by)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mm8_notebook_simple\u001b[39m(\n\u001b[1;32m      4\u001b[0m     mapper_path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m      5\u001b[0m     prompts_jsonl: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m ):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Load artifacts\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     mapper \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapper_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     prompts \u001b[38;5;241m=\u001b[39m read_prompts_jsonl(prompts_jsonl)\n\u001b[1;32m     16\u001b[0m     gold \u001b[38;5;241m=\u001b[39m read_labels_csv(labels_csv) \u001b[38;5;28;01mif\u001b[39;00m labels_csv \u001b[38;5;28;01melse\u001b[39;00m {}\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/numpy_pickle.py:735\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode, ensure_native_byte_order)\u001b[0m\n\u001b[1;32m    733\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj, ensure_native_byte_order\u001b[38;5;241m=\u001b[39mensure_native_byte_order)\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 735\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    736\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _validate_fileobject_and_memmap(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m (\n\u001b[1;32m    737\u001b[0m             fobj,\n\u001b[1;32m    738\u001b[0m             validated_mmap_mode,\n\u001b[1;32m    739\u001b[0m         ):\n\u001b[1;32m    740\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    741\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    742\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    743\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '.artifacts/defi_mapper_embed.joblib'"
     ]
    }
   ],
   "source": [
    "# ---------------- Example call (edit paths as needed) ----------------\n",
    "result = m8_notebook_simple(\n",
    "     mapper_path=\".artifacts/defi_mapper_embed.joblib\",\n",
    "     prompts_jsonl=\"tests/fixtures/defi/defi_mapper_5k_prompts.jsonl\",\n",
    "     labels_csv=\"tests/fixtures/defi/defi_mapper_labeled_5k.csv\",  # or \"\" if unlabeled\n",
    "     thresholds=[0.2,0.25,0.3,0.35,0.4],\n",
    "     max_abstain_rate=0.20,\n",
    "     min_overall_acc=0.85,\n",
    "     choose_by=\"utility\",  # or \"abstain_then_acc\"\n",
    ")\n",
    "# display(result[\"metrics\"].head())\n",
    "# display(result[\"rows\"].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
